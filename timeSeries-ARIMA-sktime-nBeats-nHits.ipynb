{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b45c20f",
   "metadata": {},
   "source": [
    "Time series forecasting methods include >>\n",
    "simple exponential smoothing, \n",
    "Holt-Winters exponential smoothing, and \n",
    "autoregressive integrated moving average (ARIMA) models.\n",
    "Regression\n",
    "LSTM\n",
    "\n",
    "clean the data - null /group if needed for duplicate\n",
    "data stationary - transform  data by taking differences or logarithms\n",
    "Data Smoothing - Simple moving average (SMA)/Weighted moving average (WMA)/Exponential smoothing\n",
    "Explore the data - trends, seasonality and patterns, ACF(MA),PACF(AR)\n",
    "Select a forecasting method - Statistical(SARIMA), Regression(Linear relation), LSTM\n",
    "Forecast horizon: The forecast horizon is the number of periods ahead\n",
    "\n",
    "Imp Steps>>\n",
    "select/convert date field - pd.to_datetime(data['period_end'])\n",
    "set date as index - data.set_index('period_end', drop = True, inplace = True)\n",
    "select relevant column - data = data['followers_total']\n",
    "set the frequency of index if required - data = data.asfreq('M')\n",
    "check data index after all changes - data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ae7cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "x = 1\n",
    "if x:\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bad0b9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['NFLX']: Exception('%ticker%: No price data found, symbol may be delisted (1d 1899-12-31 -> 2023-06-30)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 0 entries\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Open       0 non-null      float64\n",
      " 1   High       0 non-null      float64\n",
      " 2   Low        0 non-null      float64\n",
      " 3   Close      0 non-null      float64\n",
      " 4   Adj Close  0 non-null      float64\n",
      " 5   Volume     0 non-null      float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 0.0+ bytes\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Open:10'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpen:100\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpen\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mrolling(window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,min_periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m#visualization\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOpen\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOpen:10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOpen:20\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOpen:50\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOpen:100\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mplot(xlim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2015-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2024-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     43\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m#3. Time Resampling\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m#year end frequency\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ts310\\lib\\site-packages\\pandas\\core\\frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3812\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3813\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3815\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ts310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6070\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6072\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6074\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ts310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6133\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6132\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Open:10'] not in index\""
     ]
    }
   ],
   "source": [
    "#Working with yfinance in Python\n",
    "#https://www.analyticsvidhya.com/blog/2023/06/time-series-analysis-of-netflix-stocks-with-pandas/\n",
    "\n",
    "#!pip install yfinance\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "\n",
    "df =  yf.download(tickers = \"NFLX\")\n",
    "df\n",
    "    \n",
    "# print the metadata of the dataset\n",
    "df.info()\n",
    "\n",
    "# data description\n",
    "df.describe()\n",
    "\n",
    "#Visualizing the Time Series data\n",
    "df['Open'].plot(figsize=(12,6),c='g')\n",
    "plt.title(\"Netlix's Stock Prices\")\n",
    "plt.show()\n",
    "\n",
    "#1.1 Forward Shifting(Positive Lag)\n",
    "df.shift(1)\n",
    "\n",
    "#1.2 Backward Shifting(Negative Lag)\n",
    "df.shift(-1)\n",
    "\n",
    "#2. Rolling Windows\n",
    "df['Open:10 days rolling'] = df['Open'].rolling(10).mean()\n",
    "df[['Open','Open:10 days rolling']].head(20)\n",
    "df[['Open','Open:10 days rolling']].plot(figsize=(15,5))\n",
    "plt.show()\n",
    "\n",
    "df['Open:20'] = df['Open'].rolling(window=20,min_periods=1).mean()\n",
    "df['Open:50'] = df['Open'].rolling(window=50,min_periods=1).mean()\n",
    "df['Open:100'] = df['Open'].rolling(window=100,min_periods=1).mean()\n",
    "\n",
    "#visualization\n",
    "df[['Open','Open:10','Open:20','Open:50','Open:100']].plot(xlim=['2015-01-01','2024-01-01'])\n",
    "plt.show()\n",
    "\n",
    "#3. Time Resampling\n",
    "\n",
    "#year end frequency\n",
    "df.resample(rule='A').max()\n",
    "\n",
    "df['Adj Close'].resample(rule='3Y').mean().plot(kind='bar',figsize=(10,4))\n",
    "plt.title('3 Year End Mean Adj Close Price for Netflix')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3acf5d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no stored variable or alias text_features_ft\n",
      "no stored variable or alias text_features_fs\n",
      "no stored variable or alias ts_data_pp_steps_list\n",
      "no stored variable or alias ts_data_pp_steps_dict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajverma\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.0.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfrom sktime.classification.deep_learning.cnn import CNNClassifier\\nfrom sktime.classification.deep_learning.fcn import FCNClassifier\\nfrom sktime.classification.deep_learning.lstmfcn import LSTMFCNClassifier\\n\\nTBV\\nhttps://github.com/sktime/sktime/blob/main/sktime/forecasting/base/_base.py\\n\\n#dataset\\n#https://www.timeseriesclassification.com/dataset.php\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# updated on Feb03,2023 / added TimeSeries\n",
    "\n",
    "import nbimporter\n",
    "\n",
    "from Common_Functions_L import *\n",
    "\n",
    "from PP_Pipeline_Functions_L import *\n",
    "%store -r feature_create_dict\n",
    "%store -r pp_dict\n",
    "%store -r model\n",
    "%store -r params\n",
    "%store -r feature_drop_dict\n",
    "%store -r feature_discretisation_dict\n",
    "%store -r feature_transformer_dict\n",
    "%store -r feature_outlier_dict\n",
    "%store -r feature_lagfreqwindow_dict\n",
    "%store -r feature_datetime_dict\n",
    "%store -r feature_datetime_list\n",
    "%store -r feature_lagfreqwindow_list\n",
    "%store -r feature_selection_dict\n",
    "%store -r imputer_dict\n",
    "%store -r imputaion_data_type_dict\n",
    "%store -r encoder_dict\n",
    "%store -r simple_encoder_dict\n",
    "%store -r scaler_dict\n",
    "%store -r sample_type_dict\n",
    "%store -r reduction_type_dict\n",
    "\n",
    "%store -r DropDupRows_fs\n",
    "%store -r DropNullRows_fs\n",
    "%store -r DropDupRows_ft\n",
    "%store -r DropNullRows_ft\n",
    "\n",
    "%store -r GroupFeatures_ft\n",
    "%store -r GroupFeatures_fs\n",
    "\n",
    "%store -r text_features_ft\n",
    "%store -r text_features_fs\n",
    "\n",
    "%store -r text_pp_steps_list\n",
    "%store -r text_pp_steps_dict\n",
    "%store -r text_feat_list\n",
    "%store -r text_feat_options_list\n",
    "%store -r text_feat_dict\n",
    "\n",
    "%store -r SparseDropDupRows_ft\n",
    "%store -r SparseDropDupCols_ft\n",
    "%store -r SparseDropDupRows_fs\n",
    "%store -r SparseDropDupCols_fs\n",
    "\n",
    "%store -r data_pp_steps_list\n",
    "%store -r data_pp_steps_dict\n",
    "%store -r data_pp_steps_dict_dflt_list\n",
    "%store -r data_pp_steps_dict_cust_list\n",
    "\n",
    "ts_pp_steps_dict = {}\n",
    "\n",
    "from Models_Functions_L import *\n",
    "%store -r ensemble_sdictR\n",
    "%store -r models_fname_listR\n",
    "%store -r models_sname_listR\n",
    "%store -r models_fdictR\n",
    "%store -r models_sdictR\n",
    "%store -r params_pipeline_fdictR\n",
    "%store -r params_pipeline_sdictR\n",
    "\n",
    "%store -r ensemble_sdictC\n",
    "%store -r models_fname_listC\n",
    "%store -r models_sname_listC\n",
    "%store -r models_fdictC\n",
    "%store -r models_sdictC\n",
    "%store -r params_pipeline_fdictC\n",
    "%store -r params_pipeline_sdictC\n",
    "\n",
    "%store -r tt_cv_pt_dict\n",
    "\n",
    "%store -r ts_models_dict\n",
    "\n",
    "from NN_Functions_L import *\n",
    "%store -r keras_metrics_flistR\n",
    "%store -r keras_metrics_slistR\n",
    "%store -r keras_metrics_flistC\n",
    "%store -r keras_metrics_slistC\n",
    "%store -r contractions_dict\n",
    "\n",
    "from TimeSeries_Functions_L import *\n",
    "%store -r ts_data_pp_steps_list\n",
    "%store -r ts_data_pp_steps_dict\n",
    "\n",
    "from NLP_Functions_L import *\n",
    "#%store  -r text_clean_chk_list\n",
    "%store -r text_cleaning_list\n",
    "%store -r text_cleaning_dict\n",
    "%store -r text_extract_feat_list\n",
    "%store -r text_extract_feat_dict\n",
    "\n",
    "#display pipeline diagram\n",
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "#import common ib\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "from sktime.classification.deep_learning.cnn import CNNClassifier\n",
    "from sktime.classification.deep_learning.fcn import FCNClassifier\n",
    "from sktime.classification.deep_learning.lstmfcn import LSTMFCNClassifier\n",
    "\n",
    "TBV\n",
    "https://github.com/sktime/sktime/blob/main/sktime/forecasting/base/_base.py\n",
    "\n",
    "#dataset\n",
    "#https://www.timeseriesclassification.com/dataset.php\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d444f3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 551\u001b[0m\n\u001b[0;32m    548\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    549\u001b[0m target_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvolume\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 551\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdf1\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    552\u001b[0m score_dict \u001b[38;5;241m=\u001b[39m tt_cv_pt_models_ts(data, target_col, feat_list, model_type, pipeline,\n\u001b[0;32m    553\u001b[0m         index_cols_list\u001b[38;5;241m=\u001b[39mindex_cols_list,\n\u001b[0;32m    554\u001b[0m         nested_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, train_type\u001b[38;5;241m=\u001b[39mtrain_type, forecast_type \u001b[38;5;241m=\u001b[39m forecast_type,\n\u001b[0;32m    555\u001b[0m         forecast_period\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m, regression_window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, cv_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpandingWindowSplitter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    556\u001b[0m         pt_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mForecastingRandomizedSearch\u001b[39m\u001b[38;5;124m'\u001b[39m, transformer_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[0;32m    557\u001b[0m         PlotSeries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    559\u001b[0m score_dict\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "#v1.4 testing ts_forecast_models with ForecastingPipeline /Apr10,2023\n",
    "\n",
    "def create_ts_pipeline(data, pp_steps_dict, pp_steps_list):\n",
    "    '''\n",
    "    1.2 Apr05,2023 : updated for\n",
    "    1.1 Jan29,2023 added index chk,multi index\n",
    "    1.0 Jan23,2023 initial ver\n",
    "    \n",
    "    TBA\n",
    "    TransformedTargetForecaster-takes a chain of transformers and forecasters\n",
    "    MultiplexForecaster\n",
    "    TSFreshFeatureExtractor\n",
    "    WindowSummarizer: mean/stdev/n_legs features\n",
    "    DateTimeFeatures: date features\n",
    "    \n",
    "    TBC\n",
    "    diff between TransformedTargetForecaster and ForecastingPipeline\n",
    "    \n",
    "    Ref\n",
    "    https://towardsdatascience.com/why-start-using-sktime-for-forecasting-8d6881c0a518\n",
    "    '''\n",
    "    \n",
    "    from sktime.forecasting.compose import ForecastingPipeline\n",
    "    from sktime.transformations.series.outlier_detection import HampelFilter\n",
    "    from sktime.transformations.series.boxcox import LogTransformer, BoxCoxTransformer\n",
    "    from sktime.transformations.series.adapt import TabularToSeriesAdaptor\n",
    "    from sktime.transformations.series.impute import Imputer\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sktime.forecasting.theta import ThetaForecaster\n",
    "    from sktime.transformations.panel.summarize import RandomIntervalFeatureExtractor\n",
    "    \n",
    "    print('Entry create_ts_pipeline')\n",
    "    #mandetory check\n",
    "    if len(df1.index.levels) >=3:\n",
    "        ts_type = 'HierarchicalIndex_TS'\n",
    "    elif len(df1.index.levels) ==3:\n",
    "        pass\n",
    "    elif len(df1.index.levels) ==2:\n",
    "        pass\n",
    "    elif len(df1.index.levels) ==1:\n",
    "        pass\n",
    "    \n",
    "    if ts_type == 'HierarchicalIndex_TS':\n",
    "        from sktime.transformations.hierarchical.aggregate import Aggregator\n",
    "        aggregator = Aggregator()\n",
    "    else:\n",
    "        aggregator = 'passthrough'\n",
    "        \n",
    "    \n",
    "    if pp_steps_dict:\n",
    "        #2.1 ImputerCat/ImputerNum\n",
    "        if 'Outlier' in pp_steps_dict:\n",
    "            if pp_steps_dict['Outlier'] == 'HampelFilter':\n",
    "                outlier = HampelFilter()\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            outlier = 'passthrough'\n",
    "            \n",
    "        \n",
    "        if 'Transformer' in pp_steps_dict:\n",
    "            if pp_steps_dict['Transformer'] == 'LogTransformer':\n",
    "                transformer = LogTransformer()\n",
    "            elif pp_steps_dict['Transformer'] == 'BoxCoxTransformer':\n",
    "                transformer = BoxCoxTransformer(sp=4)\n",
    "            else:\n",
    "                transformer = 'passthrough'\n",
    "    \n",
    "    #5 optional pp_steps_list\n",
    "    print('\\n ##5 pp_steps_list: setup flags MatchFeat, DropNullRows, DropDupRows, DropDupCols, DropQuasiConstCols')\n",
    "    if pp_steps_list:\n",
    "        if 'FeatureExtraction' in pp_steps_list:\n",
    "            from sktime.transformations.panel.tsfresh import TSFreshFeatureExtractor\n",
    "            FeatureExtraction1 = TSFreshFeatureExtractor(default_fc_parameters=\"efficient\", show_warnings=False)\n",
    "            #FeatureExtraction2 = RandomIntervalFeatureExtractor(n_intervals=\"sqrt\", features=[np.mean, np.std, _slope]),\n",
    "        else:\n",
    "            FeatureExtraction = 'passthrough'\n",
    "            \n",
    "            \n",
    "            \n",
    "    pipeline = ForecastingPipeline(\n",
    "    [\n",
    "    (\"imputer\", Imputer(method=\"mean\")),\n",
    "    (\"minmaxscaler\", TabularToSeriesAdaptor(MinMaxScaler())),\n",
    "    ('aggregator', aggregator), # for Hierarchical TS\n",
    "    (\"outlier\", outlier),\n",
    "    (\"transformer\", transformer),\n",
    "    (\"model\", ThetaForecaster(sp=4)) #defaut otherwise error\n",
    "    ])\n",
    "    \n",
    "    print('Exit create_ts_pipeline')\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def tt_cv_pt_models_ts(data, target_col, feat_list, model_type, pipeline, index_cols_list=[],\n",
    "        nested_params=None, train_type='TrainTest', forecast_type = 'regression',\n",
    "        forecast_period=5, freq='M', regression_window=10, cv_type='ExpandingWindowSplitter',\n",
    "        pt_type='ForecastingRandomizedSearch', transformer_type=None, \n",
    "        aggregation_transformer=True, PlotSeries=False):\n",
    "    '''\n",
    "    \n",
    "    v1.6 Apr10,2023: Added model types and model list\n",
    "    v1.5 Mar24,2023: Added MutliIndex Creation\n",
    "    v1.4 Mar15,2023: Added forecast_type\n",
    "    v1.3 Jan26,2023: Updated for new model_type/ added probabilistic predictions support\n",
    "    v1.2 Jan23,2023: Updated for pipeline\n",
    "    v1.1 Jan20,2023: added regression_window, cv_type, pt_type, forecasting score/CrossValidation\n",
    "    v1.0 Jan18,2023: initial version\n",
    "    \n",
    "    Note: Important\n",
    "    index type(uni/multi-panel/Hier) / y-target (uni/multi variate) / X-feat (with/without features)\n",
    "    \n",
    "    forecast_type: regression/binary_classification/multi_classification\n",
    "    freq: M/Q/Y\n",
    "    \n",
    "    pipeline:if pipeline true then model_type (model) will update in pipeline that will be fitted else model_type will be fitted\n",
    "    train_type: TrainTest/CrossValidation/GridSearch/RandomizedSearch\n",
    "    freq: used to set index whern data is as df with date col\n",
    "    regression_window: when regression models are used\n",
    "    cv_type: used in cross_validation/GridSerach - ExpandingWindowSplitter/SlidingWindowSplitter\n",
    "    pt_type: used in parameter tuning - ForecastingGridSearch/ForecastingRandomizedSearch\n",
    "    PlotSeries: does not support for MultiIndex & Cross Validation\n",
    "    aggregation_transformer : for multi index\n",
    "    \n",
    "    On Memory testing dataset:\n",
    "    from sktime.datasets import load_airline\n",
    "    series_PeriodIndex_UV = load_airline()\n",
    "\n",
    "    from sktime.datatypes import get_examples\n",
    "    array_1D = get_examples(mtype=\"np.ndarray\", as_scitype=\"Series\")[0]\n",
    "    array_2D = get_examples(mtype=\"np.ndarray\", as_scitype=\"Series\")[1]\n",
    "    array_3D = get_examples(mtype=\"numpy3D\", as_scitype=\"Panel\")[0]\n",
    "    \n",
    "    series_NormalIndex_UV = get_examples(mtype=\"pd.Series\", as_scitype=\"Series\")[0]\n",
    "    \n",
    "    df_UniIndex_UV = get_examples(mtype=\"pd.DataFrame\")[0] #ok\n",
    "    df_UniIndex_MV = get_examples(mtype=\"pd.DataFrame\")[1]\n",
    "    \n",
    "    df_MultiIndex_MV = get_examples(mtype=\"pd-multiindex\")[0]\n",
    "    df_MultiIndexHier_MV = get_examples(mtype=\"pd_multiindex_hier\")[0]\n",
    "    df_MultiIndexHier_UV = get_examples(mtype=\"pd_multiindex_hier\")[1]\n",
    "    \n",
    "    from sktime.utils._testing.hierarchical import _make_hierarchical\n",
    "    #Multi Variate dynamic\n",
    "    df_MultiIndexHier_MV_FeatDynamic = _make_hierarchical(n_columns=2) #ok\n",
    "    \n",
    "    from sktime.utils._testing.hierarchical import _bottom_hier_datagen\n",
    "    #Multi index level Dynamic\n",
    "    df_MultiIndexHier_UV_IndexDynamic = _bottom_hier_datagen(no_levels=2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Known issues:\n",
    "    Auto_Arima with cross_validation TypeError\n",
    "    TypeError: input must be a one of (<class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.series.Series'>), but found type: <class 'float'>\n",
    "    transformer_type : fails for MultiIndex\n",
    "    TBA:\n",
    "    MultiOutputRegressor\n",
    "    '''\n",
    "    print('entry tt_cv_pt_models_ts')\n",
    "    \n",
    "    #import libs\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sktime.forecasting.base import ForecastingHorizon\n",
    "    from sktime.utils.plotting import plot_series\n",
    "    from sktime.forecasting.compose import make_reduction\n",
    "    from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "    \n",
    "    from sktime.performance_metrics.forecasting import mean_absolute_percentage_error\n",
    "    from sklearn.metrics import accuracy_score\n",
    "        \n",
    "    print('\\n ##1 index set single/multi/hierarchical')\n",
    "    print(f'data-typ:{type(data)}, data-shape:{data.shape}')\n",
    "    if len(index_cols_list) ==1: # single level index\n",
    "        print('executing single index')\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            if freq:\n",
    "                #Note1: convert partial str date(yyyy-mm or yy-q) to complete date format(yyyy-mm-dd)\n",
    "                data[index_col] = pd.PeriodIndex(data[index_col], freq=freq).to_timestamp()\n",
    "                #Note2: convert complete date format(yyyy-mm-dd) to date index w.r.t freq (yyyyqq)\n",
    "                data = data.set_index(index_col, drop=True).to_period(freq) #set index of \n",
    "            else:\n",
    "                data.set_index(index_col, drop=True, inplace=True) # convert to series\n",
    "    else: # multi level index for series/df\n",
    "        print('executing MultiLeval index')\n",
    "        #data = set_index(data, index_cols_list) #set hierarchical index/multi_index\n",
    "        index = pd.MultiIndex.from_frame(data[index_cols_list])\n",
    "        data.set_index(index, drop=True, inplace=True)\n",
    "        data.drop(index_cols_list, axis=1, inplace=True)\n",
    "          \n",
    "    print('#1.1 index_type check PeriodIndex/RangeIndex/MultiIndex')\n",
    "    #Note: pandas index types: Int64Index, RangeIndex, DatetimeIndex, PeriodIndex;\n",
    "    #Note: if DatetimeIndex, the freq attribute must be set.\n",
    "    \n",
    "    import pandas\n",
    "    if isinstance(data.index, pandas.core.indexes.period.PeriodIndex):\n",
    "        index_type = 'PeriodIndex'\n",
    "    elif isinstance(data.index, pandas.core.indexes.range.RangeIndex):\n",
    "        index_type = 'RangeIndex'\n",
    "    elif isinstance(data.index, pandas.core.indexes.multi.MultiIndex):\n",
    "        index_type = 'MultiIndex'\n",
    "        #check index level eg. no of features in index\n",
    "        if len(data.index.levels)==1:\n",
    "            index_type = 'MultiIndex'\n",
    "        elif len(data.index.levels)==2:\n",
    "            index_type = 'MultiIndexPanel'\n",
    "        elif len(data.index.levels)>=3:\n",
    "            index_type = 'MultiIndexHier'\n",
    "            \n",
    "        #note: pd_multiindex_hier format to store node-wise aggregates.add __total @ each node\n",
    "        if aggregation_transformer:\n",
    "            from sktime.transformations.hierarchical.aggregate import Aggregator\n",
    "            data = Aggregator().fit_transform(data)\n",
    "            \n",
    "    elif isinstance(data.index, pandas.core.indexes.base.Index): #TBC\n",
    "        index_type = 'NormalIndex'\n",
    "    print('index_type:', index_type)\n",
    "        \n",
    "    print('#1.2 ts_type check UniVariate vs MultiVariate')\n",
    "    if isinstance(target_col, str):\n",
    "        ts_type = 'UniVariate'\n",
    "    elif isinstance(data, pd.Series):\n",
    "        ts_type = 'UniVariate'\n",
    "    elif isinstance(target_col, list):\n",
    "        if len(target_col) == 1:\n",
    "            ts_type = 'UniVariate'\n",
    "        elif len(target_col) > 1:\n",
    "            ts_type = 'MultiVariate'\n",
    "    \n",
    "    print('ts_type:', ts_type)\n",
    "        \n",
    "        \n",
    "    print('\\n ##2 data chk, split train/valid, chk UniVariate/MultiVariate:', type(data))\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        all_feat_list = data.columns.tolist()\n",
    "        sel_feat_list = feat_list #passed thr function\n",
    "        print(f'all_feat_list:{all_feat_list}, sel_feat_list:{sel_feat_list}')\n",
    "        \n",
    "        #if len(sel_feat_list) > 1: # when external feat_list passed\n",
    "        if (len(all_feat_list) > 1) or (len(sel_feat_list) > 1):\n",
    "            print('executing: if len(sel_feat_list) > 1')\n",
    "            X = data[sel_feat_list]\n",
    "            \n",
    "            #note: bug fix for MultiIndex, Univariate target y with input features X, y should in df/Mar24,2023\n",
    "            if ('Multi' in index_type) & (ts_type == 'UniVariate'):\n",
    "                y = data[[target_col]] #MultiIndex fails if y is series\n",
    "            else:\n",
    "                y = data[target_col] \n",
    "                \n",
    "            y_train, y_valid, X_train, X_valid = temporal_train_test_split(y, X, test_size=0.2) \n",
    "            print(f'df-y_train:{y_train.shape}, y_valid:{y_valid.shape}, X_train:{X_train.shape}, X_valid:{X_valid.shape}')\n",
    "        \n",
    "        elif (len(all_feat_list) == 1) or (len(sel_feat_list) == 1):\n",
    "            print('executing: (len(all_feat_list) == 1) or (len(sel_feat_list) == 1)')\n",
    "            X = None\n",
    "            y = data\n",
    "            y_train, y_valid = temporal_train_test_split(y, test_size=0.2)\n",
    "            \n",
    "        else:\n",
    "            print('executing else for train/test')\n",
    "            X = None\n",
    "            y = data[target_col]\n",
    "            y_train, y_valid = temporal_train_test_split(y, test_size=0.2)\n",
    "            print(f'df-srs-y_train:{y_train.shape}, y_valid:{y_valid.shape}')\n",
    "            \n",
    "    elif isinstance(data, pd.Series):\n",
    "        X = None\n",
    "        y = data\n",
    "        y_train, y_valid = temporal_train_test_split(y, test_size=0.2)\n",
    "        print(f'srs-y_train:{y_train.shape}, y_valid:{y_valid.shape}')\n",
    "       \n",
    "    print('\\n ##3 set target transformer') #Not needed if pipeline is passed\n",
    "    if transformer_type:\n",
    "        if ('Multi' not in index_type):\n",
    "            #Series transformers:DTrend, Adapt, box-cox, AutoCorrelation, Cosine\n",
    "            if transformer_type == 'BoxCoxTransformer':\n",
    "                from sktime.transformations.series.boxcox import BoxCoxTransformer\n",
    "                transformer = BoxCoxTransformer(sp=4) #Box-Cox power transformation\n",
    "            elif transformer_type == 'LogTransformer':\n",
    "                from sktime.transformations.series.boxcox import LogTransformer\n",
    "                transformer = LogTransformer()\n",
    "        elif index_type == 'MultiIndexPanel':\n",
    "            pass\n",
    "            #Panel transformers:Shapelet, Segment, Reduce, Rocket, PCA, Matrix profile, Compose, Summarize, tsfresh\n",
    "    y_train = transformer.fit_transform(y_train)\n",
    "        \n",
    "    print('\\n ##4 create score dict')\n",
    "    score_dict = {}\n",
    "    score_dict['ts_type'] = ts_type\n",
    "    score_dict['model_type'] = model_type\n",
    "    score_dict['index_type'] = index_type\n",
    "    score_dict['forecast_type'] = forecast_type\n",
    "    \n",
    "    print('\\n ##5 get ForecastingHorizon w.r.t y_valid:', len(y_valid))\n",
    "    #fh_abs = ForecastingHorizon(y_valid.index, is_relative=False)\n",
    "    #print('fh_abs:',fh_abs) # y_valid index\n",
    "    \n",
    "    #note: fh_re list should same size of y_valid so pred/y_valid will be same size/calculate score\n",
    "    #note: Multi index split will have record from each first index, chk split y_train/y_valid/y/pred to undersatnd\n",
    "    #eg.  df_MultiIndex_MV: if from first level index of 4 window will be 4 records but actual will be 1 record from each window to predict\n",
    "    if 'Multi' in index_type:\n",
    "        fh_re = [1] #quick fix/TBC\n",
    "    else:\n",
    "        fh_re = [a for a in range(1, len(y_valid)+1)]\n",
    "    \n",
    "    print(f'y_valid-len:{len(y_valid)}, fh_re-len:{len(fh_re)}, fh_re:{fh_re}')\n",
    "    \n",
    "    print(f'\\n ##6 executing model:{model_type},  forecast_type:{forecast_type}')\n",
    "    if forecast_type == 'regression':\n",
    "        if model_type == 'NaiveForecaster':\n",
    "            #Forecast based on naive assumptions about past trends continuing, supports univariate/multivariate\n",
    "            from sktime.forecasting.naive import NaiveForecaster, NaiveVariance\n",
    "            forecaster = NaiveForecaster()\n",
    "\n",
    "            #NaiveVariance adds to a `forecaster` the ability to compute the prediction variance based on naive assumptions about ts.\n",
    "            #forecaster = NaiveForecaster(strategy=\"drift\")\n",
    "            #forecaster = NaiveVariance(forecaster)\n",
    "        elif model_type == 'PolynomialTrend':\n",
    "            #linear regression model with a 1st degree polynomial transformation of the feature.\n",
    "            from sktime.forecasting.trend import PolynomialTrendForecaster\n",
    "            forecaster = PolynomialTrendForecaster(degree=1)\n",
    "        elif model_type == 'ThetaForecaster':\n",
    "            #it's equivalent to simple exponential smoothing (SES) with drift/tested for seasonality\n",
    "            from sktime.forecasting.theta import ThetaForecaster\n",
    "            forecaster = ThetaForecaster(sp=12)\n",
    "\n",
    "        elif model_type == 'ExponentialSmoothing':\n",
    "            from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "            forecaster = ExponentialSmoothing(trend=\"add\", sp=12)\n",
    "            \n",
    "        elif model_type == 'BATS':\n",
    "            from sktime.forecasting.bats import BATS\n",
    "            forecaster = BATS(sp=12, use_trend=True, use_box_cox=False)\n",
    "        elif model_type == 'TBATS':\n",
    "            from sktime.forecasting.tbats import TBATS\n",
    "            forecaster = TBATS(sp=12, use_trend=True, use_box_cox=False)\n",
    "        elif model_type == 'Prophet':\n",
    "            from sktime.forecasting.fbprophet import Prophet\n",
    "            forecaster = Prophet(\n",
    "                seasonality_mode=\"multiplicative\",\n",
    "                n_changepoints=int(len(y_train) / 12),\n",
    "                add_country_holidays={\"country_name\": \"Germany\"},\n",
    "                yearly_seasonality=True,\n",
    "                weekly_seasonality=False,\n",
    "                daily_seasonality=False,\n",
    "                ) \n",
    "        elif model_type == 'UnobservedComponents':\n",
    "            from sktime.forecasting.structural import UnobservedComponents\n",
    "            forecaster = UnobservedComponents(\n",
    "                level=\"local linear trend\", freq_seasonal=[{\"period\": 12, \"harmonics\": 10}])\n",
    "        elif model_type == 'StatsForecastAutoARIMA':\n",
    "            from sktime.forecasting.statsforecast import StatsForecastAutoARIMA\n",
    "            forecaster = StatsForecastAutoARIMA(sp=12)\n",
    "            \n",
    "        elif model_type == 'AutoArima':\n",
    "            #univariate only / TBC\n",
    "            from sktime.forecasting.arima import AutoARIMA\n",
    "            forecaster = AutoARIMA(sp=4)\n",
    "        #elif (model_type == 'Arima') & (ts_type = 'UniVariate'):\n",
    "        elif model_type == 'Arima':\n",
    "            #univariate only\n",
    "            from sktime.forecasting.arima import ARIMA\n",
    "            forecaster = ARIMA()\n",
    "        elif (model_type == 'VAR') & (ts_type == 'MultiVariate'):\n",
    "            #MultiVariate only\n",
    "            from sktime.forecasting.var import VAR\n",
    "            forecaster = VAR()\n",
    "        elif model_type == 'AutoETS':\n",
    "            from sktime.forecasting.ets import AutoETS\n",
    "            forecaster = AutoETS(auto=True, sp=12, n_jobs=-1)\n",
    "            \n",
    "        \n",
    "    \n",
    "        else:\n",
    "            print(f'#executing Regression:{model_type}')\n",
    "            if regression_window:\n",
    "                '''\n",
    "                strategy=\n",
    "                Direct:create a separate model for each period we are forecasting.\n",
    "                Recursive:fit a single one-step ahead model,previous time steps output for the following input\n",
    "                Multiple outputs: one model is used to predict the entire time series horizon in a single forecast.\n",
    "                '''\n",
    "                forecaster = make_reduction(model_type, strategy=\"recursive\", window_length= regression_window)\n",
    "                #forecaster = ReducedRegressionForecaster(regressor, window_length=12)\n",
    "\n",
    "    elif forecast_type in ['binary_classification', 'multi_classification']:   \n",
    "        if model_type == 'TimeSeriesForestClassifier': \n",
    "            #from sktime.classification.compose import TimeSeriesForestClassifier\n",
    "            from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "            forecaster = TimeSeriesForestClassifier()\n",
    "        elif model_type == 'HIVECOTEV2': #TBC\n",
    "            from sktime.classification.hybrid import HIVECOTEV2\n",
    "            forecaster = HIVECOTEV2()\n",
    "            #forecaster.fit(X_sk, pd.Series(y))\n",
    "        elif model_type == 'RandomIntervalSpectralForest': #TBC\n",
    "            pass\n",
    "        elif model_type == 'IndividualBOSS': #TBC\n",
    "            from sktime.classification.dictionary_based import IndividualBOSS\n",
    "            forecaster = IndividualBOSS()\n",
    "        elif model_type == 'BaggingForecaster': #TBC\n",
    "            from sktime.forecasting.compose import BaggingForecaster\n",
    "            forecaster = BaggingForecaster()\n",
    "        \n",
    "   \n",
    "    \n",
    "    print('Executing pipeline:', pipeline)\n",
    "    #cls = make_pipeline(TSFreshFeatureExtractor(show_warnings=False), RandomForestClassifier())\n",
    "    if pipeline:\n",
    "        print('\\n ##7 add model last step of pipeline')    \n",
    "        model_name = forecaster.__class__.__name__\n",
    "\n",
    "        if pipeline.steps[-1][0] == 'model':\n",
    "            pipeline.steps.pop(-1)\n",
    "            pipeline.steps.append(('model', forecaster))\n",
    "        else:\n",
    "            pipeline.steps.append(('model', forecaster))\n",
    "            \n",
    "        from copy import deepcopy\n",
    "        forecaster = deepcopy(pipeline)\n",
    "    print('forecaster:', forecaster)\n",
    "    \n",
    "    print('\\n ##8 setup cv w.r.t cv_type:', cv_type)\n",
    "    from sktime.forecasting.model_selection import SlidingWindowSplitter, ExpandingWindowSplitter\n",
    "    from sktime.forecasting.model_selection import SingleWindowSplitter\n",
    "    \n",
    "    ##X_train = None if ts_type == 'UniVariate' else X_train # multivarite X_train will be used\n",
    "    \n",
    "    window_size = round(y_train.size/4) #TBC\n",
    "    if cv_type == 'ExpandingWindowSplitter':\n",
    "         #note: initial_window + fh shold be smaller than the length of y\n",
    "        cv = ExpandingWindowSplitter(step_length=window_size, fh=list(range(1, window_size)), initial_window=window_size)\n",
    "    elif cv_type == 'SlidingWindowSplitter':\n",
    "        #note: initial_window must greater than `window_length`\n",
    "        cv = SlidingWindowSplitter(initial_window=window_size*2, window_length=window_size)\n",
    "    elif cv_type == 'SingleWindowSplitter':\n",
    "        #note: initial_window must greater than `window_length`\n",
    "        #validation_size = 26\n",
    "        validation_size = y_train.size\n",
    "        cv = SingleWindowSplitter(window_length=len(y)-validation_size, fh=validation_size)\n",
    "        \n",
    "        \n",
    "    print('\\n ##9 forecaster fit and predict w.r.t train_type:', train_type, ts_type)\n",
    "    print(f'shape y_train:{y_train.shape}, y_valid:{y_valid.shape}')\n",
    "    print(f'type y_train:{type(y_train)}, y_valid:{type(y_valid)}')\n",
    "    print(f'len y_train:{len(y_train)}, y_valid:{len(y_valid)}, fh_re:{len(fh_re)}')\n",
    "    \n",
    "    \n",
    "    if train_type == 'TrainTest':\n",
    "        #Note1: for MultiIndex with X features, y should be DataFrame\n",
    "        #if len(sel_feat_list) > 1: #when features X there\n",
    "        if len(sel_feat_list) >= 1: #when features X there\n",
    "            print('executing train_type with X')\n",
    "            if ('Multi' in index_type) & (ts_type == 'UniVariate'): #quick fix/Mar24,2023\n",
    "                y_train = y_train.squeeze()\n",
    "                y_valid = y_valid.squeeze()\n",
    "            \n",
    "            print(f'y_train:{type(y_train)}, y_valid:{type(y_valid)}')\n",
    "            forecaster.fit(y_train, X_train)\n",
    "            #forecaster.fit(y=pd.DataFrame(y_train), X=X_train) #TBC as per Note1\n",
    "            y_pred = forecaster.predict(X=X_valid, fh=fh_re)\n",
    "        else:\n",
    "            print('executing train_type without X')\n",
    "            forecaster.fit(y_train)\n",
    "            y_pred = forecaster.predict(fh=fh_re)\n",
    "            \n",
    "        print('\\ny_pred:', y_pred)\n",
    "        print('\\n\\ny_valid:', y_valid)\n",
    "        print(f'y_pred-shp:{y_pred.shape}, y_valid:{y_valid.shape}')\n",
    "            \n",
    "    elif train_type == 'CrossValidation':\n",
    "        from sktime.forecasting.model_evaluation import evaluate\n",
    "        pred_df = evaluate(forecaster=forecaster, X=X_train, y=y_train, cv=cv, strategy=\"refit\", return_data=True)\n",
    "        \n",
    "    elif train_type in ['GridSearch', 'RandomizedSearch']:\n",
    "        from sktime.forecasting.model_selection import ForecastingGridSearchCV, ForecastingRandomizedSearchCV\n",
    "        if train_type == 'GridSearch':\n",
    "            forecaster = ForecastingGridSearchCV(forecaster, strategy=\"refit\", cv=cv, param_grid=nested_params)\n",
    "        elif train_type == 'RandomizedSearch':\n",
    "            forecaster = ForecastingRandomizedSearchCV(forecaster, strategy=\"refit\", cv=cv, \n",
    "                        param_distributions=nested_params, n_iter=1, random_state=42)\n",
    "            \n",
    "        if len(sel_feat_list) > 1: #when features X there \n",
    "            forecaster.fit(y_train, X_train)\n",
    "            y_pred = forecaster.predict(X=X_valid, fh=fh_re)\n",
    "        else:\n",
    "            forecaster.fit(y_train)\n",
    "            y_pred = forecaster.predict(fh=fh_re)\n",
    "            \n",
    "    print('\\n ##10 performance_metrics - forecasting score w.r.t:', train_type)\n",
    "    from sktime.performance_metrics.forecasting import make_forecasting_scorer\n",
    "    if train_type in ['TrainTest', 'GridSearch', 'RandomizedSearch']:\n",
    "        print(f'y_valid:{len(y_valid)}, y_pred:{len(y_pred)} ')\n",
    "        \n",
    "        if forecast_type in ['binary_classification', 'multi_classification']:\n",
    "            acc = accuracy_score(y_valid, y_pred)\n",
    "            score_dict['acc'] = acc\n",
    "            \n",
    "        if forecast_type in ['regression']:     \n",
    "            mape = mean_absolute_percentage_error(y_valid, y_pred, symmetric=False)\n",
    "            score_dict['mape'] = mape\n",
    "        \n",
    "        score_dict['y_pred'] = y_pred\n",
    "        score_dict['pipeline'] = pipeline\n",
    "        \n",
    "    elif train_type in ['CrossValidation']:\n",
    "        score_dict['pred_df'] = pred_df\n",
    "    print('score_dict:', score_dict)\n",
    "    \n",
    "    print ('\\n ##11 check probabilistic predictions support')\n",
    "    #Probabilistic forecasting: prediction intervals, quantile, variance, and distributional forecasts\n",
    "    if forecaster.get_tag(\"capability:pred_int\"):\n",
    "        \n",
    "        if forecast_type in ['regression']:\n",
    "            pred_intervals = forecaster.predict_interval(coverage=0.9)\n",
    "            pred_quantiles = forecaster.predict_quantiles(fh=None, X=None, alpha=[0.05, 0.95])\n",
    "            pred_variance = forecaster.predict_var(fh=None, X=None, cov=False)\n",
    "            score_dict['pred_intervals'] = pred_intervals\n",
    "            score_dict['pred_quantiles'] = pred_quantiles\n",
    "            score_dict['pred_variance'] = pred_variance\n",
    "        \n",
    "        if forecast_type in ['binary_classification', 'multi_classification']:\n",
    "            pred_proba = forecaster.predict_proba(fh=None, X=None, marginal=True)\n",
    "            score_dict['pred_proba'] = pred_proba\n",
    "        \n",
    "    #PlotSeries = False # temp dueto train_type == 'CrossValidation' /MultiIndex\n",
    "    if PlotSeries:\n",
    "        print(f'\\n ##12 PlotSeries w.r.t :{train_type}')\n",
    "        if train_type in ['TrainTest', 'GridSearch', 'RandomizedSearch']:\n",
    "            print(f'df-y_train:{y_train.shape}, y_valid:{y_valid.shape}, y_pred:{y_pred.shape}')\n",
    "            plot_series(y_train, y_valid, y_pred, labels=[\"y_train\", \"y_valid\", \"y_pred\"]);\n",
    "        elif train_type == 'CrossValidation':\n",
    "            n_cv = pred_df.shape[0]\n",
    "            plot_series(\n",
    "                y, *[pred_df[\"y_pred\"].iloc[x] for x in range(n_cv)],\n",
    "                markers=[\"o\", *[\".\"]*n_cv],\n",
    "                labels=[\"y_true\"] + [f\"cv: {x}\" for x in range(n_cv)]\n",
    "            );\n",
    "        \n",
    "    print('exit tt_cv_pt_models_ts')\n",
    "    return score_dict\n",
    "\n",
    "train_type = 'TrainTest'\n",
    "forecast_type = 'regression'\n",
    "index_cols_list = ['state', 'drug', 'ndc', 'pharmacy_id', 'time']\n",
    "\n",
    "feat_list = ['volume']\n",
    "model_type = 'NaiveForecaster'\n",
    "pipeline = None\n",
    "target_col = 'volume'\n",
    "\n",
    "data = df1.copy()\n",
    "score_dict = tt_cv_pt_models_ts(data, target_col, feat_list, model_type, pipeline,\n",
    "        index_cols_list=index_cols_list,\n",
    "        nested_params=None, train_type=train_type, forecast_type = forecast_type,\n",
    "        forecast_period=5, freq='M', regression_window=10, cv_type='ExpandingWindowSplitter',\n",
    "        pt_type='ForecastingRandomizedSearch', transformer_type=None, \n",
    "        PlotSeries=False)\n",
    "\n",
    "score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "460a73b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TimeSeriesForestClassifier' from 'sktime.classification.compose' (C:\\Users\\ajverma\\AppData\\Local\\anaconda3\\envs\\ts310\\lib\\site-packages\\sktime\\classification\\compose\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msktime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassification\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompose\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimeSeriesForestClassifier\n\u001b[0;32m      2\u001b[0m forecaster \u001b[38;5;241m=\u001b[39m TimeSeriesForestClassifier()\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TimeSeriesForestClassifier' from 'sktime.classification.compose' (C:\\Users\\ajverma\\AppData\\Local\\anaconda3\\envs\\ts310\\lib\\site-packages\\sktime\\classification\\compose\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from sktime.classification.compose import TimeSeriesForestClassifier\n",
    "forecaster = TimeSeriesForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7daa2352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Delivery_person_ID</th>\n",
       "      <th>Delivery_person_Age</th>\n",
       "      <th>Delivery_person_Ratings</th>\n",
       "      <th>Restaurant_latitude</th>\n",
       "      <th>Restaurant_longitude</th>\n",
       "      <th>Delivery_location_latitude</th>\n",
       "      <th>Delivery_location_longitude</th>\n",
       "      <th>Type_of_order</th>\n",
       "      <th>Type_of_vehicle</th>\n",
       "      <th>Time_taken(min)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17281</th>\n",
       "      <td>D52B</td>\n",
       "      <td>ALHRES010DEL03</td>\n",
       "      <td>28</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>Snack</td>\n",
       "      <td>motorcycle</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32188</th>\n",
       "      <td>162B</td>\n",
       "      <td>BANGRES15DEL03</td>\n",
       "      <td>37</td>\n",
       "      <td>4.3</td>\n",
       "      <td>12.975377</td>\n",
       "      <td>77.696664</td>\n",
       "      <td>13.005377</td>\n",
       "      <td>77.726664</td>\n",
       "      <td>Drinks</td>\n",
       "      <td>motorcycle</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20791</th>\n",
       "      <td>B7EC</td>\n",
       "      <td>COIMBRES14DEL01</td>\n",
       "      <td>38</td>\n",
       "      <td>4.5</td>\n",
       "      <td>11.003681</td>\n",
       "      <td>76.975525</td>\n",
       "      <td>11.013681</td>\n",
       "      <td>76.985525</td>\n",
       "      <td>Meal</td>\n",
       "      <td>scooter</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17948</th>\n",
       "      <td>95B9</td>\n",
       "      <td>COIMBRES17DEL01</td>\n",
       "      <td>34</td>\n",
       "      <td>4.7</td>\n",
       "      <td>11.026117</td>\n",
       "      <td>76.944652</td>\n",
       "      <td>11.136117</td>\n",
       "      <td>77.054652</td>\n",
       "      <td>Snack</td>\n",
       "      <td>motorcycle</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21726</th>\n",
       "      <td>DC41</td>\n",
       "      <td>AURGRES07DEL01</td>\n",
       "      <td>26</td>\n",
       "      <td>4.5</td>\n",
       "      <td>19.875908</td>\n",
       "      <td>75.358888</td>\n",
       "      <td>19.985908</td>\n",
       "      <td>75.468888</td>\n",
       "      <td>Buffet</td>\n",
       "      <td>scooter</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID Delivery_person_ID  Delivery_person_Age  Delivery_person_Ratings  \\\n",
       "17281  D52B     ALHRES010DEL03                   28                      4.7   \n",
       "32188  162B     BANGRES15DEL03                   37                      4.3   \n",
       "20791  B7EC    COIMBRES14DEL01                   38                      4.5   \n",
       "17948  95B9    COIMBRES17DEL01                   34                      4.7   \n",
       "21726  DC41     AURGRES07DEL01                   26                      4.5   \n",
       "\n",
       "       Restaurant_latitude  Restaurant_longitude  Delivery_location_latitude  \\\n",
       "17281             0.000000              0.000000                    0.090000   \n",
       "32188            12.975377             77.696664                   13.005377   \n",
       "20791            11.003681             76.975525                   11.013681   \n",
       "17948            11.026117             76.944652                   11.136117   \n",
       "21726            19.875908             75.358888                   19.985908   \n",
       "\n",
       "       Delivery_location_longitude Type_of_order Type_of_vehicle  \\\n",
       "17281                     0.090000        Snack      motorcycle    \n",
       "32188                    77.726664       Drinks      motorcycle    \n",
       "20791                    76.985525         Meal         scooter    \n",
       "17948                    77.054652        Snack      motorcycle    \n",
       "21726                    75.468888       Buffet         scooter    \n",
       "\n",
       "       Time_taken(min)  \n",
       "17281               24  \n",
       "32188               23  \n",
       "20791               21  \n",
       "17948               25  \n",
       "21726               28  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading dataset\n",
    "url = 'https://raw.githubusercontent.com/ataislucky/Data-Science/main/dataset/food_delivery.txt'\n",
    "data = pd.read_csv(url)\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149edf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "time_series_dataframe = pd.DataFrame({\n",
    "    \"feature_0\": [0., 0., 0., 1., 2., 3.],\n",
    "    \"feature_1\": [10., 20., 30., 40., 50., 60.],\n",
    "    \"feature_2\": [-4., -5., -6., -7., -8., -9.]\n",
    "    },\n",
    "    index=[\n",
    "        \"2022-01-01 00:00:00\",\n",
    "        \"2022-01-01 01:00:00\",\n",
    "        \"2022-01-01 02:00:00\",\n",
    "        \"2022-01-01 03:00:00\",\n",
    "        \"2022-01-01 04:00:00\",\n",
    "        \"2022-01-01 05:00:00\",\n",
    "    ]\n",
    ")\n",
    "time_series_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8543dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_chunk_time_series_dataframe = pd.DataFrame({\n",
    "    \"dim_0\": [\n",
    "        pd.Series([0., 0., 0.]),\n",
    "        pd.Series([1., 2., 3.])\n",
    "    ],\n",
    "    \"dim_1\": [\n",
    "        pd.Series([10., 20., 30.]),\n",
    "        pd.Series([40., 50., 60.])\n",
    "    ],\n",
    "    \"dim_2\": [\n",
    "        pd.Series([-4., -5., -6.]),\n",
    "        pd.Series([-7., -8., -9.])\n",
    "    ],\n",
    "},\n",
    "    index=[\"frame_0\", \"frame_1\"]\n",
    ")\n",
    "windowed_chunk_time_series_dataframe_label = pd.DataFrame({\n",
    "        \"y\": [0, 1]\n",
    "    },\n",
    "    index=[\"frame_0\", \"frame_1\"]\n",
    ")\n",
    "\n",
    "windowed_chunk_time_series_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485c62d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.panel.compose import ColumnConcatenator\n",
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "steps = [\n",
    "  ('concatenate', ColumnConcatenator()),\n",
    "  ('classify', TimeSeriesForestClassifier(**config['model_params']))\n",
    "]\n",
    "clf = Pipeline(steps)\n",
    "\n",
    "input_dataframe = pd.DataFrame({\n",
    "    \"dim_0\": [\n",
    "        pd.Series([0., 0., 0.]),\n",
    "        pd.Series([1., 2., 3.])\n",
    "    ],\n",
    "    \"dim_1\": [\n",
    "        pd.Series([10., 20., 30.]),\n",
    "        pd.Series([40., 50., 60.])\n",
    "    ],\n",
    "    \"dim_2\": [\n",
    "        pd.Series([-4., -5., -6.]),\n",
    "        pd.Series([-7., -8., -9.])\n",
    "    ],\n",
    "},\n",
    "    index=[\"frame_0\", \"frame_1\"]\n",
    ")\n",
    "\n",
    "concatenator = ColumnConcatenator()\n",
    "\n",
    "output_dataframe = concatenator.fit_transform(input_dataframe)\n",
    "\n",
    "expected_output_dataframe = pd.DataFrame({\n",
    "    0: [\n",
    "        pd.Series([0., 0., 0., 10., 20., 30., -4., -5., -6.]),\n",
    "        pd.Series([1., 2., 3., 40., 50., 60., -7., -8., -9.])\n",
    "    ]\n",
    "},\n",
    "    index=[\"frame_0\", \"frame_1\"]\n",
    ")\n",
    "expected_output_dataframe.index.names = [\"instances\"]\n",
    "\n",
    "pd.testing.assert_frame_equal(output_dataframe, expected_output_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "448deb1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'function_parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 524\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m score_dict\n\u001b[0;32m    523\u001b[0m \u001b[38;5;66;03m#tt_cv_pt_models_ts\u001b[39;00m\n\u001b[1;32m--> 524\u001b[0m ts_model_dict \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_parser\u001b[49m(function_name\u001b[38;5;241m=\u001b[39mtt_cv_pt_models_ts, search_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type ==\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mts_model_dict:\u001b[39m\u001b[38;5;124m'\u001b[39m, ts_model_dict)\n\u001b[0;32m    528\u001b[0m \u001b[38;5;66;03m#1 koad data\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'function_parser' is not defined"
     ]
    }
   ],
   "source": [
    "#v1.3 testing ts_forecast_models with ForecastingPipeline /Mar15,2023\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_ts_pipeline(data, pp_steps_dict, pp_steps_list):\n",
    "    '''\n",
    "    1.1 Jan 29,2023 added index chk,multi index\n",
    "    1.0 Jan 23,2023 initial ver\n",
    "    \n",
    "    TBA\n",
    "    TransformedTargetForecaster-takes a chain of transformers and forecasters\n",
    "    MultiplexForecaster\n",
    "    TSFreshFeatureExtractor\n",
    "    WindowSummarizer: mean/stdev/n_legs features\n",
    "    DateTimeFeatures: date features\n",
    "    \n",
    "    TBC\n",
    "    diff between TransformedTargetForecaster and ForecastingPipeline\n",
    "    \n",
    "    Ref\n",
    "    https://towardsdatascience.com/why-start-using-sktime-for-forecasting-8d6881c0a518\n",
    "    '''\n",
    "    \n",
    "    from sktime.forecasting.compose import ForecastingPipeline\n",
    "    from sktime.transformations.series.outlier_detection import HampelFilter\n",
    "    from sktime.transformations.series.boxcox import LogTransformer, BoxCoxTransformer\n",
    "    from sktime.transformations.series.adapt import TabularToSeriesAdaptor\n",
    "    from sktime.transformations.series.impute import Imputer\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sktime.forecasting.theta import ThetaForecaster\n",
    "    \n",
    "    print('Entry create_ts_pipeline')\n",
    "    #mandetory check\n",
    "    if len(df1.index.levels) >=3:\n",
    "        ts_type = 'HierarchicalIndex_TS'\n",
    "    elif len(df1.index.levels) ==3:\n",
    "        pass\n",
    "    elif len(df1.index.levels) ==2:\n",
    "        pass\n",
    "    elif len(df1.index.levels) ==1:\n",
    "        pass\n",
    "    \n",
    "    if ts_type == 'HierarchicalIndex_TS':\n",
    "        from sktime.transformations.hierarchical.aggregate import Aggregator\n",
    "        aggregator = Aggregator()\n",
    "    else:\n",
    "        aggregator = 'passthrough'\n",
    "        \n",
    "    \n",
    "    from sktime.forecasting.arima import ARIMA\n",
    "    from sktime.forecasting.compose import TransformedTargetForecaster\n",
    "    from sktime.transformations.series.detrend import Deseasonalizer\n",
    "#     forecaster = TransformedTargetForecaster(\n",
    "#     [\n",
    "#         (\"deseasonalize\", Deseasonalizer(model=\"multiplicative\", sp=12)),\n",
    "#         (\"forecast\", ARIMA()),\n",
    "#     ]\n",
    "#     )\n",
    "\n",
    "    \n",
    "    if pp_steps_dict:\n",
    "        #2.1 ImputerCat/ImputerNum\n",
    "        if 'Outlier' in pp_steps_dict:\n",
    "            if pp_steps_dict['Outlier'] == 'HampelFilter':\n",
    "                outlier = HampelFilter()\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            outlier = 'passthrough'\n",
    "            \n",
    "        \n",
    "        if 'Transformer' in pp_steps_dict:\n",
    "            if pp_steps_dict['Transformer'] == 'LogTransformer':\n",
    "                transformer = LogTransformer()\n",
    "            elif pp_steps_dict['Transformer'] == 'BoxCoxTransformer':\n",
    "                transformer = BoxCoxTransformer(sp=4)\n",
    "            else:\n",
    "                transformer = 'passthrough'\n",
    "    \n",
    "    #5 optional pp_steps_list\n",
    "    print('\\n ##5 pp_steps_list: setup flags MatchFeat, DropNullRows, DropDupRows, DropDupCols, DropQuasiConstCols')\n",
    "    if pp_steps_list:\n",
    "        if 'FeatureExtraction' in pp_steps_list:\n",
    "            from sktime.transformations.panel.tsfresh import TSFreshFeatureExtractor\n",
    "            FeatureExtraction = TSFreshFeatureExtractor(default_fc_parameters=\"efficient\", show_warnings=False)\n",
    "        else:\n",
    "            FeatureExtraction = 'passthrough'\n",
    "            \n",
    "            \n",
    "            \n",
    "    pipeline = ForecastingPipeline(\n",
    "    [\n",
    "    (\"imputer\", Imputer(method=\"mean\")),\n",
    "    (\"minmaxscaler\", TabularToSeriesAdaptor(MinMaxScaler())),\n",
    "    ('aggregator', aggregator), # for Hierarchical TS\n",
    "    (\"outlier\", outlier),\n",
    "    (\"transformer\", transformer),\n",
    "    (\"model\", ThetaForecaster(sp=4)) #defaut otherwise error\n",
    "    ])\n",
    "    \n",
    "    print('Exit create_ts_pipeline')\n",
    "    return pipeline\n",
    "     \n",
    "def custom_mape(y: np.array, y_pred: np.array):\n",
    "    import numpy as np\n",
    "    metrics_dict = {'score_mean': np.mean(np.abs((y - np.ceil(y_pred)) / y)),\n",
    "                    'score_row': np.abs((y - np.ceil(y_pred)) / y)}\n",
    "    return metrics_dict\n",
    "\n",
    "def set_index(data, index_cols_list=[]):\n",
    "    '''\n",
    "    index_type = Int64Index, RangeIndex, DatetimeIndex, PeriodIndex\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    \n",
    "    df = data.copy()\n",
    "    index = pd.MultiIndex.from_frame(df[index_cols_list])\n",
    "    df.set_index(index, drop=True, inplace=True)\n",
    "    df.drop(index_cols_list, axis=1, inplace=True)\n",
    "    \n",
    "    #check duplicate values\n",
    "    if df.index.has_duplicates:\n",
    "        pass\n",
    "    \n",
    "    #check index level eg. no of features in index\n",
    "    if len(index.levels)==1:\n",
    "        ts_type = 'NormalIndex_TS'\n",
    "    elif len(index.levels)==2:\n",
    "        ts_type = 'MultiIndex_TS'\n",
    "    elif len(index.levels)>=3:\n",
    "        ts_type = 'HierarchicalIndex_TS'\n",
    "    print('ts_type:', ts_type)\n",
    "    return df\n",
    "\n",
    "#TBA\n",
    "# from sklearn.multioutput import MultiOutputRegressor\n",
    "# from sklearn.linear_model import Ridge\n",
    "# X, y = load_linnerud(return_X_y=True)\n",
    "# regr = MultiOutputRegressor(Ridge(random_state=123)).fit(X, y)\n",
    "# regr.predict(X[[0]])\n",
    "\n",
    "def tt_cv_pt_models_ts(data, target_col, feat_list, model_type, pipeline, index_cols_list=[],\n",
    "        nested_params=None, train_type='TrainTest', forecast_type = 'regression',\n",
    "        forecast_period=5, freq='M', regression_window=10, cv_type='ExpandingWindowSplitter',\n",
    "        pt_type='ForecastingRandomizedSearch', transformer_type='BoxCoxTransformer', \n",
    "        aggregation_transformer=True, PlotSeries=False):\n",
    "    '''\n",
    "    v1.4 Mar15,2023: Added forecast_type\n",
    "    v1.3 Jan26,2023: Updated for new model_type/ added probabilistic predictions support\n",
    "    v1.2 Jan23,2023: Updated for pipeline\n",
    "    v1.1 Jan20,2023: added regression_window, cv_type, pt_type, forecasting score/CrossValidation\n",
    "    v1.0 Jan18,2023: initial version\n",
    "    \n",
    "    ts_type: UniVariate/MultiVariate\n",
    "    forecast_type: regression/binary_classification/multi_classification\n",
    "    \n",
    "    pipeline:if pipeline true then model_type (model) will update in pipeline that will be fitted else model_type will be fitted\n",
    "    train_type: TrainTest/CrossValidation/GridSearch/RandomizedSearch\n",
    "    freq: used to set index whern data is as df with date col\n",
    "    regression_window: when regression models are used\n",
    "    cv_type: used in cross_validation/GridSerach - ExpandingWindowSplitter/SlidingWindowSplitter\n",
    "    pt_type: used in parameter tuning - ForecastingGridSearch/ForecastingRandomizedSearch\n",
    "    PlotSeries: does not support for MultiIndex & Cross Validation\n",
    "    aggregation_transformer : for multi index\n",
    "    \n",
    "    #On Memory testing dataset:\n",
    "    from sktime.datasets import load_airline\n",
    "    series_PeriodIndex_UV = load_airline()\n",
    "\n",
    "    from sktime.datatypes import get_examples\n",
    "    array_1D = get_examples(mtype=\"np.ndarray\", as_scitype=\"Series\")[0]\n",
    "    array_2D = get_examples(mtype=\"np.ndarray\", as_scitype=\"Series\")[1]\n",
    "    array_3D = get_examples(mtype=\"numpy3D\", as_scitype=\"Panel\")[0]\n",
    "    series_NormalIndex_UV = get_examples(mtype=\"pd.Series\", as_scitype=\"Series\")[0]\n",
    "    df_UniIndex_UV = get_examples(mtype=\"pd.DataFrame\")[0] #ok\n",
    "    df_UniIndex_MV = get_examples(mtype=\"pd.DataFrame\")[1]\n",
    "    df_MultiIndex_MV = get_examples(mtype=\"pd-multiindex\")[0]\n",
    "    df_MultiIndexHier_MV = get_examples(mtype=\"pd_multiindex_hier\")[0]\n",
    "    df_MultiIndexHier_UV = get_examples(mtype=\"pd_multiindex_hier\")[1]\n",
    "    \n",
    "\n",
    "    from sktime.utils._testing.hierarchical import _make_hierarchical\n",
    "    df_MultiIndexHier_UV = _make_hierarchical() #ok\n",
    "    \n",
    "    Test issues\n",
    "    Auto_Arima with cross_validation TypeError\n",
    "    TypeError: input must be a one of (<class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.series.Series'>), but found type: <class 'float'>\n",
    "    \n",
    "    TBA:\n",
    "    MultiOutputRegressor\n",
    "    '''\n",
    "    print('entry tt_cv_pt_models_ts')\n",
    "    \n",
    "    #import libs\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sktime.forecasting.base import ForecastingHorizon\n",
    "    from sktime.utils.plotting import plot_series\n",
    "    from sktime.forecasting.compose import make_reduction\n",
    "    from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "    \n",
    "    from sktime.performance_metrics.forecasting import mean_absolute_percentage_error\n",
    "    from sklearn.metrics import accuracy_score\n",
    "        \n",
    "    print('\\n ##1 index set single/multi/hierarchical')\n",
    "    print(f'data-typ:{type(data)}, data-shape:{data.shape}')\n",
    "    if len(index_cols_list) ==1: # single level index\n",
    "        print('executing single index')\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            if freq:\n",
    "                #Note1: convert partial str date(yyyy-mm or yy-q) to complete date format(yyyy-mm-dd)\n",
    "                data[index_col] = pd.PeriodIndex(data[index_col], freq=freq).to_timestamp()\n",
    "                #Note2: convert complete date format(yyyy-mm-dd) to date index w.r.t freq (yyyyqq)\n",
    "                data = data.set_index(index_col, drop=True).to_period(freq) #set index of \n",
    "            else:\n",
    "                data.set_index(index_col, drop=True, inplace=True) # convert to series\n",
    "    else: # multi level index for series/df\n",
    "        print('executing MultiLeval index')\n",
    "        #data = set_index(data, index_cols_list) #set hierarchical index/multi_index\n",
    "        index = pd.MultiIndex.from_frame(data[index_cols_list])\n",
    "        data.set_index(index, drop=True, inplace=True)\n",
    "        data.drop(index_cols_list, axis=1, inplace=True)\n",
    "          \n",
    "    print('#1.1 index_type check PeriodIndex/RangeIndex/MultiIndex')\n",
    "    #Note: pandas index types: Int64Index, RangeIndex, DatetimeIndex, PeriodIndex;\n",
    "    #Note: if DatetimeIndex, the freq attribute must be set.\n",
    "    \n",
    "    import pandas\n",
    "    if isinstance(data.index, pandas.core.indexes.period.PeriodIndex):\n",
    "        index_type = 'PeriodIndex'\n",
    "    elif isinstance(data.index, pandas.core.indexes.range.RangeIndex):\n",
    "        index_type = 'RangeIndex'\n",
    "    elif isinstance(data.index, pandas.core.indexes.multi.MultiIndex):\n",
    "        index_type = 'MultiIndex'\n",
    "        #check index level eg. no of features in index\n",
    "        if len(data.index.levels)==1:\n",
    "            index_type = 'MultiIndexUni'\n",
    "        elif len(data.index.levels)==2:\n",
    "            index_type = 'MultiIndexPanel'\n",
    "        elif len(data.index.levels)>=3:\n",
    "            index_type = 'MultiIndexHier'\n",
    "            \n",
    "        #note: pd_multiindex_hier format to store node-wise aggregates.add __total @ each node\n",
    "        if aggregation_transformer:\n",
    "            from sktime.transformations.hierarchical.aggregate import Aggregator\n",
    "            data = Aggregator().fit_transform(data)\n",
    "            \n",
    "    elif isinstance(data.index, pandas.core.indexes.base.Index): #TBC\n",
    "        index_type = 'NormalIndex'\n",
    "    print('index_type:', index_type)\n",
    "        \n",
    "    print('#1.2 ts_type check UniVariate vs MultiVariate')\n",
    "    if isinstance(target_col, str):\n",
    "        ts_type = 'UniVariate'\n",
    "    elif isinstance(data, pd.Series):\n",
    "        ts_type = 'UniVariate'\n",
    "    elif isinstance(target_col, list):\n",
    "        if len(target_col) == 1:\n",
    "            ts_type = 'UniVariate'\n",
    "        elif len(target_col) > 1:\n",
    "            ts_type = 'MultiVariate'\n",
    "    \n",
    "    print('ts_type:', ts_type)\n",
    "        \n",
    "        \n",
    "    print('\\n ##2 data chk, split train/valid, chk UniVariate/MultiVariate:', type(data))\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        all_feat_list = data.columns.tolist()\n",
    "        sel_feat_list = feat_list #passed thr function\n",
    "        print(f'all_feat_list:{all_feat_list}, sel_feat_list:{sel_feat_list}')\n",
    "        \n",
    "        if len(sel_feat_list) > 1: # when external feat_list passed\n",
    "            print('executing: if len(sel_feat_list) > 1')\n",
    "            X = data[sel_feat_list]\n",
    "            y = data[target_col]\n",
    "            y_train, y_valid, X_train, X_valid = temporal_train_test_split(y, X, test_size=0.2)\n",
    "            print(f'df-y_train:{y_train.shape}, y_valid:{y_valid.shape}, X_train:{X_train.shape}, X_valid:{X_valid.shape}')\n",
    "        \n",
    "        elif (len(all_feat_list) == 1) or (len(sel_feat_list) == 1):\n",
    "            print('executing: (len(all_feat_list) == 1) or (len(sel_feat_list) == 1)')\n",
    "            X = None\n",
    "            y = data\n",
    "            y_train, y_valid = temporal_train_test_split(y, test_size=0.2)\n",
    "            \n",
    "        else:\n",
    "            print('executing else for train/test')\n",
    "            X = None\n",
    "            y = data[target_col]\n",
    "            y_train, y_valid = temporal_train_test_split(y, test_size=0.2)\n",
    "            print(f'df-srs-y_train:{y_train.shape}, y_valid:{y_valid.shape}')\n",
    "            \n",
    "    elif isinstance(data, pd.Series):\n",
    "        X = None\n",
    "        y = data\n",
    "        y_train, y_valid = temporal_train_test_split(y, test_size=0.2)\n",
    "        print(f'srs-y_train:{y_train.shape}, y_valid:{y_valid.shape}')\n",
    "       \n",
    "    print('\\n ##3 set target transformer') #Not needed if pipeline is passed\n",
    "    if transformer_type:\n",
    "        if transformer_type == 'BoxCoxTransformer':\n",
    "            from sktime.transformations.series.boxcox import BoxCoxTransformer\n",
    "            transformer = BoxCoxTransformer(sp=4) #Box-Cox power transformation\n",
    "        elif transformer_type == 'LogTransformer':\n",
    "            from sktime.transformations.series.boxcox import LogTransformer\n",
    "            transformer = LogTransformer()\n",
    "        y_train = transformer.fit_transform(y_train)\n",
    "        \n",
    "    print('\\n ##4 create score dict')\n",
    "    score_dict = {}\n",
    "    score_dict['ts_type'] = ts_type\n",
    "    score_dict['model_type'] = model_type\n",
    "    score_dict['index_type'] = index_type\n",
    "    score_dict['forecast_type'] = forecast_type\n",
    "    \n",
    "    print('\\n ##5 get ForecastingHorizon w.r.t y_valid:', len(y_valid))\n",
    "    #fh_abs = ForecastingHorizon(y_valid.index, is_relative=False)\n",
    "    #print('fh_abs:',fh_abs) # y_valid index\n",
    "    \n",
    "    #note: fh_re list should same size of y_valid so pred/y_valid will be same size/calculate score\n",
    "    #note: Multi index split will have record from each first index, chk split y_train/y_valid/y/pred to undersatnd\n",
    "    #eg.  df_MultiIndex_MV: if from first level index of 4 window will be 4 records but actual will be 1 record from each window to predict\n",
    "    if 'Multi' in index_type:\n",
    "        fh_re = [1] #quick fix/TBC\n",
    "    else:\n",
    "        fh_re = [a for a in range(1, len(y_valid)+1)]\n",
    "    \n",
    "    print(f'y_valid-len:{len(y_valid)}, fh_re-len:{len(fh_re)}, fh_re:{fh_re}')\n",
    "    \n",
    "    print(f'\\n ##6 executing model:{model_type},  forecast_type:{forecast_type}')\n",
    "    if forecast_type == 'regression':\n",
    "        if model_type == 'NaiveForecaster':\n",
    "            #Forecast based on naive assumptions about past trends continuing, supports univariate/multivariate\n",
    "            from sktime.forecasting.naive import NaiveForecaster, NaiveVariance\n",
    "            forecaster = NaiveForecaster()\n",
    "\n",
    "            #NaiveVariance adds to a `forecaster` the ability to compute the prediction variance based on naive assumptions about ts.\n",
    "            #forecaster = NaiveForecaster(strategy=\"drift\")\n",
    "            #forecaster = NaiveVariance(forecaster)\n",
    "        elif model_type == 'PolynomialTrend':\n",
    "            #linear regression model with a 1st degree polynomial transformation of the feature.\n",
    "            from sktime.forecasting.trend import PolynomialTrendForecaster\n",
    "            forecaster = PolynomialTrendForecaster(degree=1)\n",
    "        elif model_type == 'AutoArima':\n",
    "            from sktime.forecasting.arima import AutoARIMA\n",
    "            forecaster = AutoARIMA(sp=4)\n",
    "            #forecaster = AutoARIMA(sp=12, suppress_warnings=True)\n",
    "        elif model_type == 'ThetaForecaster':\n",
    "            #it's equivalent to simple exponential smoothing (SES) with drift/tested for seasonality\n",
    "            from sktime.forecasting.theta import ThetaForecaster\n",
    "            forecaster = ThetaForecaster(sp=12)\n",
    "\n",
    "        elif model_type == 'ExponentialSmoothing':\n",
    "            from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "            forecaster = ExponentialSmoothing(trend=\"add\", sp=12)\n",
    "            \n",
    "        elif model_type == 'BATS':\n",
    "            from sktime.forecasting.bats import BATS\n",
    "            forecaster = BATS(sp=12, use_trend=True, use_box_cox=False)\n",
    "        elif model_type == 'TBATS':\n",
    "            from sktime.forecasting.tbats import TBATS\n",
    "            forecaster = TBATS(sp=12, use_trend=True, use_box_cox=False)\n",
    "        elif model_type == 'UnobservedComponents':\n",
    "            from sktime.forecasting.structural import UnobservedComponents\n",
    "            forecaster = UnobservedComponents(\n",
    "                level=\"local linear trend\", freq_seasonal=[{\"period\": 12, \"harmonics\": 10}])\n",
    "        elif model_type == 'StatsForecastAutoARIMA':\n",
    "            from sktime.forecasting.statsforecast import StatsForecastAutoARIMA\n",
    "            forecaster = StatsForecastAutoARIMA(sp=12)\n",
    "    \n",
    "        else:\n",
    "            print(f'#executing Regression:{model_type}')\n",
    "            if regression_window:\n",
    "                '''\n",
    "                strategy=\n",
    "                Direct:create a separate model for each period we are forecasting.\n",
    "                Recursive:fit a single one-step ahead model,previous time steps output for the following input\n",
    "                Multiple outputs: one model is used to predict the entire time series horizon in a single forecast.\n",
    "                '''\n",
    "                forecaster = make_reduction(model_type, strategy=\"recursive\", window_length= regression_window)\n",
    "                #forecaster = ReducedRegressionForecaster(regressor, window_length=12)\n",
    "\n",
    "    elif forecast_type in ['binary_classification', 'multi_classification']:   \n",
    "        if model_type == 'TimeSeriesForestClassifier': \n",
    "            from sktime.classification.compose import TimeSeriesForestClassifier\n",
    "            forecaster = TimeSeriesForestClassifier()\n",
    "        \n",
    "   \n",
    "    \n",
    "    print('Executing pipeline:', pipeline)\n",
    "    if pipeline:\n",
    "        print('\\n ##7 add model last step of pipeline')    \n",
    "        model_name = forecaster.__class__.__name__\n",
    "\n",
    "        if pipeline.steps[-1][0] == 'model':\n",
    "            pipeline.steps.pop(-1)\n",
    "            pipeline.steps.append(('model', forecaster))\n",
    "        else:\n",
    "            pipeline.steps.append(('model', forecaster))\n",
    "            \n",
    "        from copy import deepcopy\n",
    "        forecaster = deepcopy(pipeline)\n",
    "    print('forecaster:', forecaster)\n",
    "    \n",
    "    print('\\n ##8 setup cv w.r.t cv_type:', cv_type)\n",
    "    from sktime.forecasting.model_selection import SlidingWindowSplitter, ExpandingWindowSplitter\n",
    "    from sktime.forecasting.model_selection import SingleWindowSplitter\n",
    "    \n",
    "    ##X_train = None if ts_type == 'UniVariate' else X_train # multivarite X_train will be used\n",
    "    \n",
    "    window_size = round(y_train.size/4) #TBC\n",
    "    if cv_type == 'ExpandingWindowSplitter':\n",
    "         #note: initial_window + fh shold be smaller than the length of y\n",
    "        cv = ExpandingWindowSplitter(step_length=window_size, fh=list(range(1, window_size)), initial_window=window_size)\n",
    "    elif cv_type == 'SlidingWindowSplitter':\n",
    "        #note: initial_window must greater than `window_length`\n",
    "        cv = SlidingWindowSplitter(initial_window=window_size*2, window_length=window_size)\n",
    "    elif cv_type == 'SingleWindowSplitter':\n",
    "        #note: initial_window must greater than `window_length`\n",
    "        #validation_size = 26\n",
    "        validation_size = y_train.size\n",
    "        cv = SingleWindowSplitter(window_length=len(y)-validation_size, fh=validation_size)\n",
    "        \n",
    "        \n",
    "    print('\\n ##9 forecaster fit and predict w.r.t train_type:', train_type, ts_type)\n",
    "    print(f'shape y_train:{y_train.shape}, y_valid:{y_valid.shape}')\n",
    "    print(f'type y_train:{type(y_train)}, y_valid:{type(y_valid)}')\n",
    "    print(f'len y_train:{len(y_train)}, y_valid:{len(y_valid)}, fh_re:{len(fh_re)}')\n",
    "    \n",
    "    \n",
    "    if train_type == 'TrainTest':\n",
    "        if len(sel_feat_list) > 1: #when features X there \n",
    "            forecaster.fit(y_train, X_train)\n",
    "            #y_pred = forecaster.predict(X=X_valid, fh=fh_abs)\n",
    "            y_pred = forecaster.predict(X=X_valid, fh=fh_re)\n",
    "            #print(f'y_pred-shp:{y_pred.shape}')\n",
    "        else:\n",
    "            forecaster.fit(y_train)\n",
    "            y_pred = forecaster.predict(fh=fh_re)\n",
    "            \n",
    "        print('\\ny_pred:', y_pred)\n",
    "        print('\\n\\ny_valid:', y_valid)\n",
    "        print(f'y_pred-shp:{y_pred.shape}, y_valid:{y_valid.shape}')\n",
    "        \n",
    "#     if train_type == 'TrainTest':\n",
    "#         if ts_type == 'UniVariate':\n",
    "#             forecaster.fit(y_train)\n",
    "#             y_pred = forecaster.predict(fh=fh_re)\n",
    "#         elif ts_type == 'MultiVariate':\n",
    "#             print(f'X_train:{X_train.shape}, X_valid:{X_valid.shape}')\n",
    "#             forecaster.fit(y_train, X_train)\n",
    "#             #y_pred = forecaster.predict(X=X_valid, fh=fh_abs)\n",
    "#             y_pred = forecaster.predict(X=X_valid, fh=fh_re)\n",
    "            \n",
    "    elif train_type == 'CrossValidation':\n",
    "        from sktime.forecasting.model_evaluation import evaluate\n",
    "        pred_df = evaluate(forecaster=forecaster, X=X_train, y=y_train, cv=cv, strategy=\"refit\", return_data=True)\n",
    "        \n",
    "    elif train_type in ['GridSearch', 'RandomizedSearch']:\n",
    "        from sktime.forecasting.model_selection import ForecastingGridSearchCV, ForecastingRandomizedSearchCV\n",
    "        if train_type == 'GridSearch':\n",
    "            forecaster = ForecastingGridSearchCV(forecaster, strategy=\"refit\", cv=cv, param_grid=nested_params)\n",
    "        elif train_type == 'RandomizedSearch':\n",
    "            forecaster = ForecastingRandomizedSearchCV(forecaster, strategy=\"refit\", cv=cv, \n",
    "                        param_distributions=nested_params, n_iter=1, random_state=42)\n",
    "            \n",
    "        if ts_type == 'UniVariate':\n",
    "            forecaster.fit(y_train)\n",
    "            y_pred = forecaster.predict(fh=fh_re)\n",
    "            #y_pred = forecaster.predict(np.arange(1, y_valid.size+1))\n",
    "        elif ts_type == 'MultiVariate':\n",
    "            print(f'X_train:{X_train.shape}, X_valid:{X_valid.shape}')\n",
    "            forecaster.fit(y_train, X_train)\n",
    "            y_pred = forecaster.predict(X=X_valid, fh=fh_re)\n",
    "            \n",
    "    print('\\n ##10 performance_metrics - forecasting score w.r.t:', train_type)\n",
    "    from sktime.performance_metrics.forecasting import make_forecasting_scorer\n",
    "    if train_type in ['TrainTest', 'GridSearch', 'RandomizedSearch']:\n",
    "        print(f'y_valid:{len(y_valid)}, y_pred:{len(y_pred)} ')\n",
    "        \n",
    "        if forecast_type in ['binary_classification', 'multi_classification']:\n",
    "            acc = accuracy_score(y_valid, y_pred)\n",
    "            score_dict['acc'] = acc\n",
    "            \n",
    "        if forecast_type in ['regression']:     \n",
    "            mape = mean_absolute_percentage_error(y_valid, y_pred, symmetric=False)\n",
    "            score_dict['mape'] = mape\n",
    "        \n",
    "        score_dict['y_pred'] = y_pred\n",
    "        score_dict['pipeline'] = pipeline\n",
    "        \n",
    "    elif train_type in ['CrossValidation']:\n",
    "        score_dict['pred_df'] = pred_df\n",
    "    print('score_dict:', score_dict)\n",
    "    \n",
    "    print ('\\n ##11 check probabilistic predictions support')\n",
    "    if forecaster.get_tag(\"capability:pred_int\"):\n",
    "        forecast_intervals = forecaster.predict_interval(coverage=0.9)\n",
    "        forecast_quantiles = forecaster.predict_quantiles(fh=None, X=None, alpha=[0.05, 0.95])\n",
    "        forecast_variance = forecaster.predict_var(fh=None, X=None, cov=False)\n",
    "        #forecast_proba = forecaster.predict_proba(fh=None, X=None, marginal=True)\n",
    "        score_dict['forecast_intervals'] = forecast_intervals\n",
    "        score_dict['forecast_quantiles'] = forecast_quantiles\n",
    "        score_dict['forecast_variance'] = forecast_variance\n",
    "        #score_dict['forecast_proba'] = forecast_proba #TBC\n",
    "        \n",
    "    #PlotSeries = False # temp dueto train_type == 'CrossValidation' /MultiIndex\n",
    "    if PlotSeries:\n",
    "        print(f'\\n ##12 PlotSeries w.r.t :{train_type}')\n",
    "        if train_type in ['TrainTest', 'GridSearch', 'RandomizedSearch']:\n",
    "            print(f'df-y_train:{y_train.shape}, y_valid:{y_valid.shape}, y_pred:{y_pred.shape}')\n",
    "            plot_series(y_train, y_valid, y_pred, labels=[\"y_train\", \"y_valid\", \"y_pred\"]);\n",
    "        elif train_type == 'CrossValidation':\n",
    "            n_cv = pred_df.shape[0]\n",
    "            plot_series(\n",
    "                y, *[pred_df[\"y_pred\"].iloc[x] for x in range(n_cv)],\n",
    "                markers=[\"o\", *[\".\"]*n_cv],\n",
    "                labels=[\"y_true\"] + [f\"cv: {x}\" for x in range(n_cv)]\n",
    "            );\n",
    "        \n",
    "    print('exit tt_cv_pt_models_ts')\n",
    "    return score_dict\n",
    "\n",
    "#tt_cv_pt_models_ts\n",
    "ts_model_dict = function_parser(function_name=tt_cv_pt_models_ts, search_string='model_type ==')\n",
    "print('ts_model_dict:', ts_model_dict)\n",
    "\n",
    "\n",
    "#1 koad data\n",
    "from sktime.datasets import load_airline\n",
    "series_UniIndex_UV = load_airline()#ok\n",
    "\n",
    "#2\n",
    "from sktime.datatypes import get_examples\n",
    "df_UniIndex_UV = get_examples(mtype=\"pd.DataFrame\")[0] #ok\n",
    "df_UniIndex_MV = get_examples(mtype=\"pd.DataFrame\")[1]#ok\n",
    "df_MultiIndex_MV = get_examples(mtype=\"pd-multiindex\")[0]#ok\n",
    "df_MultiIndexHier_MV = get_examples(mtype=\"pd_multiindex_hier\")[0]#ok\n",
    "df_MultiIndexHier_UV = get_examples(mtype=\"pd_multiindex_hier\")[1]\n",
    "    \n",
    "from sktime.utils._testing.hierarchical import _make_hierarchical\n",
    "df_MultiIndexHier_UV = _make_hierarchical() #ok\n",
    "\n",
    "#3\n",
    "# from sktime.datasets import load_longley\n",
    "# from sktime.forecasting.var import VAR\n",
    "# _, y = load_longley()\n",
    "# y = y.drop(columns=[\"UNEMP\", \"ARMED\", \"POP\"]) #ok\n",
    "# data = y\n",
    "# target_col = ['GNPDEFL', 'GNP']\n",
    "\n",
    "train_type = 'TrainTest'\n",
    "forecast_type = 'regression'\n",
    "index_cols_list = []\n",
    "\n",
    "data = df_MultiIndexHier_UV\n",
    "print(f'data-typ:{type(data)}, data-sz:{data.shape}, data-cols:{data.columns.tolist()}')\n",
    "target_col =['var_0', 'var_1']\n",
    "\n",
    "print(f'data-typ:{type(data)}, data-sz:{data.shape}')\n",
    "\n",
    "#target_col = 'var_1'\n",
    "#feat_list = ['var_0']\n",
    "\n",
    "#target_col = 'a'\n",
    "\n",
    "feat_list = []\n",
    "model_type = 'NaiveForecaster'\n",
    "pipeline = None\n",
    "\n",
    "score_dict = tt_cv_pt_models_ts(data, target_col, feat_list, model_type, pipeline,\n",
    "        index_cols_list=index_cols_list,\n",
    "        nested_params=None, train_type=train_type, forecast_type = forecast_type,\n",
    "        forecast_period=5, freq='M', regression_window=10, cv_type='ExpandingWindowSplitter',\n",
    "        pt_type='ForecastingRandomizedSearch', transformer_type='BoxCoxTransformer', \n",
    "        PlotSeries=False)\n",
    "\n",
    "score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700849be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305b04c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.datasets import load_longley\n",
    "#from sktime.forecasting.compose import ReducedRegressionForecaster\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.performance_metrics.forecasting import mean_absolute_percentage_error\n",
    "#from sktime.pipeline import Pipeline\n",
    "# from sktime.transformers.series_as_features.compose import FeatureUnion\n",
    "# from sktime.transformers.series_as_features.reduce import Tabularizer\n",
    "# from sktime.transformers.series_as_features.segment import RandomIntervalSegmenter\n",
    "# from sktime.transformers.single_series.adapt import SingleSeriesTransformAdaptor\n",
    "# from sktime.transformers.single_series.detrend import Deseasonalizer, Detrender\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "y, X = load_longley(y_name ='TOTEMP')\n",
    "X_train, X_test, y_train, y_test = temporal_train_test_split(X, y, test_size=10)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a0d15e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty\n",
      "univariate\n",
      "equally_spaced\n",
      "no null\n"
     ]
    }
   ],
   "source": [
    "#validation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y = np.array([1, 6, 3, 7, 2])\n",
    "y\n",
    "\n",
    "# check_raise\n",
    "from sktime.datatypes import check_raise\n",
    "\n",
    "#check_raise(y, mtype=\"np.ndarray\")\n",
    "check_raise(y.reshape(-1, 1), mtype=\"np.ndarray\")\n",
    "\n",
    "# check_is_mtype\n",
    "from sktime.datatypes import check_is_mtype\n",
    "\n",
    "metadata_tuple = check_is_mtype(y, mtype=\"np.ndarray\", return_metadata=True)\n",
    "#check_is_mtype(y, mtype=\"np.ndarray\", return_metadata=False)\n",
    "#check_is_mtype(y.reshape(-1, 1), mtype=\"np.ndarray\", return_metadata=True)\n",
    "\n",
    "metadata_dict = metadata_tuple[2]\n",
    "if not metadata_dict['is_empty']:\n",
    "    print('empty')\n",
    "if metadata_dict['is_univariate']:\n",
    "    print('univariate')\n",
    "if metadata_dict['is_equally_spaced']:\n",
    "    print('equally_spaced')\n",
    "if not metadata_dict['has_nans']:\n",
    "    print('no null')\n",
    "\n",
    "#if mtype(X, as_scitype=\"Panel\"):\n",
    "#    pass\n",
    "\n",
    "metadata_dict\n",
    "\n",
    "check_is_mtype(y, mtype=\"np.ndarray\", scitype=\"Series\", return_metadata=True)\n",
    "\n",
    "from sktime.datatypes import mtype\n",
    "#mtype(y, as_scitype=\"Panel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402d509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cols = [\"instances\", \"time points\"] + [f\"var_{i}\" for i in range(2)]\n",
    "X = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame([[0, 0, 1, 4], [0, 1, 2, 5], [0, 2, 3, 6]], columns=cols),\n",
    "        pd.DataFrame([[1, 0, 1, 4], [1, 1, 2, 55], [1, 2, 3, 6]], columns=cols),\n",
    "        pd.DataFrame([[2, 0, 1, 42], [2, 1, 2, 5], [2, 2, 3, 6]], columns=cols),\n",
    "    ]\n",
    ").set_index([\"instances\", \"time points\"])\n",
    "\n",
    "X\n",
    "check_raise(X, mtype=\"pd-multiindex\")\n",
    "\n",
    "from sktime.datatypes import mtype\n",
    "\n",
    "mtype(X, as_scitype=\"Panel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d03815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.datatypes import get_examples\n",
    "\n",
    "X = get_examples(mtype=\"numpy3D\", as_scitype=\"Panel\")[0]\n",
    "X\n",
    "\n",
    "from sktime.datatypes import convert, convert_to\n",
    "\n",
    "X1 = convert(X, from_type=\"numpy3D\", to_type=\"pd-multiindex\")\n",
    "X2 = convert_to(X, to_type=\"pd-multiindex\")\n",
    "\n",
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119d1fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.datatypes import convert_to\n",
    "\n",
    "X2 = convert_to(X, to_type=\"pd-multiindex\")\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef4a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict['is_empty']\n",
    "\n",
    "#type(metadata_dict)\n",
    "\n",
    "len(metadata_tuple)\n",
    "\n",
    "type(metadata_tuple[2])\n",
    "metadata_dict['is_empty']\n",
    "\n",
    "metadata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4983190",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_examples(mtype=\"pd.DataFrame\", as_scitype=\"Series\")[0]\n",
    "x = get_examples(mtype=\"pd.DataFrame\", as_scitype=\"Series\")[1]\n",
    "x = get_examples(mtype=\"pd.Series\", as_scitype=\"Series\")[0]\n",
    "x = get_examples(mtype=\"np.ndarray\", as_scitype=\"Series\")[0]\n",
    "x = get_examples(mtype=\"np.ndarray\", as_scitype=\"Series\")[1]\n",
    "x = get_examples(mtype=\"pd-multiindex\", as_scitype=\"Panel\")[0]\n",
    "x = get_examples(mtype=\"numpy3D\", as_scitype=\"Panel\")[0]\n",
    "x = get_examples(mtype=\"df-list\", as_scitype=\"Panel\")[0]\n",
    "x = get_examples(mtype=\"pd_multiindex_hier\", as_scitype=\"Hierarchical\")[0]\n",
    "print('x:', type(x))\n",
    "print(f'x-typ: {type(x)}, x-shp: {len(x)}')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f799f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Panel forecasts and hierarchical forecasts testing - HierIndex UniVariate with dynamic index level\n",
    "\n",
    "from sktime.utils._testing.hierarchical import _bottom_hier_datagen\n",
    "y = _bottom_hier_datagen(no_levels=2)\n",
    "\n",
    "from sktime.forecasting.arima import ARIMA\n",
    "fh = [1, 2, 3]\n",
    "\n",
    "forecaster = ARIMA()\n",
    "forecaster.fit(y, fh=fh)\n",
    "y_pred = forecaster.predict()\n",
    "print(y_pred)\n",
    "\n",
    "forecaster.forecasters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767db4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Panel forecasts and hierarchical forecasts testing - HierIndex MultiVariate with dynamic columns\n",
    "\n",
    "from sktime.utils._testing.hierarchical import _make_hierarchical\n",
    "from sktime.forecasting.arima import ARIMA\n",
    "\n",
    "y = _make_hierarchical(n_columns=3)\n",
    "fh = [1, 2, 3]\n",
    "\n",
    "forecaster = ARIMA()\n",
    "forecaster.fit(y, fh=fh)\n",
    "\n",
    "y_pred = forecaster.predict()\n",
    "print(y_pred)\n",
    "\n",
    "forecaster.forecasters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f98a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Panel forecasts and hierarchical forecasts testing - HierIndex MultiVariate with dynamic columns\n",
    "\n",
    "from sktime.utils._testing.hierarchical import _make_hierarchical\n",
    "from sktime.forecasting.var import VAR\n",
    "\n",
    "y = _make_hierarchical(n_columns=3)\n",
    "fh = [1, 2, 3]\n",
    "\n",
    "forecaster = VAR()\n",
    "forecaster.fit(y, fh=fh)\n",
    "\n",
    "y_pred = forecaster.predict()\n",
    "print(y_pred)\n",
    "\n",
    "forecaster.forecasters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034a31af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MultiVariate\n",
    "from sktime.datasets import load_longley\n",
    "from sktime.forecasting.var import VAR\n",
    "\n",
    "_, y = load_longley()\n",
    "\n",
    "y = y.drop(columns=[\"UNEMP\", \"ARMED\", \"POP\"])\n",
    "\n",
    "forecaster = VAR()\n",
    "forecaster.fit(y, fh=[1, 2, 3])\n",
    "\n",
    "y_pred = forecaster.predict()\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576b9d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_X_y(self, X=None, y=None):\n",
    "        \"\"\"Check and coerce X/y for fit/predict/update functions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : pd.Series, pd.DataFrame, or np.ndarray (1D or 2D), optional (default=None)\n",
    "            Time series to check.\n",
    "        X : pd.DataFrame, or 2D np.array, optional (default=None)\n",
    "            Exogeneous time series.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_inner : Series, Panel, or Hierarchical object, or VectorizedDF\n",
    "                compatible with self.get_tag(\"y_inner_mtype\") format\n",
    "            Case 1: self.get_tag(\"y_inner_mtype\") supports scitype of y, then\n",
    "                converted/coerced version of y, mtype determined by \"y_inner_mtype\" tag\n",
    "            Case 2: self.get_tag(\"y_inner_mtype\") does not support scitype of y, then\n",
    "                VectorizedDF of y, iterated as the most complex supported scitype\n",
    "                    (complexity order: Hierarchical > Panel > Series)\n",
    "            Case 3: None if y was None\n",
    "        X_inner :  Series, Panel, or Hierarchical object, or VectorizedDF\n",
    "                compatible with self.get_tag(\"X_inner_mtype\") format\n",
    "            Case 1: self.get_tag(\"X_inner_mtype\") supports scitype of X, then\n",
    "                converted/coerced version of X, mtype determined by \"X_inner_mtype\" tag\n",
    "            Case 2: self.get_tag(\"X_inner_mtype\") does not support scitype of X, then\n",
    "                VectorizedDF of X, iterated as the most complex supported scitype\n",
    "            Case 3: None if X was None\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError if y or X is not one of the permissible Series mtypes\n",
    "        TypeError if y is not compatible with self.get_tag(\"scitype:y\")\n",
    "            if tag value is \"univariate\", y must be univariate\n",
    "            if tag value is \"multivariate\", y must be bi- or higher-variate\n",
    "            if tag value is \"both\", y can be either\n",
    "        TypeError if self.get_tag(\"X-y-must-have-same-index\") is True\n",
    "            and the index set of X is not a super-set of the index set of y\n",
    "\n",
    "        Writes to self\n",
    "        --------------\n",
    "        _y_mtype_last_seen : str, mtype of y\n",
    "        _converter_store_y : dict, metadata from conversion for back-conversion\n",
    "        \"\"\"\n",
    "        if X is None and y is None:\n",
    "            return None, None\n",
    "\n",
    "        def _most_complex_scitype(scitypes):\n",
    "            \"\"\"Return most complex scitype in a list of str.\"\"\"\n",
    "            if \"Hierarchical\" in scitypes:\n",
    "                return \"Hierarchical\"\n",
    "            elif \"Panel\" in scitypes:\n",
    "                return \"Panel\"\n",
    "            elif \"Series\" in scitypes:\n",
    "                return \"Series\"\n",
    "            else:\n",
    "                raise ValueError(\"no series scitypes supported, bug in estimator\")\n",
    "\n",
    "        # retrieve supported mtypes\n",
    "        y_inner_mtype = _coerce_to_list(self.get_tag(\"y_inner_mtype\"))\n",
    "        X_inner_mtype = _coerce_to_list(self.get_tag(\"X_inner_mtype\"))\n",
    "        y_inner_scitype = mtype_to_scitype(y_inner_mtype, return_unique=True)\n",
    "        X_inner_scitype = mtype_to_scitype(X_inner_mtype, return_unique=True)\n",
    "\n",
    "        ALLOWED_SCITYPES = [\"Series\", \"Panel\", \"Hierarchical\"]\n",
    "\n",
    "        # checking y\n",
    "        if y is not None:\n",
    "            y_valid, _, y_metadata = check_is_scitype(\n",
    "                y, scitype=ALLOWED_SCITYPES, return_metadata=True, var_name=\"y\"\n",
    "            )\n",
    "            msg = (\n",
    "                \"y must be in an sktime compatible format, \"\n",
    "                \"of scitype Series, Panel or Hierarchical, \"\n",
    "                \"for instance a pandas.DataFrame with sktime compatible time indices, \"\n",
    "                \"or with MultiIndex and lowest level a sktime compatible time index. \"\n",
    "                \"See the forecasting tutorial examples/01_forecasting.ipynb, or\"\n",
    "                \" the data format tutorial examples/AA_datatypes_and_datasets.ipynb\"\n",
    "            )\n",
    "            if not y_valid:\n",
    "                raise TypeError(msg)\n",
    "\n",
    "            y_scitype = y_metadata[\"scitype\"]\n",
    "            self._y_mtype_last_seen = y_metadata[\"mtype\"]\n",
    "\n",
    "            requires_vectorization = y_scitype not in y_inner_scitype\n",
    "\n",
    "            if (\n",
    "                self.get_tag(\"scitype:y\") == \"univariate\"\n",
    "                and not y_metadata[\"is_univariate\"]\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    \"y must be univariate, but found more than one variable\"\n",
    "                )\n",
    "            if (\n",
    "                self.get_tag(\"scitype:y\") == \"multivariate\"\n",
    "                and y_metadata[\"is_univariate\"]\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    \"y must have two or more variables, but found only one\"\n",
    "                )\n",
    "        else:\n",
    "            # y_scitype is used below - set to None if y is None\n",
    "            y_scitype = None\n",
    "        # end checking y\n",
    "\n",
    "        # checking X\n",
    "        if X is not None:\n",
    "            X_valid, _, X_metadata = check_is_scitype(\n",
    "                X, scitype=ALLOWED_SCITYPES, return_metadata=True, var_name=\"X\"\n",
    "            )\n",
    "\n",
    "            msg = (\n",
    "                \"X must be either None, or in an sktime compatible format, \"\n",
    "                \"of scitype Series, Panel or Hierarchical, \"\n",
    "                \"for instance a pandas.DataFrame with sktime compatible time indices, \"\n",
    "                \"or with MultiIndex and lowest level a sktime compatible time index. \"\n",
    "                \"See the forecasting tutorial examples/01_forecasting.ipynb, or\"\n",
    "                \" the data format tutorial examples/AA_datatypes_and_datasets.ipynb\"\n",
    "            )\n",
    "            if not X_valid:\n",
    "                raise TypeError(msg)\n",
    "\n",
    "            X_scitype = X_metadata[\"scitype\"]\n",
    "            requires_vectorization = X_scitype not in X_inner_scitype\n",
    "        else:\n",
    "            # X_scitype is used below - set to None if X is None\n",
    "            X_scitype = None\n",
    "        # end checking X\n",
    "\n",
    "        # compatibility checks between X and y\n",
    "        if X is not None and y is not None:\n",
    "            if self.get_tag(\"X-y-must-have-same-index\"):\n",
    "                check_equal_time_index(X, y, mode=\"contains\")\n",
    "\n",
    "            if y_scitype != X_scitype:\n",
    "                raise TypeError(\"X and y must have the same scitype\")\n",
    "        # end compatibility checking X and y\n",
    "\n",
    "        # todo: add tests that :\n",
    "        #   y_inner_scitype are same as X_inner_scitype\n",
    "        #   y_inner_scitype always includes \"less index\" scitypes\n",
    "\n",
    "        # convert X & y to supported inner type, if necessary\n",
    "        #####################################################\n",
    "\n",
    "        # convert X and y to a supported internal mtype\n",
    "        #  it X/y mtype is already supported, no conversion takes place\n",
    "        #  if X/y is None, then no conversion takes place (returns None)\n",
    "        #  if vectorization is required, we wrap in Vect\n",
    "\n",
    "        if not requires_vectorization:\n",
    "            # converts y, skips conversion if already of right type\n",
    "            y_inner = convert_to(\n",
    "                y,\n",
    "                to_type=y_inner_mtype,\n",
    "                as_scitype=y_scitype,  # we are dealing with series\n",
    "                store=self._converter_store_y,\n",
    "                store_behaviour=\"reset\",\n",
    "            )\n",
    "\n",
    "            # converts X, converts None to None if X is None\n",
    "            X_inner = convert_to(\n",
    "                X,\n",
    "                to_type=X_inner_mtype,\n",
    "                as_scitype=X_scitype,  # we are dealing with series\n",
    "            )\n",
    "        else:\n",
    "            iterate_as = _most_complex_scitype(y_inner_scitype)\n",
    "            if y is not None:\n",
    "                y_inner = VectorizedDF(X=y, iterate_as=iterate_as, is_scitype=y_scitype)\n",
    "            else:\n",
    "                y_inner = None\n",
    "            if X is not None:\n",
    "                X_inner = VectorizedDF(X=X, iterate_as=iterate_as, is_scitype=X_scitype)\n",
    "            else:\n",
    "                X_inner = None\n",
    "\n",
    "        return X_inner, y_inner\n",
    "\n",
    "#def _check_X(self, X=None):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b89a315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4b8df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing diff sktime usecase\n",
    "# https://github.com/sktime/sktime/issues/3617\n",
    "\n",
    "from sktime.datatypes import get_examples\n",
    "from sktime.datasets import load_unit_test\n",
    "from sktime.datatypes import convert_to\n",
    "from sktime.datatypes import mtype, check_raise, check_is_scitype, check_is_mtype\n",
    "from sktime.classification.dictionary_based import IndividualBOSS\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.forecasting.compose import BaggingForecaster\n",
    "\n",
    "from sklearn.metrics import f1_score # classification\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#1 case 1- nested_univariate classification\n",
    "def case_1():\n",
    "    #Returns: F1 score\n",
    "    st, mt ='Panel', 'nested_univ'\n",
    "    X,y = load_unit_test(return_X_y=True)\n",
    "    if mtype(X, as_scitype=st) == mt:\n",
    "        print(f'\\nCase_1 Scitype/mtype: {st}/{mt}')\n",
    "    y_train, y_test, X_train, X_test = \\\n",
    "        temporal_train_test_split(y=y,X=X,test_size=12)\n",
    "    clf = IndividualBOSS()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"From Case_1: F1 score {f1_score(y_test,y_pred, average='weighted'):0.4f}\") \n",
    "#case_1()\n",
    "\n",
    "#2a Case_2a and Case 2b: pd-multiindex classification\n",
    "# 2a: Cannot load train/test pairs\n",
    "def case_2a():\n",
    "    #Returns: TypeError: as_index=False only valid with DataFrame\n",
    "    st, mt = 'Panel', 'pd-multiindex'\n",
    "    X = load_unit_test(return_X_y=False)\n",
    "    X2 = convert_to(X,'pd-multiindex')\n",
    "    if mtype(X2, as_scitype=st) == mt:\n",
    "        print(f'\\nCase_2a Scitype/mtype: {st}/{mt}')\n",
    "    # separate X, y\n",
    "    y2 = X2['class_val']\n",
    "    X2.drop(columns=['class_val'], inplace=True)\n",
    "    try:\n",
    "        y_train2, y_test2, X_train2, X_test2 = \\\n",
    "            temporal_train_test_split(y=y2, X=X2,test_size=.25)\n",
    "    except Exception as e:\n",
    "        print(f'Error in Case_2a: {e}')\n",
    "# 2a: Cannot fit pd-multiindex data\n",
    "\n",
    "#2b\n",
    "def case_2b():\n",
    "    #Returns: ValueError: Mismatch in number of cases. Number in X = 42 nos in y = 1008\n",
    "    st, mt = 'Panel', 'pd-multiindex'\n",
    "    X = load_unit_test(return_X_y=False)\n",
    "    X2 = convert_to(X,'pd-multiindex')\n",
    "    if mtype(X2, as_scitype=st) == mt:\n",
    "        print(f'\\nCase_2b Scitype/mtype: {st}/{mt}')\n",
    "    # separate X, y\n",
    "    y2 = X2['class_val']\n",
    "    X2.drop(columns=['class_val'], inplace=True)\n",
    "    clf = IndividualBOSS()\n",
    "    try:\n",
    "        clf.fit(X=X2, y=y2)\n",
    "    except Exception as e:\n",
    "        print(f'Error in Case_2b: {e}')    \n",
    "#case_2b()\n",
    "\n",
    "#3\n",
    "def case_3():\n",
    "    #Example data from AA_Dataypes_and_Datasets NB\n",
    "    fcstr = BaggingForecaster()\n",
    "    st, mt = 'Panel', 'pd-multiindex'\n",
    "    cols = [\"instances\", \"timepoints\"] + [f\"var_{i}\" for i in range(2)]\n",
    "    X = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame([[0, 0, 1, 4], [0, 1, 2, 5], [0, 2, 3, 6]], columns=cols),\n",
    "            pd.DataFrame([[1, 0, 1, 4], [1, 1, 2, 55], [1, 2, 3, 6]], columns=cols),\n",
    "            pd.DataFrame([[2, 0, 1, 42], [2, 1, 2, 5], [2, 2, 3, 6]], columns=cols),\n",
    "        ]\n",
    "    ).set_index([\"instances\", \"timepoints\"])\n",
    "    if mtype(X, as_scitype=st) == mt:\n",
    "        print(f'\\nCase_3 Scitype/mtype: {st}/{mt}')\n",
    "    # extract y\n",
    "    col_y = 'var_1'\n",
    "    y = X[col_y]\n",
    "    X.drop(columns=[col_y], inplace=True)\n",
    "    try:\n",
    "        fcstr.fit(X=X, y=y)\n",
    "    except Exception as e:\n",
    "        print(f'Error in Case_3: {e}')\n",
    "    #Get additional information on y\n",
    "    try:\n",
    "        mtype(y, as_scitype=st) == mt\n",
    "    except Exception as e:\n",
    "        print(f'Error in Case_3 re: y mtype: {e}')     \n",
    "#case_3()\n",
    "\n",
    "#4\n",
    "def case_4():\n",
    "    #Example data from AA_Dataypes_and_Datasets NB\n",
    "    # Based on results of Case 3, the target values will be converted to a pandas DataFrame\n",
    "    fcstr = BaggingForecaster()\n",
    "    st, mt = 'Panel', 'pd-multiindex'\n",
    "    cols = [\"instances\", \"timepoints\"] + [f\"var_{i}\" for i in range(2)]\n",
    "    X = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame([[0, 0, 1, 4], [0, 1, 2, 5], [0, 2, 3, 6]], columns=cols),\n",
    "            pd.DataFrame([[1, 0, 1, 4], [1, 1, 2, 55], [1, 2, 3, 6]], columns=cols),\n",
    "            pd.DataFrame([[2, 0, 1, 42], [2, 1, 2, 5], [2, 2, 3, 6]], columns=cols),\n",
    "        ]\n",
    "    ).set_index([\"instances\", \"timepoints\"])\n",
    "    \n",
    "    if mtype(X, as_scitype=st) == mt:\n",
    "        print(f'\\nCase_4 Scitype/mtype: {st}/{mt}')\n",
    "    # extract y\n",
    "    col_y = 'var_1'\n",
    "    y = X[col_y]\n",
    "    X.drop(columns=[col_y], inplace=True)\n",
    "    \n",
    "    y_train, y_test, X_train, X_test = temporal_train_test_split(y=pd.DataFrame(y), X=X)\n",
    "    \n",
    "    try:\n",
    "        fcstr.fit(X=X, y=pd.DataFrame(y))\n",
    "        #fcstr.fit(X=X, y=y)\n",
    "    except Exception as e:\n",
    "        print(f'Error in Case_4: {e}')\n",
    "    #confirm the fitted forecaster can predict\n",
    "    pred = fcstr.predict(fh=[1,2,3])\n",
    "    print(f'Case 4: Predictions from fitted forecaster {pred}')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    case_1()\n",
    "    case_2a()\n",
    "    case_2b()\n",
    "    case_3()\n",
    "    case_4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77a13d09",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_unit_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#X,y = load_unit_test(return_X_y=True)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mload_unit_test\u001b[49m(return_X_y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m X2 \u001b[38;5;241m=\u001b[39m convert_to(X,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpd-multiindex\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m X2\u001b[38;5;241m.\u001b[39mshape, X\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_unit_test' is not defined"
     ]
    }
   ],
   "source": [
    "#X,y = load_unit_test(return_X_y=True)\n",
    "\n",
    "X = load_unit_test(return_X_y=False)\n",
    "X2 = convert_to(X,'pd-multiindex')\n",
    "\n",
    "X2.shape, X.shape\n",
    "\n",
    "cols = [\"instances\", \"timepoints\"] + [f\"var_{i}\" for i in range(2)]\n",
    "cols\n",
    "\n",
    "X = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame([[0, 0, 1, 4], [0, 1, 2, 5], [0, 2, 3, 6]], columns=cols),\n",
    "            pd.DataFrame([[1, 0, 1, 4], [1, 1, 2, 55], [1, 2, 3, 6]], columns=cols),\n",
    "            pd.DataFrame([[2, 0, 1, 42], [2, 1, 2, 5], [2, 2, 3, 6]], columns=cols),\n",
    "        ]\n",
    "    ).set_index([\"instances\", \"timepoints\"])\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03204634",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9326c4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VAR\n",
    "from sktime.datasets import load_longley\n",
    "from sktime.forecasting.var import VAR\n",
    "\n",
    "_, y = load_longley()\n",
    "\n",
    "y = y.drop(columns=[\"UNEMP\", \"ARMED\", \"POP\"])\n",
    "\n",
    "y = df_MultiIndex_MV\n",
    "\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "y_train, y_valid = temporal_train_test_split(y, test_size=0.2)\n",
    "\n",
    "forecaster = VAR()\n",
    "forecaster.fit(y_train, fh=[1, 2, 3])\n",
    "\n",
    "y_pred = forecaster.predict()\n",
    "\n",
    "print(f'y_valid:{y_valid.shape}, y_pred:{y_pred.shape}')\n",
    "from sktime.performance_metrics.forecasting import mean_absolute_percentage_error\n",
    "mape = mean_absolute_percentage_error(y_valid, y_pred, symmetric=False)\n",
    "\n",
    "mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc736f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c80360",
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n",
    "data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e0a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'MultiVariate'\n",
    "\n",
    "if 'Multi' in x:\n",
    "    print('tes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3f8d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e0d9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MultiIndex_MV.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b643fffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MultiIndex_MV\n",
    "df_MultiIndex_MV.columns.tolist()\n",
    "#df_MultiIndex_MV.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37351eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MultiIndex_MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb855d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import to retrieve examples\n",
    "\n",
    "from sktime.datatypes import get_examples\n",
    "\n",
    "s1 = get_examples(mtype=\"pd.DataFrame\")[0]\n",
    "#s1.index.levels\n",
    "s2 = get_examples(mtype=\"pd.DataFrame\")[1]\n",
    "s3 = get_examples(mtype=\"pd-multiindex\")[0]\n",
    "s4 = get_examples(mtype=\"pd-multiindex\")[1]\n",
    "\n",
    "s3_1 = s3.reset_index()\n",
    "\n",
    "index_col = ['instances', 'timepoints']\n",
    "num_cols_list = ['var_0', 'var_1']\n",
    "\n",
    "FEATURE = 'var_1'\n",
    "\n",
    "df = s3_1.copy()\n",
    "\n",
    "# for FEATURE in num_cols_list:\n",
    "#     df[f'{FEATURE}_max'] = df.groupby(index_col)[FEATURE].transform('max')\n",
    "#     df[f'{FEATURE}_min'] = df.groupby(index_col)[FEATURE].transform('min')\n",
    "#     df[f'{FEATURE}_std'] = df.groupby(index_col)[FEATURE].transform('std')\n",
    "#     df[f'{FEATURE}_mean'] = df.groupby(index_col)[FEATURE].transform('mean')\n",
    "    \n",
    "# df\n",
    "\n",
    "def ts_features(data_df, index_col_list, num_feat_col_list, nlags, nwindow):\n",
    "    '''\n",
    "    v1.0 Mar20,2023 initial copy\n",
    "    '''\n",
    "    \n",
    "    df = data_df.copy()\n",
    "    if num_feat_col_list:\n",
    "        for FEATURE in num_feat_col_list:\n",
    "            \n",
    "            #1 Aggregation features\n",
    "            df[f'{FEATURE}_max'] = df.groupby(index_col_list)[FEATURE].transform('max')\n",
    "            df[f'{FEATURE}_min'] = df.groupby(index_col_list)[FEATURE].transform('min')\n",
    "            df[f'{FEATURE}_std'] = df.groupby(index_col_list)[FEATURE].transform('std')\n",
    "            df[f'{FEATURE}_mean'] = df.groupby(index_col_list)[FEATURE].transform('mean')\n",
    "            \n",
    "            #2 lag features\n",
    "            df[f\"lag_{nlags}\"] = df.groupby(index_col_list)[FEATURE].shift(nlags).fillna(0)\n",
    "            \n",
    "            #3 Rolling features\n",
    "            df[f\"rolling_mean_{nwindow}\"] = df.groupby(index_col_list)[FEATURE].transform(lambda x : x.rolling(nwindow).mean()).fillna(0)\n",
    "            \n",
    "            #4 Normalization (min/max scaling)\n",
    "            df[f'{FEATURE}_norm'] = df[FEATURE]/df[f'{FEATURE}_max']\n",
    "\n",
    "            #5 Some items are can be inflation dependent and some items are very \"stable\"\n",
    "            df[f'{FEATURE}_nunique'] = df.groupby(index_col_list)[FEATURE].transform('nunique')\n",
    "\n",
    "            #6 Feature \"momentum\" \n",
    "            df[f'{FEATURE}_momentum'] = df[FEATURE]/df.groupby(index_col_list)[FEATURE].transform(lambda x: x.shift(1))\n",
    "            \n",
    "    #drop same value features\n",
    "    \n",
    "    return df\n",
    "        \n",
    "        \n",
    "df = ts_features(data_df=s3, index_col_list=index_col, num_feat_col_list=num_cols_list, nlags=4, nwindow=5)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80168ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.datasets import load_airline\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.utils.plotting import plot_series\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# step 1: data specification\n",
    "y = load_airline()\n",
    "\n",
    "# step 2: specifying forecasting horizon\n",
    "fh = np.arange(1, 37)\n",
    "#fh = np.array([2, 5])  # 2nd and 5th step ahead\n",
    "fh = ForecastingHorizon(\n",
    "    pd.PeriodIndex(pd.date_range(\"1961-01\", periods=36, freq=\"M\")), is_relative=False\n",
    ")\n",
    "\n",
    "# step 3: specifying the forecasting algorithm\n",
    "forecaster = NaiveForecaster(strategy=\"last\", sp=12)\n",
    "\n",
    "# step 4: fitting the forecaster\n",
    "forecaster.fit(y)\n",
    "\n",
    "# step 5: querying predictions\n",
    "y_pred = forecaster.predict(fh)\n",
    "# optional: plotting predictions and past data\n",
    "plot_series(y, y_pred, labels=[\"y\", \"y_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd80040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fh = ForecastingHorizon(\n",
    "#     pd.PeriodIndex(pd.date_range(\"1961-01\", periods=36, freq=\"M\")), is_relative=False\n",
    "# )\n",
    "\n",
    "cutoff = pd.Period(\"1960-12\", freq=\"M\")\n",
    "cutoff\n",
    "fh.to_relative(cutoff)\n",
    "#fh.to_absolute(cutoff)\n",
    "\n",
    "\n",
    "from sktime.datasets import load_longley\n",
    "from sktime.forecasting.arima import ARIMA\n",
    "\n",
    "_, y = load_longley()\n",
    "\n",
    "y = y.drop(columns=[\"UNEMP\", \"ARMED\", \"POP\"])\n",
    "\n",
    "forecaster = ARIMA()\n",
    "forecaster.fit(y, fh=[1, 2, 3])\n",
    "\n",
    "forecaster.forecasters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768bfbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/towards-data-science/a-step-by-step-guide-to-feature-engineering-for-multivariate-time-series-162ccf232e2f\n",
    "#https://github.com/vcerqueira/blog/blob/main/src/tde.py\n",
    "#https://github.com/vcerqueira/blog/tree/main/data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data_path = 'D:\\\\dataset\\\\TS\\\\MV\\\\'\n",
    "# skipping second row, setting time column as a datetime column\n",
    "# dataset available here: https://github.com/vcerqueira/blog/tree/main/data\n",
    "buoy = pd.read_csv(data_path+'smart_buoy.csv', \n",
    "                   skiprows=[1], \n",
    "                   parse_dates=['time'])\n",
    "\n",
    "# setting time as index\n",
    "buoy.set_index('time', inplace=True)\n",
    "# resampling to hourly data\n",
    "buoy = buoy.resample('H').mean()\n",
    "# simplifying column names\n",
    "buoy.columns = [\n",
    "    'PeakP', 'PeakD', 'Upcross',\n",
    "    'SWH', 'SeaTemp', 'Hmax', 'THmax',\n",
    "    'MCurDir', 'MCurSpd'\n",
    "]\n",
    "\n",
    "#1 Auto-regressive model\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# https://github.com/vcerqueira/blog/blob/main/src/tde.py\n",
    "#from src.tde import time_delay_embedding\n",
    "\n",
    "target_var = 'SWH'\n",
    "\n",
    "colnames = buoy.columns.tolist()\n",
    "\n",
    "# create data set with lagged features using time delay embedding\n",
    "buoy_ds = []\n",
    "for col in buoy:\n",
    "    col_df = time_delay_embedding(buoy[col], n_lags=24, horizon=12)\n",
    "    buoy_ds.append(col_df)\n",
    "\n",
    "# concatenating all variables\n",
    "buoy_df = pd.concat(buoy_ds, axis=1).dropna()\n",
    "\n",
    "# defining target (Y) and explanatory variables (X)\n",
    "predictor_variables = buoy_df.columns.str.contains('\\(t\\-')\n",
    "target_variables = buoy_df.columns.str.contains(f'{target_var}\\(t\\+')\n",
    "X = buoy_df.iloc[:, predictor_variables]\n",
    "Y = buoy_df.iloc[:, target_variables]\n",
    "\n",
    "# train/test split\n",
    "X_tr, X_ts, Y_tr, Y_ts = train_test_split(X, Y, test_size=0.3, shuffle=False)\n",
    "\n",
    "# fitting a lgbm model without feature engineering\n",
    "model_wo_fe = MultiOutputRegressor(LGBMRegressor())\n",
    "model_wo_fe.fit(X_tr, Y_tr)\n",
    "\n",
    "# getting forecasts for the test set\n",
    "preds_wo_fe = model_wo_fe.predict(X_ts)\n",
    "\n",
    "# computing the MAPE error\n",
    "mape(Y_ts, preds_wo_fe)\n",
    "\n",
    "#2 Univariate Feature Extraction\n",
    "import numpy as np\n",
    "\n",
    "SUMMARY_STATS = {\n",
    "    'mean': np.mean,\n",
    "    'sdev': np.std,\n",
    "}\n",
    "\n",
    "univariate_features = {}\n",
    "# for each column in the data\n",
    "for col in colnames:\n",
    "    # get lags for that column\n",
    "    X_col = X.iloc[:, X.columns.str.startswith(col)]\n",
    "\n",
    "    # for each summary stat\n",
    "    for feat, func in SUMMARY_STATS.items():\n",
    "        # compute that stat along the rows\n",
    "        univariate_features[f'{col}_{feat}'] = X_col.apply(func, axis=1)\n",
    "\n",
    "# concatenate features into a pd.DF\n",
    "univariate_features_df = pd.concat(univariate_features, axis=1)\n",
    "\n",
    "#3 Bivariate Feature Extraction\n",
    "'''\n",
    "Rolling binary statistics. Compute statistics that take pairs of variables as input.\n",
    "For example, the rolling covariance or rolling correlation;\n",
    "Rolling binary transformation followed by univariate statistics.\n",
    "Transform a pair of variables into a single variable, and summarise this variable.\n",
    "For example, computing the elementwise cross-correlation and then taking its average value.\n",
    "'''\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy import signal\n",
    "from scipy.special import rel_entr\n",
    "\n",
    "#from src.feature_extraction import covariance, co_integration\n",
    "\n",
    "BIVARIATE_STATS = {\n",
    "    'covariance': covariance,\n",
    "    'co_integration': co_integration,\n",
    "    'js_div': jensenshannon,\n",
    "}\n",
    "\n",
    "BIVARIATE_TRANSFORMATIONS = {\n",
    "    'corr': signal.correlate,\n",
    "    'conv': signal.convolve,\n",
    "    'rel_entr': rel_entr,\n",
    "}\n",
    "\n",
    "# get all pairs of variables\n",
    "col_combs = list(itertools.combinations(colnames, 2))\n",
    "\n",
    "bivariate_features = []\n",
    "# for each row\n",
    "for i, _ in X.iterrows():\n",
    "    # feature set in the i-th time-step\n",
    "    feature_set_i = {}\n",
    "    for col1, col2 in col_combs:\n",
    "        # features for pair of columns col1, col2\n",
    "\n",
    "        # getting the i-th instance for each column\n",
    "        x1 = X.loc[i, X.columns.str.startswith(col1)]\n",
    "        x2 = X.loc[i, X.columns.str.startswith(col2)]\n",
    "\n",
    "        # compute each summary stat\n",
    "        for feat, func in BIVARIATE_SUMMARY_STATS.items():\n",
    "            feature_set_i[f'{col1}|{col2}_{feat}'] = func(x1, x2)\n",
    "\n",
    "        # for each transformation\n",
    "        for trans_f, t_func in BIVARIATE_TRANSFORMATIONS.items():\n",
    "\n",
    "            # apply transformation\n",
    "            xt = t_func(x1, x2)\n",
    "\n",
    "            # compute summary stat\n",
    "            for feat, s_func in SUMMARY_STATS.items():\n",
    "                feature_set_i[f'{col1}|{col2}_{trans_f}_{feat}'] = s_func(xt)\n",
    "\n",
    "    bivariate_features.append(feature_set_i)\n",
    "\n",
    "bivariate_features_df = pd.DataFrame(bivariate_features, index=X.index)\n",
    "\n",
    "#4 concatenating all features with lags\n",
    "X_with_features = pd.concat([X, univariate_features_df, bivariate_features_df], axis=1)\n",
    "\n",
    "# train/test split\n",
    "X_tr, X_ts, Y_tr, Y_ts = train_test_split(X_with_features, Y, test_size=0.3, shuffle=False)\n",
    "\n",
    "# fitting a lgbm model with feature engineering\n",
    "model_w_fe = MultiOutputRegressor(LGBMRegressor())\n",
    "model_w_fe.fit(X_tr, Y_tr)\n",
    "\n",
    "# getting forecasts for the test set\n",
    "preds_w_fe = model_w_fe.predict(X_ts)\n",
    "\n",
    "# computing MAPE error\n",
    "print(mape(Y_ts, preds_w_fe))\n",
    "\n",
    "\n",
    "#5 Feature Selection\n",
    "# getting the importance of each feature in each horizon\n",
    "avg_imp = pd.DataFrame([x.feature_importances_\n",
    "                        for x in model_w_fe.estimators_]).mean()\n",
    "\n",
    "# getting the top 100 features\n",
    "n_top_features = 100\n",
    "\n",
    "importance_scores = pd.Series(dict(zip(X_tr.columns, avg_imp)))\n",
    "top_features = importance_scores.sort_values(ascending=False)[:n_top_features]\n",
    "top_features_nm = top_features.index\n",
    "\n",
    "# subsetting training and testing sets by those features\n",
    "X_tr_top = X_tr[top_features_nm]\n",
    "X_ts_top = X_ts[top_features_nm]\n",
    "\n",
    "# re-fitting the lgbm model\n",
    "model_top_features = MultiOutputRegressor(LGBMRegressor())\n",
    "model_top_features.fit(X_tr_top, Y_tr)\n",
    "\n",
    "# getting forecasts for the test set\n",
    "preds_top_feats = model_top_features.predict(X_ts_top)\n",
    "\n",
    "# computing MAE error\n",
    "mape(Y_ts, preds_top_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d9ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MultiOutputRegressor\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_linnerud\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "X, y = load_linnerud(return_X_y=True)\n",
    "regr = MultiOutputRegressor(Ridge(random_state=123)).fit(X, y)\n",
    "regr.predict(X[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c480890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n-Beats\n",
    "#https://www.datasciencewithmarco.com/blog/the-easiest-way-to-forecast-time-series-using-n-beats\n",
    "\n",
    "#1 import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "#from darts import TimeSeries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data_path = 'D:\\\\dataset\\\\TS\\\\MV\\\\'\n",
    "df = pd.read_csv(data_path+'wine_sales.csv')\n",
    "                 \n",
    "#2 DataFrame to a TimeSeries object\n",
    "series = TimeSeries.from_dataframe(df, time_col='date_time')\n",
    "series.plot()\n",
    "\n",
    "#3 check_seasonality                 \n",
    "from darts.utils.statistics import check_seasonality\n",
    "\n",
    "is_daily_seasonal, daily_period = check_seasonality(series, m=24, max_lag=400, alpha=0.05)\n",
    "is_weekly_seasonal, weekly_period = check_seasonality(series, m=168, max_lag=400, alpha=0.05)\n",
    "\n",
    "print(f'Daily seasonality: {is_daily_seasonal} - period = {daily_period}')\n",
    "print(f'Weekly seasonality: {is_weekly_seasonal} - period = {weekly_period}')\n",
    "                 \n",
    "#4 Split the data\n",
    "train, test = series[:-120], series[-120:]\n",
    "train.plot(label='train')\n",
    "test.plot(label='test')\n",
    "                 \n",
    "#5 Baseline model - benchmark to determine if a more complex model is actually better.\n",
    "from darts.models.forecasting.baselines import NaiveSeasonal\n",
    "\n",
    "naive_seasonal = NaiveSeasonal(K=168)\n",
    "naive_seasonal.fit(train)\n",
    "pred_naive = naive_seasonal.predict(120)                 \n",
    "                 \n",
    "#5 visualize the forecasts coming from the baseline model.\n",
    "test.plot(label='test')\n",
    "pred_naive.plot(label='Baseline')\n",
    "                 \n",
    "#6 evaluate the performance of the baseline using the mean absolute error (MAE)\n",
    "from darts.metrics import mae\n",
    "naive_mae = mae(test, pred_naive)\n",
    "print(naive_mae)  \n",
    "                 \n",
    "#7 N-BEATS without covariates\n",
    "from darts.models import NBEATSModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "\n",
    "train_scaler = Scaler()\n",
    "scaled_train = train_scaler.fit_transform(train)\n",
    "\n",
    "#initialize the N-BEATS model.I/P length will contain a full week of data,the model will output 24h of data.\n",
    "nbeats = NBEATSModel(\n",
    "    input_chunk_length=168, \n",
    "    output_chunk_length=24,\n",
    "    generic_architecture=True,\n",
    "    random_state=42)\n",
    "\n",
    "nbeats.fit(\n",
    "    scaled_train,\n",
    "    epochs=50)\n",
    "\n",
    "#forecast over the horizon of the test set\n",
    "scaled_pred_nbeats = nbeats.predict(n=120)\n",
    "pred_nbeats = train_scaler.inverse_transform(scaled_pred_nbeats)\n",
    "mae_nbeats = mae(test, pred_nbeats)\n",
    "\n",
    "print(mae_nbeats)\n",
    "\n",
    "#8 N-BEATS with covariates\n",
    "'''\n",
    "there are two seasonal periods that are significant in our time series. We can encode that information and pass it to the model as covariates.\n",
    "we add two features to the model that tells it where we are during the day and during the week.\n",
    "'''\n",
    "from darts import concatenate\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries as dt_attr\n",
    "\n",
    "cov = concatenate(\n",
    "    [dt_attr(series.time_index, 'day', dtype=np.float32), dt_attr(series.time_index, 'week', dtype=np.float32)],\n",
    "    axis='component'\n",
    ")\n",
    "\n",
    "cov_scaler = Scaler()\n",
    "scaled_cov = cov_scaler.fit_transform(cov)\n",
    "\n",
    "#initializing N-BEATS and fitting with covariates\n",
    "nbeats_cov = NBEATSModel(\n",
    "    input_chunk_length=168, \n",
    "    output_chunk_length=24,\n",
    "    generic_architecture=True,\n",
    "    random_state=42)\n",
    "\n",
    "nbeats_cov.fit(\n",
    "    scaled_train,\n",
    "    past_covariates=scaled_cov,\n",
    "    epochs=50\n",
    ")\n",
    "\n",
    "scaled_pred_nbeats_cov = nbeats_cov.predict(past_covariates=scaled_cov, n=120)\n",
    "pred_nbeats_cov = train_scaler.inverse_transform(scaled_pred_nbeats_cov)\n",
    "\n",
    "test.plot(label='test')\n",
    "pred_nbeats.plot(label='N-BEATS')\n",
    "\n",
    "mae_nbeats_cov = mae(test, pred_nbeats_cov)\n",
    "print(mae_nbeats_cov)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c030c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n-Hits\n",
    "#https://towardsdatascience.com/all-about-n-hits-the-latest-breakthrough-in-time-series-forecasting-a8ddcb27b0d5\n",
    "#https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume#\n",
    "#!pip install darts\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from darts import TimeSeries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('data/daily_traffic.csv')\n",
    "\n",
    "#from DataFrame to a TimeSeries object\n",
    "series = TimeSeries.from_dataframe(df, time_col='date_time')\n",
    "\n",
    "#seasonal periods: weekly and daily\n",
    "from darts.utils.statistics import check_seasonality\n",
    "\n",
    "is_daily_seasonal, daily_period = check_seasonality(series, m=24, max_lag=400, alpha=0.05)\n",
    "is_weekly_seasonal, weekly_period = check_seasonality(series, m=168, max_lag=400, alpha=0.05)\n",
    "\n",
    "print(f'Daily seasonality: {is_daily_seasonal} - period = {daily_period}')\n",
    "print(f'Weekly seasonality: {is_weekly_seasonal} - period = {weekly_period}')\n",
    "\n",
    "#split the data\n",
    "train, test = series[:-120], series[-120:]\n",
    "\n",
    "train.plot(label='train')\n",
    "test.plot(label='test')\n",
    "\n",
    "# Baseline model\n",
    "from darts.models.forecasting.baselines import NaiveSeasonal\n",
    "\n",
    "naive_seasonal = NaiveSeasonal(K=168)\n",
    "naive_seasonal.fit(train)\n",
    "pred_naive = naive_seasonal.predict(120)\n",
    "\n",
    "test.plot(label='test')\n",
    "pred_naive.plot(label='Baseline')\n",
    "\n",
    "#evaluate the performance\n",
    "from darts.metrics import mae\n",
    "naive_mae = mae(test, pred_naive)\n",
    "print(naive_mae)\n",
    "\n",
    "#Applying N-HiTS - Darts makes it very easy for us to use state-of-the-art models like N-BEATS and N-Hits\n",
    "\n",
    "from darts.models import NHiTSModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "train_scaler = Scaler()\n",
    "scaled_train = train_scaler.fit_transform(train)\n",
    "\n",
    "#initialize the N-HiTS model\n",
    "nhits = NHiTSModel(\n",
    "    input_chunk_length=168, \n",
    "    output_chunk_length=120,\n",
    "    random_state=42)\n",
    "\n",
    "nhits.fit(\n",
    "    scaled_train,\n",
    "    epochs=50)\n",
    "\n",
    "'''\n",
    "we can really see how N-HiTS trains much faster than N-BEATS.\n",
    "Plus, Darts shows us the size of the model. In the case of N-HiTS,\n",
    "our model weighs 8.5 MB, whereas N-BEATS weighed 58.6 MB. It is almost 7 times lighter!\n",
    "'''\n",
    "#predictions will be bet 0 and 1, as we fit on the scaled training set, need to reverse the transformation.\n",
    "scaled_pred_nhits = nhits.predict(n=120)\n",
    "pred_nhits = train_scaler.inverse_transform(scaled_pred_nhits)\n",
    "\n",
    "test.plot(label='Actual')\n",
    "pred_nhits.plot(label='N-HiTS')\n",
    "\n",
    "mae_nhits = mae(test, pred_nhits)\n",
    "print(mae_nhits)\n",
    "\n",
    "'''\n",
    "N-HiTS builds upon N-BEATS by adding a MaxPool layer at each block.\n",
    "This subsamples the series and allows each stack to focus on either\n",
    "short-term or long-term effects, depending on the kernel size.\n",
    "Then, the partial predicttions of each stacks are combined using hierarchical interpolation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7517233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sktime-Hierarchical time series\n",
    "#https://github.com/sktime/sktime/blob/main/examples/01c_forecasting_hierarchical_global.ipynb\n",
    "#structure convention: obj.index must be a 3 or more level multi-index\n",
    "from sktime.datatypes import get_examples\n",
    "\n",
    "#X = get_examples(mtype=\"pd_multiindex_hier\")[0]\n",
    "ts_UV_df = get_examples(mtype=\"pd.DataFrame\")[0] #univariate\n",
    "ts_BV_df = get_examples(mtype=\"pd.DataFrame\")[1] #bivariate\n",
    "ts_MI_BV_df = get_examples(mtype=\"pd-multiindex\")[0] #multiindex\n",
    "ts_MI_hier_df = get_examples(mtype=\"pd_multiindex_hier\")[0]\n",
    "\n",
    "X = ts_MI_hier_df.copy()\n",
    "#X.index.get_level_values(level=-1)\n",
    "\n",
    "#Automated vectorization (casting) of forecasters and transformers\n",
    "from sktime.utils._testing.hierarchical import _make_hierarchical\n",
    "\n",
    "y = _make_hierarchical()\n",
    "from sktime.forecasting.arima import ARIMA\n",
    "\n",
    "forecaster = ARIMA()\n",
    "\n",
    "y_pred = forecaster.fit(y, fh=[1, 2]).predict()\n",
    "y_pred\n",
    "\n",
    "#individual forecasters fitted per hierarchy level are stored in the forecasters_ attribute\n",
    "forecaster.forecasters_\n",
    "\n",
    "#summary\n",
    "forecaster.forecasters_.iloc[0, 0].summary()\n",
    "\n",
    "forecaster.predict_interval()\n",
    "\n",
    "#For forecasters, inspect y_inner_mtype, this is a list of internally supported machine types:\n",
    "forecaster.get_tag(\"y_inner_mtype\")\n",
    "\n",
    "'''\n",
    "ARIMA supports only one Series level mtype. For a register of all mtype codes and their corresponding scitype (series, panel or hierarchical),\n",
    "see sktime.datatypes.MTYPE_REGISTER (convert to pandas.DataFrame for pretty-printing)\n",
    "'''\n",
    "import pandas as pd\n",
    "from sktime.datatypes import MTYPE_REGISTER\n",
    "pd.DataFrame(MTYPE_REGISTER)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51df5939",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA\n",
    "from sktime.datasets import load_macroeconomic\n",
    "from sktime.transformations.series.detrend import Deseasonalizer, Detrender\n",
    "\n",
    "data = load_macroeconomic()\n",
    "y = data[\"unemp\"]\n",
    "X = data.drop(columns=[\"unemp\"])\n",
    "\n",
    "#y_train, y_test, X_train, X_test = temporal_train_test_split(y, X)\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# result = seasonal_decompose(y_train, model='multiplicative')\n",
    "# result.plot()\n",
    "# plt.show()\n",
    "\n",
    "X.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcf2d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing VAR\n",
    "from sktime.datasets import load_longley\n",
    "from sktime.forecasting.var import VAR\n",
    "\n",
    "X, y = load_longley()\n",
    "\n",
    "y = y.drop(columns=[\"UNEMP\", \"ARMED\", \"POP\"])\n",
    "\n",
    "model_type = 'AutoArima'\n",
    "\n",
    "if model_type == 'NaiveForecaster':\n",
    "    from sktime.forecasting.naive import NaiveForecaster, NaiveVariance\n",
    "    forecaster = NaiveForecaster()\n",
    "elif model_type == 'PolynomialTrend':\n",
    "    #linear regression model with a 1st degree polynomial transformation of the feature.\n",
    "    from sktime.forecasting.trend import PolynomialTrendForecaster\n",
    "    forecaster = PolynomialTrendForecaster(degree=1)\n",
    "elif model_type == 'AutoArima':\n",
    "    from sktime.forecasting.arima import AutoARIMA\n",
    "    forecaster = AutoARIMA(sp=4)\n",
    "elif model_type == 'ThetaForecaster':\n",
    "    #it's equivalent to simple exponential smoothing (SES) with drift/tested for seasonality\n",
    "    from sktime.forecasting.theta import ThetaForecaster\n",
    "    forecaster = ThetaForecaster(sp=12)\n",
    "elif model_type == 'ExponentialSmoothing':\n",
    "    from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "    forecaster = ExponentialSmoothing(trend=\"add\", sp=12)\n",
    "elif model_type == 'BATS':\n",
    "    from sktime.forecasting.bats import BATS\n",
    "    forecaster = BATS(sp=12, use_trend=True, use_box_cox=False)\n",
    "elif model_type == 'TBATS':\n",
    "    from sktime.forecasting.tbats import TBATS\n",
    "    forecaster = TBATS(sp=12, use_trend=True, use_box_cox=False)\n",
    "elif model_type == 'UnobservedComponents':\n",
    "    from sktime.forecasting.structural import UnobservedComponents\n",
    "    forecaster = UnobservedComponents(\n",
    "        level=\"local linear trend\", freq_seasonal=[{\"period\": 12, \"harmonics\": 10}])\n",
    "elif model_type == 'StatsForecastAutoARIMA':\n",
    "    from sktime.forecasting.statsforecast import StatsForecastAutoARIMA\n",
    "    forecaster = StatsForecastAutoARIMA(sp=12)\n",
    "            \n",
    "#1\n",
    "#forecaster = VAR()\n",
    "\n",
    "#forecaster.fit(y, fh=[1, 2, 3])\n",
    "forecaster.fit(y, X, fh=[1, 2, 3])\n",
    "y_pred = forecaster.predict()\n",
    "\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ceb06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(forecaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a63cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster.fit(y, fh=[1, 2, 3])\n",
    "y_pred = forecaster.predict()\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9bdf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0915cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d505c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_longley()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fdc4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.utils._testing.hierarchical import _make_hierarchical\n",
    "df_MultiIndexHier_UV = _make_hierarchical()\n",
    "type(df_MultiIndexHier_UV)\n",
    "\n",
    "df_MultiIndexHier_UV.columns.tolist()\n",
    "\n",
    "#fh_re =[1,2,3]\n",
    "\n",
    "fh_re = []\n",
    "\n",
    "x =1\n",
    "for a in range(1, x+1):\n",
    "     fh_re.append(a)\n",
    "        \n",
    "fh_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3cd641",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= 2\n",
    "[a for a in range(1, x+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630742a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.datasets import load_airline\n",
    "y = load_airline()\n",
    "type(y)\n",
    "y.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b675c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_UniIndex_UV\n",
    "import pandas\n",
    "if isinstance(data.index, pandas.core.indexes.period.PeriodIndex):\n",
    "    index_type = 'PeriodIndex'\n",
    "elif isinstance(data.index, pandas.core.indexes.range.RangeIndex):\n",
    "    index_type = 'RangeIndex'\n",
    "elif isinstance(data.index, pandas.core.indexes.multi.MultiIndex):\n",
    "    index_type = 'MultiIndex'\n",
    "    #check index level eg. no of features in index\n",
    "    if len(data.index.levels)==1:\n",
    "        index_type = 'MultiIndexUni'\n",
    "    elif len(data.index.levels)==2:\n",
    "        index_type = 'MultiIndexBi'\n",
    "    elif len(data.index.levels)>=3:\n",
    "        index_type = 'MultiIndexMulti'\n",
    "elif isinstance(data.index, pandas.core.indexes.base.Index):\n",
    "    index_type = 'NormalIndex'\n",
    "print('index_type:', index_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e525c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_MultiIndex_MV.index.levels)\n",
    "\n",
    "type(df_MultiIndex_MV.index)\n",
    "\n",
    "type(df_UniIndex_UV.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1806d820",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MultiIndex_MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b28511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diff types of data\n",
    "\n",
    "#1 load the data- monthly - shampoo-sales.csv\n",
    "data_path='D:\\\\timeseries\\\\'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "ts_month = pd.read_csv(data_path+'shampoo-sales.csv')\n",
    "index_col = 'Month'\n",
    "value_col = 'Sales'\n",
    "\n",
    "#2 load the data- yearly\n",
    "ts_year = sm.datasets.sunspots.load_pandas().data\n",
    "index_col = 'YEAR'\n",
    "value_col = 'SUNACTIVITY'\n",
    "\n",
    "#3 load the data- year-month\n",
    "from sktime.datasets import load_airline\n",
    "\n",
    "ts_yearmonth = load_airline() # series\n",
    "#index_col = 'YEAR'\n",
    "#value_col = 'SUNACTIVITY'\n",
    "\n",
    "type(ts_yearmonth.index.values.tolist()[0])\n",
    "\n",
    "#ts_yearmonth.index.to_series().astype(str).values.tolist()[-1]\n",
    "\n",
    "data = ts_month\n",
    "index_col = 'Month'\n",
    "value_col = 'Sales'\n",
    "if isinstance(data, pd.DataFrame):\n",
    "    #set index\n",
    "    #data[index_col] = pd.to_datetime(data[index_col])\n",
    "    data.set_index(index_col, drop = True, inplace = True) # convert to series\n",
    "    \n",
    "y=data[value_col]\n",
    "\n",
    "#ts_forecast_period_index(ts=y, periods=12, freq=\"M\") #OutOfBoundsDatetime\n",
    "#ts_month\n",
    "\n",
    "data\n",
    "type(data.index)\n",
    "\n",
    "#4 multi index\n",
    "from sktime.datatypes import get_examples\n",
    "\n",
    "df_UniIndex_UV = get_examples(mtype=\"pd.DataFrame\")[0]\n",
    "df_UniIndex_MV = get_examples(mtype=\"pd.DataFrame\")[1]\n",
    "df_MultiIndex_MV = get_examples(mtype=\"pd-multiindex\")[0]\n",
    "df_MultiIndexHier_MV = get_examples(mtype=\"pd-multiindex\")[0]\n",
    "\n",
    "from sktime.utils._testing.hierarchical import _make_hierarchical\n",
    "df_MultiIndexHier_UV = _make_hierarchical()\n",
    "df_MultiIndexHier_UV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752cbfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.datasets import load_arrow_head\n",
    "X, y = load_arrow_head()\n",
    "\n",
    "type(X.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfbcb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#v1.2 testing ts_forecast_models with ForecastingPipeline /Feb03,2023\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_ts_pipeline(data, pp_steps_dict, pp_steps_list):\n",
    "    '''\n",
    "    1.1 Jan 29,2023 added index chk,multi index\n",
    "    1.0 Jan 23,2023 initial ver\n",
    "    \n",
    "    TBA\n",
    "    TransformedTargetForecaster-takes a chain of transformers and forecasters\n",
    "    MultiplexForecaster\n",
    "    TSFreshFeatureExtractor\n",
    "    WindowSummarizer: mean/stdev/n_legs features\n",
    "    DateTimeFeatures: date features\n",
    "    \n",
    "    TBC\n",
    "    diff between TransformedTargetForecaster and ForecastingPipeline\n",
    "    '''\n",
    "    \n",
    "    from sktime.forecasting.compose import ForecastingPipeline\n",
    "    from sktime.transformations.series.outlier_detection import HampelFilter\n",
    "    from sktime.transformations.series.boxcox import LogTransformer, BoxCoxTransformer\n",
    "    from sktime.transformations.series.adapt import TabularToSeriesAdaptor\n",
    "    from sktime.transformations.series.impute import Imputer\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sktime.forecasting.theta import ThetaForecaster\n",
    "    \n",
    "    print('Entry create_ts_pipeline')\n",
    "    #mandetory check\n",
    "    if len(df1.index.levels) >=3:\n",
    "        ts_type = 'HierarchicalIndex_TS'\n",
    "    elif len(df1.index.levels) ==3:\n",
    "        pass\n",
    "    elif len(df1.index.levels) ==2:\n",
    "        pass\n",
    "    elif len(df1.index.levels) ==1:\n",
    "        pass\n",
    "    \n",
    "    if ts_type == 'HierarchicalIndex_TS':\n",
    "        from sktime.transformations.hierarchical.aggregate import Aggregator\n",
    "        aggregator = Aggregator()\n",
    "    else:\n",
    "        aggregator = 'passthrough'\n",
    "        \n",
    "    \n",
    "    if pp_steps_dict:\n",
    "        #2.1 ImputerCat/ImputerNum\n",
    "        if 'Outlier' in pp_steps_dict:\n",
    "            if pp_steps_dict['Outlier'] == 'HampelFilter':\n",
    "                outlier = HampelFilter()\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            outlier = 'passthrough'\n",
    "            \n",
    "        \n",
    "        if 'Transformer' in pp_steps_dict:\n",
    "            if pp_steps_dict['Transformer'] == 'LogTransformer':\n",
    "                transformer = LogTransformer()\n",
    "            elif pp_steps_dict['Transformer'] == 'BoxCoxTransformer':\n",
    "                transformer = BoxCoxTransformer(sp=4)\n",
    "            else:\n",
    "                transformer = 'passthrough'\n",
    "    \n",
    "    #5 optional pp_steps_list\n",
    "    print('\\n ##5 pp_steps_list: setup flags MatchFeat, DropNullRows, DropDupRows, DropDupCols, DropQuasiConstCols')\n",
    "    if pp_steps_list:\n",
    "        if 'FeatureExtraction' in pp_steps_list:\n",
    "            from sktime.transformations.panel.tsfresh import TSFreshFeatureExtractor\n",
    "            FeatureExtraction = TSFreshFeatureExtractor(default_fc_parameters=\"efficient\", show_warnings=False)\n",
    "        else:\n",
    "            FeatureExtraction = 'passthrough'\n",
    "            \n",
    "            \n",
    "            \n",
    "    pipeline = ForecastingPipeline(\n",
    "    [\n",
    "    (\"imputer\", Imputer(method=\"mean\")),\n",
    "    (\"minmaxscaler\", TabularToSeriesAdaptor(MinMaxScaler())),\n",
    "    ('aggregator', aggregator), # for Hierarchical TS\n",
    "    (\"outlier\", outlier),\n",
    "    (\"transformer\", transformer),\n",
    "    (\"model\", ThetaForecaster(sp=4)) #defaut otherwise error\n",
    "    ])\n",
    "    \n",
    "    print('Exit create_ts_pipeline')\n",
    "    return pipeline\n",
    "     \n",
    "def custom_mape(y: np.array, y_pred: np.array):\n",
    "    import numpy as np\n",
    "    metrics_dict = {'score_mean': np.mean(np.abs((y - np.ceil(y_pred)) / y)),\n",
    "                    'score_row': np.abs((y - np.ceil(y_pred)) / y)}\n",
    "    return metrics_dict\n",
    "\n",
    "def set_index(data, index_cols_list=[]):\n",
    "    '''\n",
    "    index_type = Int64Index, RangeIndex, DatetimeIndex, PeriodIndex\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    \n",
    "    df = data.copy()\n",
    "    index = pd.MultiIndex.from_frame(df[index_cols_list])\n",
    "    df.set_index(index, drop=True, inplace=True)\n",
    "    df.drop(index_cols_list, axis=1, inplace=True)\n",
    "    \n",
    "    #check duplicate values\n",
    "    if df.index.has_duplicates:\n",
    "        pass\n",
    "    \n",
    "    #check index level eg. no of features in index\n",
    "    if len(index.levels)==1:\n",
    "        ts_type = 'NormalIndex_TS'\n",
    "    elif len(index.levels)==2:\n",
    "        ts_type = 'MultiIndex_TS'\n",
    "    elif len(index.levels)>=3:\n",
    "        ts_type = 'HierarchicalIndex_TS'\n",
    "    print('ts_type:', ts_type)\n",
    "    return df\n",
    "\n",
    "\n",
    "def tt_cv_pt_models_ts(data, target_col, feat_list, model_type, pipeline, index_cols_list=[], nested_params=None, train_type='TrainTest',\n",
    "        forecast_period=5, freq='M', regression_window=10, cv_type='ExpandingWindowSplitter',\n",
    "        pt_type='ForecastingRandomizedSearch', transformer_type='BoxCoxTransformer', PlotSeries=True):\n",
    "    '''\n",
    "    v1.3 Jan26,2023: Updated for new model_type/ added probabilistic predictions support\n",
    "    v1.2 Jan23,2023: Updated for pipeline\n",
    "    v1.1 Jan20,2023: added regression_window, cv_type, pt_type, forecasting score/CrossValidation\n",
    "    v1.0 Jan18,2023: initial version\n",
    "    ts_type: UniVariate/MultiVariate\n",
    "    \n",
    "    \n",
    "    \n",
    "    pipeline:if pipeline true then model_type (model) will update in pipeline that will be fitted else model_type will be fitted\n",
    "    train_type: TrainTest/CrossValidation/GridSearch/RandomizedSearch\n",
    "    freq: used to set index whern data is as df with date col\n",
    "    regression_window: when regression models are used\n",
    "    cv_type: used in cross_validation/GridSerach - ExpandingWindowSplitter/SlidingWindowSplitter\n",
    "    pt_type: used in parameter tuning - ForecastingGridSearch/ForecastingRandomizedSearch\n",
    "    \n",
    "    Test issues\n",
    "    Auto_Arima with cross_validation TypeError\n",
    "    TypeError: input must be a one of (<class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.series.Series'>), but found type: <class 'float'>\n",
    "    '''\n",
    "    print('entry tt_cv_pt_models_ts')\n",
    "    \n",
    "    #import libs\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sktime.forecasting.base import ForecastingHorizon\n",
    "    from sktime.utils.plotting import plot_series\n",
    "    from sktime.forecasting.compose import make_reduction\n",
    "    from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "    \n",
    "    from sktime.performance_metrics.forecasting import mean_absolute_percentage_error\n",
    "    from sklearn.metrics import accuracy_score\n",
    "        \n",
    "    print('#1 index set single/multi/hierarchical')\n",
    "    if len(index_cols_list) ==1: # single level index\n",
    "        print(f'data-typ:{type(data)}, data-shape:{data.shape}')\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            #data[index_col] = pd.to_datetime(data[index_col]) #TBC\n",
    "            if freq:\n",
    "                #Note1: convert partial str date(yyyy-mm or yy-q) to complete date format(yyyy-mm-dd)\n",
    "                data[index_col] = pd.PeriodIndex(data[index_col], freq=freq).to_timestamp()\n",
    "                #Note2: convert complete date format(yyyy-mm-dd) to date index w.r.t freq (yyyyqq)\n",
    "                data = data.set_index(index_col, drop=True).to_period(freq) #set index of \n",
    "            else:\n",
    "                data.set_index(index_col, drop=True, inplace=True) # convert to series\n",
    "    else: # multi level index for series/df\n",
    "        data = set_index(data, index_cols_list) #set hierarchical index/multi_index\n",
    "        \n",
    "    #check index level eg. no of features in index\n",
    "    if len(data.index.levels)==1:\n",
    "        index_type = 'UniLevalIndex'\n",
    "    elif len(data.index.levels)==2:\n",
    "        index_type = 'BiLevalIndex'\n",
    "    elif len(data.index.levels)>=3:\n",
    "        index_type = 'MultiLevalIndex'\n",
    "    print('index_type:', index_type)\n",
    "            \n",
    "    print('#2 data chk/split and chk UniVariate/MultiVariate')\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        if feat_list:\n",
    "            X = data[feat_list]\n",
    "            y = data[target_col]\n",
    "            y_train, y_valid, X_train, X_valid = temporal_train_test_split(y, X, test_size=30)\n",
    "            ts_type = 'MultiVariate'\n",
    "            print(f'df-y_train:{y_train.shape}, y_valid:{y_valid.shape}, X_train:{X_train.shape}, X_valid:{X_valid.shape}')\n",
    "        else:\n",
    "            X = None\n",
    "            y = data[target_col]\n",
    "            y_train, y_valid = temporal_train_test_split(y, test_size=30)\n",
    "            ts_type = 'UniVariate'\n",
    "            print(f'df-srs-y_train:{y_train.shape}, y_valid:{y_valid.shape}')\n",
    "            \n",
    "    elif isinstance(data, pd.Series):\n",
    "        X = None\n",
    "        y = data\n",
    "        y_train, y_valid = temporal_train_test_split(y, test_size=30)\n",
    "        ts_type = 'UniVariate'\n",
    "        print(f'srs-y_train:{y_train.shape}, y_valid:{y_valid.shape}')\n",
    "    print('ts_type:', ts_type)\n",
    "       \n",
    "    print('target transformer') #Not needed if pipeline is passed\n",
    "    if transformer_type:\n",
    "        if transformer_type == 'BoxCoxTransformer':\n",
    "            from sktime.transformations.series.boxcox import BoxCoxTransformer\n",
    "            transformer = BoxCoxTransformer(sp=4) #Box-Cox power transformation\n",
    "        elif transformer_type == 'LogTransformer':\n",
    "            from sktime.transformations.series.boxcox import LogTransformer\n",
    "            transformer = LogTransformer()\n",
    "        y_train = transformer.fit_transform(y_train)\n",
    "        \n",
    "    print('#create score dict')\n",
    "    score_dict = {}\n",
    "    score_dict['ts_type'] = ts_type\n",
    "    score_dict['model_type'] = model_type\n",
    "    \n",
    "    print('#get ForecastingHorizon')\n",
    "    fh_abs = ForecastingHorizon(y_valid.index, is_relative=False)\n",
    "    print('fh_abs:',fh_abs) # y_valid index\n",
    "    fh_re =[1,2,3] #next index forecast / not correct\n",
    "    \n",
    "    print(f'#executing:{model_type}')\n",
    "    if model_type == 'NaiveForecaster':\n",
    "        #Forecast based on naive assumptions about past trends continuing, supports univariate/multivariate\n",
    "        from sktime.forecasting.naive import NaiveForecaster, NaiveVariance\n",
    "        forecaster = NaiveForecaster()\n",
    "        \n",
    "        #NaiveVariance adds to a `forecaster` the ability to compute the prediction variance based on naive assumptions about ts.\n",
    "        #forecaster = NaiveForecaster(strategy=\"drift\")\n",
    "        #forecaster = NaiveVariance(forecaster)\n",
    "    elif model_type == 'PolynomialTrend':\n",
    "        #linear regression model with a 1st degree polynomial transformation of the feature.\n",
    "        from sktime.forecasting.trend import PolynomialTrendForecaster\n",
    "        forecaster = PolynomialTrendForecaster(degree=1)\n",
    "    elif model_type == 'AutoArima':\n",
    "        from sktime.forecasting.arima import AutoARIMA\n",
    "        forecaster = AutoARIMA(sp=4)\n",
    "    elif model_type == 'ThetaForecaster':\n",
    "        #it's equivalent to simple exponential smoothing (SES) with drift/tested for seasonality\n",
    "        from sktime.forecasting.theta import ThetaForecaster\n",
    "        forecaster = ThetaForecaster(sp=12)\n",
    "        \n",
    "    elif model_type == 'ExponentialSmoothing':\n",
    "        from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "        forecaster = ExponentialSmoothing(trend=\"add\", sp=12)\n",
    "        \n",
    "    #TBC it's Classifier   \n",
    "    elif model_type == 'TimeSeriesForestClassifier': \n",
    "        from sktime.classification.compose import TimeSeriesForestClassifier\n",
    "        forecaster = TimeSeriesForestClassifier()\n",
    "        \n",
    "    else:\n",
    "        print(f'#executing Regression:{model_type}')\n",
    "        if regression_window:\n",
    "            '''\n",
    "            strategy=\n",
    "            Direct:create a separate model for each period we are forecasting.\n",
    "            Recursive:fit a single one-step ahead model,previous time steps output for the following input\n",
    "            Multiple outputs: one model is used to predict the entire time series horizon in a single forecast.\n",
    "            '''\n",
    "            forecaster = make_reduction(model_type, strategy=\"recursive\", window_length= regression_window)\n",
    "            #forecaster = ReducedRegressionForecaster(regressor, window_length=12)\n",
    "    \n",
    "    print('Executing pipeline:', pipeline)\n",
    "    if pipeline:\n",
    "        print('#5 add model last step of pipeline')    \n",
    "        model_name = forecaster.__class__.__name__\n",
    "\n",
    "        if pipeline.steps[-1][0] == 'model':\n",
    "            pipeline.steps.pop(-1)\n",
    "            pipeline.steps.append(('model', forecaster))\n",
    "        else:\n",
    "            pipeline.steps.append(('model', forecaster))\n",
    "            \n",
    "        from copy import deepcopy\n",
    "        forecaster = deepcopy(pipeline)\n",
    "    print('forecaster:', forecaster)\n",
    "    \n",
    "    print('#cv_type:', cv_type)\n",
    "    from sktime.forecasting.model_selection import SlidingWindowSplitter, ExpandingWindowSplitter\n",
    "    from sktime.forecasting.model_selection import SingleWindowSplitter\n",
    "    \n",
    "    X_train = None if ts_type == 'UniVariate' else X_train # multivarite X_train will be used\n",
    "    window_size = round(y_train.size/4) #TBC\n",
    "    if cv_type == 'ExpandingWindowSplitter':\n",
    "         #note: initial_window + fh shold be smaller than the length of y\n",
    "        cv = ExpandingWindowSplitter(step_length=window_size, fh=list(range(1, window_size)), initial_window=window_size)\n",
    "    elif cv_type == 'SlidingWindowSplitter':\n",
    "        #note: initial_window must greater than `window_length`\n",
    "        cv = SlidingWindowSplitter(initial_window=window_size*2, window_length=window_size)\n",
    "    elif cv_type == 'SingleWindowSplitter':\n",
    "        #note: initial_window must greater than `window_length`\n",
    "        #validation_size = 26\n",
    "        validation_size = y_train.size\n",
    "        cv = SingleWindowSplitter(window_length=len(y)-validation_size, fh=validation_size)\n",
    "        \n",
    "        \n",
    "    print('#train_type:',train_type, ts_type)\n",
    "    print(f'y_train:{y_train.shape}, y_valid:{y_valid.shape}')\n",
    "    if train_type == 'TrainTest':\n",
    "        if ts_type == 'UniVariate':\n",
    "            forecaster.fit(y_train)\n",
    "            y_pred = forecaster.predict(fh=fh_abs)\n",
    "        elif ts_type == 'MultiVariate':\n",
    "            print(f'X_train:{X_train.shape}, X_valid:{X_valid.shape}')\n",
    "            forecaster.fit(y_train, X_train)\n",
    "            y_pred = forecaster.predict(X=X_valid, fh=fh_abs)\n",
    "            \n",
    "        #score_dict update\n",
    "        fs_score_dict = custom_mape(y_valid, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_valid, y_pred, symmetric=False)\n",
    "        score_dict['mape'] = mape\n",
    "        score_dict.update(fs_score_dict)\n",
    "        score_dict['y_pred'] = y_pred\n",
    "        \n",
    "    elif train_type == 'CrossValidation':\n",
    "        from sktime.forecasting.model_evaluation import evaluate\n",
    "        pred_df = evaluate(forecaster=forecaster, X=X_train, y=y_train, cv=cv, strategy=\"refit\", return_data=True)\n",
    "        \n",
    "        #score_dict update\n",
    "        score_dict['pred_df'] = pred_df\n",
    "        #mape = mean_absolute_percentage_error(y_valid, y_pred, symmetric=False) #TBC\n",
    "        #score_dict['mape'] = mape\n",
    "        \n",
    "    elif train_type in ['GridSearch', 'RandomizedSearch']:\n",
    "        from sktime.forecasting.model_selection import ForecastingGridSearchCV, ForecastingRandomizedSearchCV\n",
    "        if train_type == 'GridSearch':\n",
    "            forecaster = ForecastingGridSearchCV(forecaster, strategy=\"refit\", cv=cv, param_grid=nested_params)\n",
    "        elif train_type == 'RandomizedSearch':\n",
    "            forecaster = ForecastingRandomizedSearchCV(forecaster, strategy=\"refit\", cv=cv, \n",
    "                        param_distributions=nested_params, n_iter=1, random_state=42)\n",
    "            \n",
    "        if ts_type == 'UniVariate':\n",
    "            forecaster.fit(y_train)\n",
    "            y_pred = forecaster.predict(fh=fh_abs)\n",
    "            #y_pred = forecaster.predict(np.arange(1, y_valid.size+1))\n",
    "        elif ts_type == 'MultiVariate':\n",
    "            print(f'X_train:{X_train.shape}, X_valid:{X_valid.shape}')\n",
    "            forecaster.fit(y_train, X_train)\n",
    "            y_pred = forecaster.predict(X=X_valid, fh=fh_abs)\n",
    "        \n",
    "        #score_dict update\n",
    "        mape = mean_absolute_percentage_error(y_valid, y_pred, symmetric=False)\n",
    "        score_dict['mape'] = mape\n",
    "        score_dict['y_pred'] = y_pred\n",
    "        score_dict['pipeline'] = pipeline\n",
    "    \n",
    "    # check probabilistic predictions support\n",
    "    if forecaster.get_tag(\"capability:pred_int\"):\n",
    "        forecast_intervals = forecaster.predict_interval(coverage=0.9)\n",
    "        forecast_quantiles = forecaster.predict_quantiles(fh=None, X=None, alpha=[0.05, 0.95])\n",
    "        forecast_variance = forecaster.predict_var(fh=None, X=None, cov=False)\n",
    "        #forecast_proba = forecaster.predict_proba(fh=None, X=None, marginal=True)\n",
    "        score_dict['forecast_intervals'] = forecast_intervals\n",
    "        score_dict['forecast_quantiles'] = forecast_quantiles\n",
    "        score_dict['forecast_variance'] = forecast_variance\n",
    "        #score_dict['forecast_proba'] = forecast_proba #TBC\n",
    "\n",
    "\n",
    "    print('#performance_metrics - forecasting score')\n",
    "    from sktime.performance_metrics.forecasting import make_forecasting_scorer\n",
    "        \n",
    "    #PlotSeries = False # temp dueto train_type == 'CrossValidation'\n",
    "    if PlotSeries:\n",
    "        #if train_type == 'TrainTest':\n",
    "        if train_type in ['TrainTest', 'GridSearch', 'RandomizedSearch']:\n",
    "            print(f'df-y_train:{y_train.shape}, y_valid:{y_valid.shape}, y_pred:{y_pred.shape}')\n",
    "            plot_series(y_train, y_valid, y_pred, labels=[\"y_train\", \"y_valid\", \"y_pred\"]);\n",
    "        elif train_type == 'CrossValidation':\n",
    "            n_cv = pred_df.shape[0]\n",
    "            plot_series(\n",
    "                y, *[pred_df[\"y_pred\"].iloc[x] for x in range(n_cv)],\n",
    "                markers=[\"o\", *[\".\"]*n_cv],\n",
    "                labels=[\"y_true\"] + [f\"cv: {x}\" for x in range(n_cv)]\n",
    "            );\n",
    "        \n",
    "    print('exit ts_forecast_models')\n",
    "    return score_dict\n",
    "\n",
    "#tt_cv_pt_models_ts\n",
    "\n",
    "ts_model_dict = function_parser(function_name=tt_cv_pt_models_ts, search_string='model_type ==')\n",
    "ts_model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5016f895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa21b60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry create_ts_pipeline\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RangeIndex' object has no attribute 'levels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 456\u001b[0m\n\u001b[0;32m    454\u001b[0m pp_steps_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutlier\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHampelFilter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogTransformer\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m    455\u001b[0m pp_steps_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeatureExtraction\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 456\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_ts_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpp_steps_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpp_steps_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpipeline:\u001b[39m\u001b[38;5;124m'\u001b[39m, pipeline)\n\u001b[0;32m    459\u001b[0m pipeline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 30\u001b[0m, in \u001b[0;36mcreate_ts_pipeline\u001b[1;34m(data, pp_steps_dict, pp_steps_list)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntry create_ts_pipeline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#mandetory check\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mdf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlevels\u001b[49m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m     31\u001b[0m     ts_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHierarchicalIndex_TS\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df1\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mlevels) \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m3\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RangeIndex' object has no attribute 'levels'"
     ]
    }
   ],
   "source": [
    "#v1.1 testing ts_forecast_models with ForecastingPipeline\n",
    "\n",
    "def create_ts_pipeline(data, pp_steps_dict, pp_steps_list):\n",
    "    '''\n",
    "    1.1 Jan 29,2023 added index chk,multi index\n",
    "    1.0 Jan 23,2023 initial ver\n",
    "    \n",
    "    TBA\n",
    "    TransformedTargetForecaster-takes a chain of transformers and forecasters\n",
    "    MultiplexForecaster\n",
    "    TSFreshFeatureExtractor\n",
    "    WindowSummarizer: mean/stdev/n_legs features\n",
    "    DateTimeFeatures: date features\n",
    "    \n",
    "    TBC\n",
    "    diff between TransformedTargetForecaster and ForecastingPipeline\n",
    "    '''\n",
    "    \n",
    "    from sktime.forecasting.compose import ForecastingPipeline\n",
    "    from sktime.transformations.series.outlier_detection import HampelFilter\n",
    "    from sktime.transformations.series.boxcox import LogTransformer, BoxCoxTransformer\n",
    "    from sktime.transformations.series.adapt import TabularToSeriesAdaptor\n",
    "    from sktime.transformations.series.impute import Imputer\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sktime.forecasting.theta import ThetaForecaster\n",
    "    \n",
    "    df1 = data.copy()\n",
    "    print('Entry create_ts_pipeline')\n",
    "    #mandetory check\n",
    "    if len(df1.index.levels) >=3:\n",
    "        ts_type = 'HierarchicalIndex_TS'\n",
    "    elif len(df1.index.levels) ==3:\n",
    "        pass\n",
    "    elif len(df1.index.levels) ==2:\n",
    "        pass\n",
    "    elif len(df1.index.levels) ==1:\n",
    "        pass\n",
    "    \n",
    "    if ts_type == 'HierarchicalIndex_TS':\n",
    "        from sktime.transformations.hierarchical.aggregate import Aggregator\n",
    "        aggregator = Aggregator()\n",
    "    else:\n",
    "        aggregator = 'passthrough'\n",
    "        \n",
    "    \n",
    "    if pp_steps_dict:\n",
    "        #2.1 ImputerCat/ImputerNum\n",
    "        if 'Outlier' in pp_steps_dict:\n",
    "            if pp_steps_dict['Outlier'] == 'HampelFilter':\n",
    "                outlier = HampelFilter()\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            outlier = 'passthrough'\n",
    "            \n",
    "        \n",
    "        if 'Transformer' in pp_steps_dict:\n",
    "            if pp_steps_dict['Transformer'] == 'LogTransformer':\n",
    "                transformer = LogTransformer()\n",
    "            elif pp_steps_dict['Transformer'] == 'BoxCoxTransformer':\n",
    "                transformer = BoxCoxTransformer(sp=4)\n",
    "            else:\n",
    "                transformer = 'passthrough'\n",
    "    \n",
    "    #5 optional pp_steps_list\n",
    "    print('\\n ##5 pp_steps_list: setup flags MatchFeat, DropNullRows, DropDupRows, DropDupCols, DropQuasiConstCols')\n",
    "    if pp_steps_list:\n",
    "        if 'FeatureExtraction' in pp_steps_list:\n",
    "            from sktime.transformations.panel.tsfresh import TSFreshFeatureExtractor\n",
    "            FeatureExtraction = TSFreshFeatureExtractor(default_fc_parameters=\"efficient\", show_warnings=False)\n",
    "        else:\n",
    "            FeatureExtraction = 'passthrough'\n",
    "            \n",
    "            \n",
    "            \n",
    "    pipeline = ForecastingPipeline(\n",
    "    [\n",
    "    (\"imputer\", Imputer(method=\"mean\")),\n",
    "    (\"minmaxscaler\", TabularToSeriesAdaptor(MinMaxScaler())),\n",
    "    ('aggregator', aggregator), # for Hierarchical TS\n",
    "    (\"outlier\", outlier),\n",
    "    (\"transformer\", transformer),\n",
    "    (\"model\", ThetaForecaster(sp=4)) #defaut otherwise error\n",
    "    ])\n",
    "    \n",
    "    print('Exit create_ts_pipeline')\n",
    "    return pipeline\n",
    "    \n",
    "import numpy as np\n",
    "def custom_mape(y: np.array, y_pred: np.array):\n",
    "    import numpy as np\n",
    "    \n",
    "    metrics_dict = {'score_mean': np.mean(np.abs((y - np.ceil(y_pred)) / y)),\n",
    "                    'score_row': np.abs((y - np.ceil(y_pred)) / y)}\n",
    "    return metrics_dict\n",
    "\n",
    "def set_index(data, index_cols_list=[]):\n",
    "    '''\n",
    "    index_type = Int64Index, RangeIndex, DatetimeIndex, PeriodIndex\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    \n",
    "    df = data.copy()\n",
    "    index = pd.MultiIndex.from_frame(df[index_cols_list])\n",
    "    df.set_index(index, drop=True, inplace=True)\n",
    "    df.drop(index_cols_list, axis=1, inplace=True)\n",
    "    \n",
    "    #check duplicate values\n",
    "    if df.index.has_duplicates:\n",
    "        pass\n",
    "    \n",
    "    #check index level eg. no of features in index\n",
    "    if len(index.levels)==1:\n",
    "        ts_type = 'NormalIndex_TS'\n",
    "    elif len(index.levels)==2:\n",
    "        ts_type = 'MultiIndex_TS'\n",
    "    elif len(index.levels)>=3:\n",
    "        ts_type = 'HierarchicalIndex_TS'\n",
    "    print('ts_type:', ts_type)\n",
    "    return df\n",
    "\n",
    "# def ts_forecast_models(data, target_col, feat_list, model_type, pipeline, index_cols_list=[], nested_params=None, train_type='TrainTest',\n",
    "#         forecast_period=5, freq='M', regression_window=10, cv_type='ExpandingWindowSplitter',\n",
    "#         pt_type='ForecastingRandomizedSearch', transformer_type='BoxCoxTransformer', PlotSeries=True):\n",
    "    \n",
    "def tt_cv_pt_models_ts(data, target_col, feat_list, model_type, pipeline, index_cols_list=[], nested_params=None, train_type='TrainTest',\n",
    "        forecast_period=5, freq='M', regression_window=10, cv_type='ExpandingWindowSplitter',\n",
    "        pt_type='ForecastingRandomizedSearch', transformer_type='BoxCoxTransformer', PlotSeries=True):\n",
    "    '''\n",
    "    \n",
    "    v1.3 Jan26,2023: Updated for new model_type/ added probabilistic predictions support\n",
    "    v1.2 Jan23,2023: Updated for pipeline\n",
    "    v1.1 Jan20,2023: added regression_window, cv_type, pt_type, forecasting score/CrossValidation\n",
    "    v1.0 Jan18,2023: initial version\n",
    "    ts_type: UniVariate/MultiVariate\n",
    "    \n",
    "    \n",
    "    \n",
    "    pipeline:if pipeline true then model_type (model) will update in pipeline that will be fitted else model_type will be fitted\n",
    "    train_type: TrainTest/CrossValidation/GridSearch/RandomizedSearch\n",
    "    freq: used to set index whern data is as df with date col\n",
    "    regression_window: when regression models are used\n",
    "    cv_type: used in cross_validation/GridSerach - ExpandingWindowSplitter/SlidingWindowSplitter\n",
    "    pt_type: used in parameter tuning - ForecastingGridSearch/ForecastingRandomizedSearch\n",
    "    \n",
    "    Test issues\n",
    "    Auto_Arima with cross_validation TypeError\n",
    "    TypeError: input must be a one of (<class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.series.Series'>), but found type: <class 'float'>\n",
    "    '''\n",
    "    print('entry ts_forecast_models')\n",
    "    \n",
    "    #import libs\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sktime.forecasting.base import ForecastingHorizon\n",
    "    from sktime.utils.plotting import plot_series\n",
    "    from sktime.forecasting.compose import make_reduction\n",
    "    from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "    \n",
    "    from sktime.performance_metrics.forecasting import mean_absolute_percentage_error\n",
    "        \n",
    "    print('#1 index set single/multi/hierarchical')\n",
    "    if len(index_cols_list) ==1: # single level index\n",
    "        print(f'data-typ:{type(data)}, data-shape:{data.shape}')\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            #data[index_col] = pd.to_datetime(data[index_col]) #TBC\n",
    "            if freq:\n",
    "                #Note1: convert partial str date(yyyy-mm or yy-q) to complete date format(yyyy-mm-dd)\n",
    "                data[index_col] = pd.PeriodIndex(data[index_col], freq=freq).to_timestamp()\n",
    "                #Note2: convert complete date format(yyyy-mm-dd) to date index w.r.t freq (yyyyqq)\n",
    "                data = data.set_index(index_col, drop=True).to_period(freq) #set index of \n",
    "            else:\n",
    "                data.set_index(index_col, drop=True, inplace=True) # convert to series\n",
    "    else: # multi level index for series/df\n",
    "        data = set_index(data, index_cols_list) #set hierarchical index/multi_index\n",
    "        \n",
    "    #check index level eg. no of features in index\n",
    "    if len(data.index.levels)==1:\n",
    "        index_type = 'UniLevalIndex'\n",
    "    elif len(data.index.levels)==2:\n",
    "        index_type = 'BiLevalIndex'\n",
    "    elif len(data.index.levels)>=3:\n",
    "        index_type = 'MultiLevalIndex'\n",
    "    print('index_type:', index_type)\n",
    "            \n",
    "    print('#2 data chk/split and chk UniVariate/MultiVariate')\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        if feat_list:\n",
    "            X = data[feat_list]\n",
    "            y = data[target_col]\n",
    "            y_train, y_valid, X_train, X_valid = temporal_train_test_split(y, X, test_size=30)\n",
    "            ts_type = 'MultiVariate'\n",
    "            print(f'df-y_train:{y_train.shape}, y_valid:{y_valid.shape}, X_train:{X_train.shape}, X_valid:{X_valid.shape}')\n",
    "        else:\n",
    "            X = None\n",
    "            y = data[target_col]\n",
    "            y_train, y_valid = temporal_train_test_split(y, test_size=30)\n",
    "            ts_type = 'UniVariate'\n",
    "            print(f'df-srs-y_train:{y_train.shape}, y_valid:{y_valid.shape}')\n",
    "            \n",
    "    elif isinstance(data, pd.Series):\n",
    "        X = None\n",
    "        y = data\n",
    "        y_train, y_valid = temporal_train_test_split(y, test_size=30)\n",
    "        ts_type = 'UniVariate'\n",
    "        print(f'srs-y_train:{y_train.shape}, y_valid:{y_valid.shape}')\n",
    "    print('ts_type:', ts_type)\n",
    "       \n",
    "    print('target transformer') #Not needed if pipeline is passed\n",
    "    if transformer_type:\n",
    "        if transformer_type == 'BoxCoxTransformer':\n",
    "            from sktime.transformations.series.boxcox import BoxCoxTransformer\n",
    "            transformer = BoxCoxTransformer(sp=4) #Box-Cox power transformation\n",
    "        elif transformer_type == 'LogTransformer':\n",
    "            from sktime.transformations.series.boxcox import LogTransformer\n",
    "            transformer = LogTransformer()\n",
    "        y_train = transformer.fit_transform(y_train)\n",
    "        \n",
    "    print('#create score dict')\n",
    "    score_dict = {}\n",
    "    score_dict['ts_type'] = ts_type\n",
    "    score_dict['model_type'] = model_type\n",
    "    \n",
    "    print('#get ForecastingHorizon')\n",
    "    fh_abs = ForecastingHorizon(y_valid.index, is_relative=False)\n",
    "    print('fh_abs:',fh_abs) # y_valid index\n",
    "    fh_re =[1,2,3] #next index forecast / not correct\n",
    "    \n",
    "    print(f'#executing:{model_type}')\n",
    "    if model_type == 'NaiveForecaster':\n",
    "        #Forecast based on naive assumptions about past trends continuing, supports univariate/multivariate\n",
    "        from sktime.forecasting.naive import NaiveForecaster, NaiveVariance\n",
    "        forecaster = NaiveForecaster()\n",
    "        \n",
    "        #NaiveVariance adds to a `forecaster` the ability to compute the prediction variance based on naive assumptions about ts.\n",
    "        #forecaster = NaiveForecaster(strategy=\"drift\")\n",
    "        #forecaster = NaiveVariance(forecaster)\n",
    "    elif model_type == 'PolynomialTrend':\n",
    "        #linear regression model with a 1st degree polynomial transformation of the feature.\n",
    "        from sktime.forecasting.trend import PolynomialTrendForecaster\n",
    "        forecaster = PolynomialTrendForecaster(degree=1)\n",
    "    elif model_type == 'AutoArima':\n",
    "        from sktime.forecasting.arima import AutoARIMA\n",
    "        forecaster = AutoARIMA(sp=4)\n",
    "    elif model_type == 'ThetaForecaster':\n",
    "        #it's equivalent to simple exponential smoothing (SES) with drift/tested for seasonality\n",
    "        from sktime.forecasting.theta import ThetaForecaster\n",
    "        forecaster = ThetaForecaster()\n",
    "        \n",
    "    elif model_type == 'ExponentialSmoothing':\n",
    "        from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "        forecaster = ExponentialSmoothing(trend=\"add\", sp=12)\n",
    "        \n",
    "    elif model_type == 'ABC':\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    elif model_type == 'TimeSeriesForestClassifier': #TBC it's Classifier\n",
    "        from sktime.classification.compose import TimeSeriesForestClassifier\n",
    "        forecaster = TimeSeriesForestClassifier()\n",
    "        \n",
    "    else:\n",
    "        print(f'#executing Regression:{model_type}')\n",
    "        if regression_window:\n",
    "            '''\n",
    "            strategy=\n",
    "            Direct:create a separate model for each period we are forecasting.\n",
    "            Recursive:fit a single one-step ahead model,previous time steps output for the following input\n",
    "            Multiple outputs: one model is used to predict the entire time series horizon in a single forecast.\n",
    "            '''\n",
    "            forecaster = make_reduction(model_type, strategy=\"recursive\", window_length= regression_window)\n",
    "            #forecaster = ReducedRegressionForecaster(regressor, window_length=12)\n",
    "    \n",
    "    print('Executing pipeline:', pipeline)\n",
    "    if pipeline:\n",
    "        print('#5 add model last step of pipeline')    \n",
    "        model_name = forecaster.__class__.__name__\n",
    "\n",
    "        if pipeline.steps[-1][0] == 'model':\n",
    "            pipeline.steps.pop(-1)\n",
    "            pipeline.steps.append(('model', forecaster))\n",
    "        else:\n",
    "            pipeline.steps.append(('model', forecaster))\n",
    "            \n",
    "        from copy import deepcopy\n",
    "        forecaster = deepcopy(pipeline)\n",
    "    print('forecaster:', forecaster)\n",
    "    \n",
    "    print('#cv_type:', cv_type)\n",
    "    from sktime.forecasting.model_selection import SlidingWindowSplitter, ExpandingWindowSplitter\n",
    "    from sktime.forecasting.model_selection import SingleWindowSplitter\n",
    "    \n",
    "    X_train = None if ts_type == 'UniVariate' else X_train # multivarite X_train will be used\n",
    "    window_size = round(y_train.size/4) #TBC\n",
    "    if cv_type == 'ExpandingWindowSplitter':\n",
    "         #note: initial_window + fh shold be smaller than the length of y\n",
    "        cv = ExpandingWindowSplitter(step_length=window_size, fh=list(range(1, window_size)), initial_window=window_size)\n",
    "    elif cv_type == 'SlidingWindowSplitter':\n",
    "        #note: initial_window must greater than `window_length`\n",
    "        cv = SlidingWindowSplitter(initial_window=window_size*2, window_length=window_size)\n",
    "    elif cv_type == 'SingleWindowSplitter':\n",
    "        #note: initial_window must greater than `window_length`\n",
    "        #validation_size = 26\n",
    "        validation_size = y_train.size\n",
    "        cv = SingleWindowSplitter(window_length=len(y)-validation_size, fh=validation_size)\n",
    "        \n",
    "        \n",
    "    print('#train_type:',train_type, ts_type)\n",
    "    print(f'y_train:{y_train.shape}, y_valid:{y_valid.shape}')\n",
    "    if train_type == 'TrainTest':\n",
    "        if ts_type == 'UniVariate':\n",
    "            forecaster.fit(y_train)\n",
    "            y_pred = forecaster.predict(fh=fh_abs)\n",
    "        elif ts_type == 'MultiVariate':\n",
    "            print(f'X_train:{X_train.shape}, X_valid:{X_valid.shape}')\n",
    "            forecaster.fit(y_train, X_train)\n",
    "            y_pred = forecaster.predict(X=X_valid, fh=fh_abs)\n",
    "            \n",
    "        #score_dict update\n",
    "        fs_score_dict = custom_mape(y_valid, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_valid, y_pred, symmetric=False)\n",
    "        score_dict['mape'] = mape\n",
    "        score_dict.update(fs_score_dict)\n",
    "        score_dict['y_pred'] = y_pred\n",
    "        \n",
    "    elif train_type == 'CrossValidation':\n",
    "        from sktime.forecasting.model_evaluation import evaluate\n",
    "        pred_df = evaluate(forecaster=forecaster, X=X_train, y=y_train, cv=cv, strategy=\"refit\", return_data=True)\n",
    "        \n",
    "        #score_dict update\n",
    "        score_dict['pred_df'] = pred_df\n",
    "        #mape = mean_absolute_percentage_error(y_valid, y_pred, symmetric=False) #TBC\n",
    "        #score_dict['mape'] = mape\n",
    "        \n",
    "    elif train_type in ['GridSearch', 'RandomizedSearch']:\n",
    "        from sktime.forecasting.model_selection import ForecastingGridSearchCV, ForecastingRandomizedSearchCV\n",
    "        if train_type == 'GridSearch':\n",
    "            forecaster = ForecastingGridSearchCV(forecaster, strategy=\"refit\", cv=cv, param_grid=nested_params)\n",
    "        elif train_type == 'RandomizedSearch':\n",
    "            forecaster = ForecastingRandomizedSearchCV(forecaster, strategy=\"refit\", cv=cv, \n",
    "                        param_distributions=nested_params, n_iter=1, random_state=42)\n",
    "            \n",
    "        if ts_type == 'UniVariate':\n",
    "            forecaster.fit(y_train)\n",
    "            y_pred = forecaster.predict(fh=fh_abs)\n",
    "            #y_pred = forecaster.predict(np.arange(1, y_valid.size+1))\n",
    "        elif ts_type == 'MultiVariate':\n",
    "            print(f'X_train:{X_train.shape}, X_valid:{X_valid.shape}')\n",
    "            forecaster.fit(y_train, X_train)\n",
    "            y_pred = forecaster.predict(X=X_valid, fh=fh_abs)\n",
    "        \n",
    "        #score_dict update\n",
    "        mape = mean_absolute_percentage_error(y_valid, y_pred, symmetric=False)\n",
    "        score_dict['mape'] = mape\n",
    "        score_dict['y_pred'] = y_pred\n",
    "        score_dict['pipeline'] = pipeline\n",
    "    \n",
    "    # check probabilistic predictions support\n",
    "    if forecaster.get_tag(\"capability:pred_int\"):\n",
    "        forecast_intervals = forecaster.predict_interval(coverage=0.9)\n",
    "        forecast_quantiles = forecaster.predict_quantiles(fh=None, X=None, alpha=[0.05, 0.95])\n",
    "        forecast_variance = forecaster.predict_var(fh=None, X=None, cov=False)\n",
    "        #forecast_proba = forecaster.predict_proba(fh=None, X=None, marginal=True)\n",
    "        score_dict['forecast_intervals'] = forecast_intervals\n",
    "        score_dict['forecast_quantiles'] = forecast_quantiles\n",
    "        score_dict['forecast_variance'] = forecast_variance\n",
    "        #score_dict['forecast_proba'] = forecast_proba #TBC\n",
    "\n",
    "\n",
    "    print('#performance_metrics - forecasting score')\n",
    "    from sktime.performance_metrics.forecasting import make_forecasting_scorer\n",
    "        \n",
    "    #PlotSeries = False # temp dueto train_type == 'CrossValidation'\n",
    "    if PlotSeries:\n",
    "        #if train_type == 'TrainTest':\n",
    "        if train_type in ['TrainTest', 'GridSearch', 'RandomizedSearch']:\n",
    "            print(f'df-y_train:{y_train.shape}, y_valid:{y_valid.shape}, y_pred:{y_pred.shape}')\n",
    "            plot_series(y_train, y_valid, y_pred, labels=[\"y_train\", \"y_valid\", \"y_pred\"]);\n",
    "        elif train_type == 'CrossValidation':\n",
    "            n_cv = pred_df.shape[0]\n",
    "            plot_series(\n",
    "                y, *[pred_df[\"y_pred\"].iloc[x] for x in range(n_cv)],\n",
    "                markers=[\"o\", *[\".\"]*n_cv],\n",
    "                labels=[\"y_true\"] + [f\"cv: {x}\" for x in range(n_cv)]\n",
    "            );\n",
    "        \n",
    "    print('exit ts_forecast_models')\n",
    "    return score_dict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#1 univariate/series /done\n",
    "# from sktime.datasets import load_airline\n",
    "# data = load_airline()\n",
    "# target_col = None\n",
    "# index_col = None\n",
    "# feat_list = None\n",
    "\n",
    "#2 univariate/df / data issue\n",
    "# data_path='D:\\\\timeseries\\\\'\n",
    "\n",
    "# data = pd.read_csv(data_path+'shampoo-sales.csv')\n",
    "# target_col = 'Sales'\n",
    "# index_col = 'Month'\n",
    "# feat_list = None\n",
    "# data[index_col] = data[index_col].apply(lambda x:date_parser(x)) # convert date format\n",
    "\n",
    "#3 univariate/df\n",
    "# import statsmodels.api as sm\n",
    "# data = sm.datasets.macrodata.load_pandas()['data']\n",
    "\n",
    "# data['q_date'] = data.apply(lambda x: str(int(x['year'])) + '-Q' \n",
    "#                                   + str(int(x['quarter'])), axis=1)\n",
    "# target_col = 'realgdp'\n",
    "# index_col = 'q_date'\n",
    "# feat_list = None\n",
    "# #train_type='TrainTest'\n",
    "# train_type='CrossValidation'\n",
    "\n",
    "#4 multivariate/df/done\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data = sm.datasets.macrodata.load_pandas()['data']\n",
    "data['q_date'] = data.apply(lambda x: str(int(x['year'])) + '-Q' \n",
    "                                  + str(int(x['quarter'])), axis=1)\n",
    "target_col = 'realgdp'\n",
    "index_col = 'q_date'\n",
    "feat_list = ['realgdp', 'realcons', 'realinv', 'realgovt'] \n",
    "#feat_list = None\n",
    "\n",
    "freq='Q'\n",
    "train_type_list = ['TrainTest', 'CrossValidation', 'GridSearch', 'RandomizedSearch']\n",
    "train_type = train_type_list[0]\n",
    "#cv_type = 'ExpandingWindowSplitter'\n",
    "cv_type = 'SlidingWindowSplitter'\n",
    "nested_params = {\"window_length\": list(range(2,21)), \n",
    "                 #\"estimator__max_depth\": list(range(5,16))\n",
    "                }\n",
    "param_grid = nested_params\n",
    "\n",
    "#testing\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#regressor = GradientBoostingRegressor()\n",
    "regressor = XGBRegressor()\n",
    "\n",
    "model_list = ['PolynomialTrend','NaiveForecaster', 'AutoArima', 'ThetaForecaster', regressor,\n",
    "             'ExponentialSmoothing']\n",
    "model_type = model_list[1]\n",
    "\n",
    "#1 create_ts_pipeline()\n",
    "pp_steps_dict = {'Outlier': 'HampelFilter', 'Transformer': 'LogTransformer'}\n",
    "pp_steps_list = ['FeatureExtraction']\n",
    "pipeline = create_ts_pipeline(data, pp_steps_dict, pp_steps_list)\n",
    "print('pipeline:', pipeline)\n",
    "\n",
    "pipeline=None\n",
    "#2 ts_forecast_models()\n",
    "score_dict = ts_forecast_models(data, target_col, index_col, feat_list, model_type, pipeline, nested_params=param_grid, train_type=train_type,\n",
    "        forecast_period=5, freq=freq, regression_window=5, cv_type = cv_type, PlotSeries=True)\n",
    "print('score_dict:', score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962e4140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5778ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate-multi-timeseries horizonal/multi output/ VAR Sktime\n",
    "#https://www.sktime.org/en/latest/api_reference/auto_generated/sktime.forecasting.var.VAR.html\n",
    "\n",
    "#1 VAR\n",
    "from sktime.forecasting.var import VAR\n",
    "from sktime.datasets import load_longley\n",
    "# _, y = load_longley()\n",
    "# forecaster = VAR()  \n",
    "# forecaster.fit(y)  \n",
    "\n",
    "# y_pred = forecaster.predict(fh=[1,2,3])  \n",
    "\n",
    "#2 VARMAX\n",
    "from sktime.forecasting.varmax import VARMAX\n",
    "from sktime.datasets import load_macroeconomic\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "y = load_macroeconomic()  \n",
    "forecaster = VARMAX(suppress_warnings=True)  \n",
    "forecaster.fit(y[['realgdp', 'unemp']])  \n",
    "\n",
    "#y_pred = forecaster.predict(fh=[1,4,12])  \n",
    "y_pred = forecaster.predict(fh=[1,2,3]) \n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e795a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff sktime models and options testing - TrainTest, CrossValidation, GridSearch, RandomSearch\n",
    "#https://towardsdatascience.com/why-start-using-sktime-for-forecasting-8d6881c0a518\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.utils.plotting import plot_series\n",
    "from sktime.forecasting.arima import AutoARIMA\n",
    "\n",
    "df = sm.datasets.macrodata.load_pandas()['data']\n",
    "df['q_date'] = df.apply(lambda x: str(int(x['year'])) + '-Q' \n",
    "                                  + str(int(x['quarter'])), axis=1)\n",
    "df['date'] = pd.PeriodIndex(df['q_date'], freq='Q').to_timestamp()\n",
    "df = df.set_index('date').to_period(\"Q\")\n",
    "\n",
    "#creating lagged values for forecasting with exogenous variables\n",
    "df.loc[:, 'realinv_lagged'] = df.loc[:, 'realinv'].shift()\n",
    "df[['realinv_lagged']] = df[['realinv_lagged']].fillna(method='backfill')\n",
    "\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "\n",
    "y = df['realgdp']\n",
    "y_train, y_test = temporal_train_test_split(y, test_size=30)\n",
    "\n",
    "#\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.forecasting.theta import ThetaForecaster\n",
    "from sktime.performance_metrics.forecasting import make_forecasting_scorer\n",
    "\n",
    "#custom mape calculated on predictions with a ceiling\n",
    "def custom_mape(y: np.array, y_hat: np.array, multioutput: str):\n",
    "    metrics_dict = {'uniform_average': np.mean(np.abs((y - np.ceil(y_hat)) / y)),\n",
    "                    'raw_values': np.abs((y - np.ceil(y_hat)) / y)}\n",
    "    try:\n",
    "        return metrics_dict[multioutput]\n",
    "    except KeyError:\n",
    "        print(\"multioutput not specified correctly - \"\n",
    "              \"pick `raw_values` or `uniform_average`\")\n",
    "\n",
    "#specific data points\n",
    "fh_abs = ForecastingHorizon(y_test.index, is_relative=False)  \n",
    "\n",
    "#last point in the training series\n",
    "cutoff = pd.Period(\"2002-01-01\", freq=\"Q\")\n",
    "fh_rel = fh_abs.to_relative(cutoff)\n",
    "fh_rel\n",
    "\n",
    "#29 quarters after the last point in the training series\n",
    "cutoff_insample = pd.Period(\"2009-09-01\", freq=\"Q\")\n",
    "fh_rel_insample = fh_abs.to_relative(cutoff_insample)\n",
    "fh_rel_insample\n",
    "\n",
    "# forecaster = NaiveForecaster().fit(y_train)\n",
    "# y_pred_rel = forecaster.predict(fh=fh_rel)\n",
    "# y_pred_rel_insample = forecaster.predict(fh=fh_rel_insample)\n",
    "\n",
    "# plot_series(y_train, y_test, y_pred_rel, y_pred_rel_insample, \n",
    "#             labels=[\"y_train\", \"y_test\", \"y_pred\", \"y_pred in-sample\"]);\n",
    "\n",
    "#1 uniivariate NaiveForecaster - forecasting one step ahead\n",
    "# forecaster = NaiveForecaster()\n",
    "# forecaster.fit(y_train)\n",
    "# y_pred = forecaster.predict(fh=1)\n",
    "\n",
    "# y_pred\n",
    "\n",
    "#2 NaiveForecaster - forecasting using ForecastingHorizon - Absolute values\n",
    "# y = df['realgdp']\n",
    "# y_train, y_test = temporal_train_test_split(y, test_size=30)\n",
    "\n",
    "# forecaster = NaiveForecaster()\n",
    "# forecaster.fit(y_train)\n",
    "# y_pred = forecaster.predict(fh=fh_abs)\n",
    "# y_pred\n",
    "\n",
    "#3 uniivariate AutoARIMA - forecasting using ForecastingHorizon - Absolute values\n",
    "# y = df['realgdp']\n",
    "# y_train, y_test = temporal_train_test_split(y, test_size=30)\n",
    "\n",
    "# fh_abs = ForecastingHorizon(y_test.index, is_relative=False) \n",
    "# forecaster = AutoARIMA(sp=4)\n",
    "# forecaster.fit(y_train)\n",
    "# y_pred = forecaster.predict(fh=fh_abs)\n",
    "# y_pred\n",
    "\n",
    "#4 make_forecasting_scorer\n",
    "from sktime.forecasting.theta import ThetaForecaster\n",
    "from sktime.performance_metrics.forecasting import make_forecasting_scorer\n",
    "\n",
    "forecaster = ThetaForecaster()\n",
    "forecaster.fit(y_train)\n",
    "y_pred = forecaster.predict(np.arange(1, y_test.size+1))\n",
    "        \n",
    "# mape_changed = make_forecasting_scorer(func=custom_mape, multioutput = 'uniform_average')\n",
    "# mape_changed_per_row = make_forecasting_scorer(func=custom_mape, multioutput = 'raw_values')\n",
    "# print(f\"custom MAPE: {mape_changed(y_test, y_pred)}\")\n",
    "# print(f\"custom MAPE per row:\\n{mape_changed_per_row(y_test, y_pred).head()}\")\n",
    "\n",
    "\n",
    "#5 multivariate exogenous time series\n",
    "#It includes both a training series and a data frame with exogenous variables.\n",
    "# y = df['realgdp']\n",
    "# feat_list = ['realgdp', 'realcons', 'realinv', 'realgovt'] \n",
    "# X = df[feat_list] \n",
    "\n",
    "# y_train, y_test, X_train, X_test = temporal_train_test_split(y, X, test_size=30)\n",
    "# fh_abs = ForecastingHorizon(y_test.index, is_relative=False) \n",
    "\n",
    "# forecaster = AutoARIMA(sp=4)\n",
    "# forecaster.fit(y_train, X_train)\n",
    "# y_pred = forecaster.predict(X=X_test, fh=fh_abs)\n",
    "# y_pred\n",
    "# plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);\n",
    "\n",
    "#6 cross-validation\n",
    "# from sktime.forecasting.model_selection import ExpandingWindowSplitter\n",
    "# from sktime.forecasting.model_evaluation import evaluate\n",
    "\n",
    "# y = df['realgdp']\n",
    "# forecaster = AutoARIMA(sp=4)\n",
    "# cv = ExpandingWindowSplitter(step_length=30, fh=list(range(1,31)), initial_window=40)\n",
    "# cv_df = evaluate(forecaster=forecaster, y=y, cv=cv, strategy=\"update\", return_data=True)\n",
    "\n",
    "# plot_series(\n",
    "#     y, *[cv_df[\"y_pred\"].iloc[x] for x in range(5)],\n",
    "#     markers=[\"o\", *[\".\"]*5],\n",
    "#     labels=[\"y_true\"] + [f\"cv: {x}\" for x in range(5)]\n",
    "# );\n",
    "\n",
    "#7 GridSearch\n",
    "from sktime.forecasting.model_selection import ForecastingRandomizedSearchCV, ForecastingGridSearchCV\n",
    "from sktime.forecasting.model_selection import SlidingWindowSplitter\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.forecasting.compose import make_reduction\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "y = df['realgdp']\n",
    "y_train, y_test = temporal_train_test_split(y, test_size=30)\n",
    "\n",
    "regressor = RandomForestRegressor(n_estimators=20)\n",
    "forecaster = make_reduction(regressor)\n",
    "nested_params = {\"window_length\": list(range(2,21)), \n",
    "                 \"estimator__max_depth\": list(range(5,16))}\n",
    "\n",
    "cv = SlidingWindowSplitter(initial_window=60, window_length=30)\n",
    "# nrcv = ForecastingRandomizedSearchCV(forecaster, strategy=\"refit\", cv=cv, \n",
    "#                                      param_distributions=nested_params, \n",
    "#                                      n_iter=5, random_state=42)\n",
    "\n",
    "from sktime.forecasting.model_selection import ForecastingGridSearchCV\n",
    "nrcv = ForecastingGridSearchCV(forecaster, strategy=\"refit\", cv=cv, \n",
    "                             param_grid=nested_params)\n",
    "# nrcv.fit(y_train)\n",
    "# y_pred = nrcv.predict(np.arange(1, y_test.size+1))\n",
    "\n",
    "# print(nrcv.best_params_)\n",
    "# print(nrcv.best_score_)\n",
    "\n",
    "#8 GridSearch with AutoML, Multimodels selection\n",
    "from sktime.forecasting.compose import MultiplexForecaster\n",
    "from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "from sktime.forecasting.naive import NaiveForecaster, NaiveVariance\n",
    "\n",
    "cv = SlidingWindowSplitter(initial_window=60, window_length=30)\n",
    "\n",
    "forecaster = MultiplexForecaster(\n",
    "    forecasters=[\n",
    "        (\"naive\", NaiveForecaster(strategy=\"last\")),\n",
    "        (\"ets\", ExponentialSmoothing(trend=\"add\", sp=12)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "forecaster_param_grid = {\"selected_forecaster\": [\"ets\", \"naive\"]}\n",
    "gscv = ForecastingGridSearchCV(forecaster, cv=cv, param_grid=forecaster_param_grid)\n",
    "\n",
    "gscv.fit(y)\n",
    "gscv.best_params_\n",
    "\n",
    "\n",
    "#9 transformers\n",
    "'''\n",
    "Detrender  removing trend from time series,\n",
    "Deseasonalizer  removing seasonal patterns from time series,\n",
    "BoxCoxTransformer  transforming time series to resemble a normal distribution,\n",
    "HampelFilter  detecting outliers in time series,\n",
    "TabularToSeriesAdaptor  adapting tabular transformations to series (e.g. adapting preprocessing functionalities from scikit-learn).\n",
    "'''\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.transformations.series.boxcox import BoxCoxTransformer\n",
    "from sktime.forecasting.theta import ThetaForecaster\n",
    "from sktime.utils.plotting import plot_series\n",
    "import seaborn as sns\n",
    "\n",
    "# sns.set(rc={'figure.figsize':(12,6)})\n",
    "\n",
    "# y = df['realgdp']\n",
    "# y_train, y_test = temporal_train_test_split(y, test_size=30)\n",
    "\n",
    "# forecaster = ThetaForecaster(sp=4)\n",
    "# transformer = BoxCoxTransformer(sp=4) #Box-Cox power transformation\n",
    "# y_train_transformed = transformer.fit_transform(y_train)\n",
    "\n",
    "# forecaster.fit(y_train_transformed)\n",
    "# y_pred = forecaster.predict(np.arange(1, y_test.size+1))\n",
    "# y_pred_inversed = transformer.inverse_transform(y_pred)\n",
    "\n",
    "# plot_series(y_train, y_test, y_pred_inversed, \n",
    "#             labels=[\"y_train\", \"y_test\", \"y_pred_inversed_transformation\"]);\n",
    "\n",
    "#10 pipelines\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.forecasting.compose import ForecastingPipeline\n",
    "from sktime.transformations.series.outlier_detection import HampelFilter\n",
    "from sktime.transformations.series.boxcox import LogTransformer\n",
    "from sktime.forecasting.theta import ThetaForecaster\n",
    "\n",
    "# y = df['realgdp']\n",
    "# X = df[['realinv_lagged']]\n",
    "# y_train, y_test, X_train, X_test = temporal_train_test_split(y, X, test_size=30) \n",
    "\n",
    "# forecaster = ForecastingPipeline(\n",
    "#     [(\"hampel\", HampelFilter()),\n",
    "#     (\"log\", LogTransformer()),\n",
    "#     (\"forecaster\", ThetaForecaster(sp=4))])\n",
    "\n",
    "# forecaster.fit(y_train, X_train)\n",
    "# y_pred = forecaster.predict(fh=np.arange(1, y_test.size+1), X=X_test)\n",
    "\n",
    "#11 TimeSeriesForestClassifier- Each row as series data /multiclassification\n",
    "from sktime.datasets import load_arrow_head\n",
    "#from sktime.classification.compose import TimeSeriesForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# X, y = load_arrow_head(return_X_y=True)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "# classifier = TimeSeriesForestClassifier()\n",
    "# classifier.fit(X_train, y_train)\n",
    "# y_pred = classifier.predict(X_test)\n",
    "# accuracy_score(y_test, y_pred)\n",
    "\n",
    "#12A TransformedTargetForecaster\n",
    "# from sktime.datasets import load_macroeconomic\n",
    "# from sktime.forecasting.arima import ARIMA\n",
    "# from sktime.forecasting.compose import ForecastingPipeline\n",
    "# from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "# from sktime.transformations.series.impute import Imputer\n",
    "\n",
    "# from sktime.forecasting.compose import TransformedTargetForecaster\n",
    "# from sktime.transformations.series.detrend import Deseasonalizer, Detrender\n",
    "\n",
    "# data = load_macroeconomic()\n",
    "# y = data[\"unemp\"]\n",
    "# X = data.drop(columns=[\"unemp\"])\n",
    "\n",
    "# y_train, y_test, X_train, X_test = temporal_train_test_split(y, X)\n",
    "\n",
    "# forecaster = TransformedTargetForecaster(\n",
    "#     [\n",
    "#         (\"deseasonalize\", Deseasonalizer(sp=12)),\n",
    "#         (\"detrend\", Detrender()),\n",
    "#         (\"forecast\", ARIMA()),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "#12B\n",
    "# regressor = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "# forecaster = TransformedTargetForecaster(\n",
    "#     [\n",
    "#         (\"deseasonalize\", Deseasonalizer(model=\"multiplicative\", sp=52)),\n",
    "#         (\"detrend\", Detrender(forecaster=PolynomialTrendForecaster(degree=1))),\n",
    "#         (\"forecast\", make_reduction(regressor, window_length=52, strategy=\"recursive\"),\n",
    "#         ),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# param_grid = {\n",
    "#     'deseasonalize__model': ['multiplicative', 'additive'],\n",
    "#     'detrend__forecaster__degree': [1, 2, 3],\n",
    "#     'forecast__estimator__max_depth': [3, 5, 6, 10, 15, 20],\n",
    "#     'forecast__estimator__learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "#     'forecast__estimator__subsample': np.arange(0.5, 1.0, 0.1),\n",
    "#     'forecast__estimator__colsample_bytree': np.arange(0.4, 1.0, 0.1),\n",
    "#     'forecast__estimator__colsample_bylevel': np.arange(0.4, 1.0, 0.1),\n",
    "#     'forecast__estimator__n_estimators': [100, 500, 1000]\n",
    "# }\n",
    "# gscv = ForecastingRandomizedSearchCV(forecaster, cv=cv, param_distributions=param_grid, n_iter=100, random_state=42)\n",
    "\n",
    "# gscv.fit(y=y_train, X=X_train)\n",
    "# y_pred = gscv.predict(fh=fh, X=X_test)\n",
    "\n",
    "# #quick creation also possible via the * dunder method, same pipeline:\n",
    "# #forecaster = Deseasonalizer(sp=12) * Detrender() * ARIMA()\n",
    "\n",
    "# forecast = forecaster.fit(y, fh=[1, 2, 3])\n",
    "# forecast_interval = forecaster.predict_interval()\n",
    "# forecast_quantiles = forecaster.predict_quantiles()\n",
    "\n",
    "#13 estimator checking on the fly using check_estimator\n",
    "\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.utils.estimator_checks import check_estimator\n",
    "'''\n",
    "this prints any failed tests, and returns dictionary with keys of test runs and results from the test run\n",
    "run individual tests using the tests_to_run arg or the fixtures_to_run_arg these need to be identical to test or test/fixture names, see docstring\n",
    "'''\n",
    "\n",
    "#check_estimator(NaiveForecaster)\n",
    "\n",
    "#14 WindowSummarizer\n",
    "#https://github.com/sktime/sktime/blob/main/examples/01c_forecasting_hierarchical_global.ipynb\n",
    "\n",
    "from sktime.datasets import load_longley\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.forecasting.compose import ForecastingPipeline\n",
    "from sktime.transformations.series.summarize import WindowSummarizer\n",
    "\n",
    "y_ll, X_ll = load_longley()\n",
    "y_train_ll, y_test_ll, X_train_ll, X_test_ll = temporal_train_test_split(y_ll, X_ll)\n",
    "fh = ForecastingHorizon(X_test_ll.index, is_relative=False)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def count_gt130(x):\n",
    "    \"\"\"Count how many observations lie above threshold 130.\"\"\"\n",
    "    return np.sum((x > 700)[::-1])\n",
    "\n",
    "kwargs = {\n",
    "    \"lag_feature\": {\n",
    "        \"lag\": [1],\n",
    "        count_gt130: [[2, 3]],\n",
    "        \"std\": [[1, 4]],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Example transforming only X\n",
    "pipe = ForecastingPipeline(\n",
    "    steps=[\n",
    "        (\"a\", WindowSummarizer(n_jobs=1, target_cols=[\"POP\", \"GNPDEFL\"])),\n",
    "        (\"b\", WindowSummarizer(n_jobs=1, target_cols=[\"GNP\"], **kwargs)),\n",
    "        (\"forecaster\", NaiveForecaster(strategy=\"drift\")),\n",
    "    ]\n",
    ")\n",
    "pipe_return = pipe.fit(y_train_ll, X_train_ll)\n",
    "y_pred1 = pipe_return.predict(fh=fh, X=X_test_ll)\n",
    "\n",
    "y_pred1\n",
    "\n",
    "#15 DateTimeFeatures\n",
    "from sktime.transformations.series.date import DateTimeFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba9f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b851ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diff models\n",
    "from sktime.datasets import load_airline\n",
    "#!pip install sktime[all_extras] # for Prophet\n",
    "\n",
    "from sktime.forecasting.fbprophet import Prophet\n",
    "# Prophet requires to have data with a pandas.DatetimeIndex\n",
    "y = load_airline().to_timestamp(freq='M')\n",
    "forecaster = Prophet(  \n",
    "    seasonality_mode='multiplicative',\n",
    "    n_changepoints=int(len(y) / 12),\n",
    "    add_country_holidays={'country_name': 'Germany'},\n",
    "    yearly_seasonality=True)\n",
    "forecaster.fit(y)  \n",
    "\n",
    "y_pred = forecaster.predict(fh=[1,2,3])\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02802e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.utils.estimator_checks import check_estimator\n",
    "\n",
    "check_estimator(NaiveForecaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d48576",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index.levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75814607",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataframe\n",
    "windowed_chunk_time_series_dataframe_label.values.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f794b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry create_ts_pipeline\n",
      "pipeline: ForecastingPipeline(steps=[('imputer', Imputer(method='mean')),\n",
      "                           ('minmaxscaler',\n",
      "                            TabularToSeriesAdaptor(transformer=MinMaxScaler())),\n",
      "                           ('hampel', HampelFilter()),\n",
      "                           ('log', LogTransformer()),\n",
      "                           ('model', ThetaForecaster(sp=4))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajverma\\AppData\\Local\\anaconda3\\envs\\ts310\\lib\\site-packages\\sktime\\transformations\\series\\boxcox.py:291: RuntimeWarning: divide by zero encountered in log\n",
      "  Xt = np.log(scale * (X + offset))\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m y_train, y_valid, X_train, X_valid \u001b[38;5;241m=\u001b[39m temporal_train_test_split(y, X, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m     73\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(y_train, X_train)\n\u001b[1;32m---> 75\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_valid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m y_pred\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ts310\\lib\\site-packages\\sktime\\forecasting\\base\\_base.py:405\u001b[0m, in \u001b[0;36mBaseForecaster.predict\u001b[1;34m(self, fh, X)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;66;03m# we call the ordinary _predict if no looping/vectorization needed\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_vectorized:\n\u001b[1;32m--> 405\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_inner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;66;03m# otherwise we call the vectorized version of predict\u001b[39;00m\n\u001b[0;32m    408\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vectorize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m=\u001b[39mX_inner, fh\u001b[38;5;241m=\u001b[39mfh)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ts310\\lib\\site-packages\\sktime\\forecasting\\compose\\_pipeline.py:516\u001b[0m, in \u001b[0;36mForecastingPipeline._predict\u001b[1;34m(self, fh, X)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, fh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, X\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;124;03m\"\"\"Forecast time series at future horizon.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \n\u001b[0;32m    504\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;124;03m        Point predictions\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforecaster_\u001b[38;5;241m.\u001b[39mpredict(fh, X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ts310\\lib\\site-packages\\sktime\\forecasting\\compose\\_pipeline.py:690\u001b[0m, in \u001b[0;36mForecastingPipeline._transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_tag(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignores-exogeneous-X\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    689\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, transformer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_transformers():\n\u001b[1;32m--> 690\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ts310\\lib\\site-packages\\sktime\\transformations\\base.py:536\u001b[0m, in \u001b[0;36mBaseTransformer.transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    533\u001b[0m X_inner, y_inner, metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, return_metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X_inner, VectorizedDF):\n\u001b[1;32m--> 536\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_inner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;66;03m# otherwise we call the vectorized version of predict\u001b[39;00m\n\u001b[0;32m    539\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vectorize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m=\u001b[39mX_inner, y\u001b[38;5;241m=\u001b[39my_inner)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ts310\\lib\\site-packages\\sktime\\transformations\\series\\outlier_detection.py:100\u001b[0m, in \u001b[0;36mHampelFilter._transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Z, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m Z:\n\u001b[1;32m--> 100\u001b[0m         Z[col] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# univariate\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m     Z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_series(Z)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ts310\\lib\\site-packages\\sktime\\transformations\\series\\outlier_detection.py:133\u001b[0m, in \u001b[0;36mHampelFilter._transform_series\u001b[1;34m(self, Z)\u001b[0m\n\u001b[0;32m    128\u001b[0m cv \u001b[38;5;241m=\u001b[39m SlidingWindowSplitter(\n\u001b[0;32m    129\u001b[0m     window_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_length, step_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, start_with_window\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    130\u001b[0m )\n\u001b[0;32m    131\u001b[0m half_window_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_length \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 133\u001b[0m Z \u001b[38;5;241m=\u001b[39m \u001b[43m_hampel_filter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mZ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_sigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_sigma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhalf_window_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhalf_window_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# data post-processing\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_bool:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ts310\\lib\\site-packages\\sktime\\transformations\\series\\outlier_detection.py:172\u001b[0m, in \u001b[0;36m_hampel_filter\u001b[1;34m(Z, cv, n_sigma, half_window_length, k)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(Z):\n\u001b[0;32m    171\u001b[0m     cv_window \u001b[38;5;241m=\u001b[39m i[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 172\u001b[0m     cv_median \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmedian(\u001b[43mZ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcv_window\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    173\u001b[0m     cv_sigma \u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmedian(np\u001b[38;5;241m.\u001b[39mabs(Z[cv_window] \u001b[38;5;241m-\u001b[39m cv_median))\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;66;03m# find outliers at start and end of z\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ts310\\lib\\site-packages\\pandas\\core\\series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1004\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m   1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_values(key)\n\u001b[1;32m-> 1007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ts310\\lib\\site-packages\\pandas\\core\\series.py:1042\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minteger\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;66;03m# We need to decide whether to treat this as a positional indexer\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;66;03m#  (i.e. self.iloc) or label-based (i.e. self.loc)\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_should_fallback_to_positional:\n\u001b[1;32m-> 1042\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1044\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[key]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ts310\\lib\\site-packages\\pandas\\core\\indexing.py:1073\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1070\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1072\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m-> 1073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ts310\\lib\\site-packages\\pandas\\core\\indexing.py:1301\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1299\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ts310\\lib\\site-packages\\pandas\\core\\indexing.py:1239\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[1;32m-> 1239\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   1241\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ts310\\lib\\site-packages\\pandas\\core\\indexing.py:1432\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1429\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1430\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1432\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ts310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6070\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6072\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6074\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ts310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6130\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   6129\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 6130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6132\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "pp_steps_dict = {'Outlier': 'HampelFilter', 'Transformer': 'LogTransformer'}\n",
    "pp_steps_list = []\n",
    "\n",
    "from sktime.forecasting.compose import ForecastingPipeline\n",
    "from sktime.transformations.series.outlier_detection import HampelFilter\n",
    "from sktime.transformations.series.boxcox import LogTransformer, BoxCoxTransformer\n",
    "from sktime.forecasting.theta import ThetaForecaster\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "\n",
    "from sktime.transformations.series.adapt import TabularToSeriesAdaptor\n",
    "from sktime.transformations.series.impute import Imputer\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print('Entry create_ts_pipeline')\n",
    "if pp_steps_dict:\n",
    "    #2.1 ImputerCat/ImputerNum\n",
    "    if 'Outlier' in pp_steps_dict:\n",
    "        if pp_steps_dict['Outlier'] == 'HampelFilter':\n",
    "            outlier = HampelFilter()\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        outlier = 'passthrough'\n",
    "\n",
    "\n",
    "    if 'Transformer' in pp_steps_dict:\n",
    "        if pp_steps_dict['Transformer'] == 'LogTransformer':\n",
    "            transformer = LogTransformer()\n",
    "        elif pp_steps_dict['Transformer'] == 'BoxCoxTransformer':\n",
    "            transformer = BoxCoxTransformer(sp=4)\n",
    "    else:\n",
    "        transformer = 'passthrough'\n",
    "\n",
    "\n",
    "pipeline = ForecastingPipeline(\n",
    "[\n",
    "(\"imputer\", Imputer(method=\"mean\")),\n",
    "(\"minmaxscaler\", TabularToSeriesAdaptor(MinMaxScaler())),\n",
    "(\"hampel\", outlier),\n",
    "(\"log\", transformer),\n",
    "(\"model\", ThetaForecaster(sp=4))\n",
    "])\n",
    "\n",
    "print('pipeline:', pipeline)\n",
    "forecaster = NaiveForecaster()\n",
    "\n",
    "if pipeline.steps[-1][0] == 'model':\n",
    "    pipeline.steps.pop(-1)\n",
    "    pipeline.steps.append(('model', forecaster))\n",
    "else:\n",
    "    pipeline.steps.append(('model', forecaster))\n",
    "    \n",
    "#4 multivariate/df/done\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data = sm.datasets.macrodata.load_pandas()['data']\n",
    "data['q_date'] = data.apply(lambda x: str(int(x['year'])) + '-Q' \n",
    "                                  + str(int(x['quarter'])), axis=1)\n",
    "target_col = 'realgdp'\n",
    "index_col = 'q_date'\n",
    "feat_list = ['realgdp', 'realcons', 'realinv', 'realgovt'] \n",
    "\n",
    "y = data[target_col]\n",
    "X = data[feat_list]\n",
    "\n",
    "\n",
    "# y_train, y_valid = temporal_train_test_split(y, test_size=30)\n",
    "# pipeline.fit(y_train)\n",
    "\n",
    "y_train, y_valid, X_train, X_valid = temporal_train_test_split(y, X, test_size=30)\n",
    "pipeline.fit(y_train, X_train)\n",
    "\n",
    "y_pred = pipeline.predict(fh=np.arange(1, y_valid.size+1), X=X_valid)\n",
    "\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4efc2514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ForecastingPipeline(steps=[(&#x27;imputer&#x27;, Imputer(method=&#x27;mean&#x27;)),\n",
       "                           (&#x27;minmaxscaler&#x27;,\n",
       "                            TabularToSeriesAdaptor(transformer=MinMaxScaler())),\n",
       "                           (&#x27;hampel&#x27;, HampelFilter()),\n",
       "                           (&#x27;log&#x27;, LogTransformer()),\n",
       "                           (&#x27;model&#x27;, NaiveForecaster())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ForecastingPipeline</label><div class=\"sk-toggleable__content\"><pre>ForecastingPipeline(steps=[(&#x27;imputer&#x27;, Imputer(method=&#x27;mean&#x27;)),\n",
       "                           (&#x27;minmaxscaler&#x27;,\n",
       "                            TabularToSeriesAdaptor(transformer=MinMaxScaler())),\n",
       "                           (&#x27;hampel&#x27;, HampelFilter()),\n",
       "                           (&#x27;log&#x27;, LogTransformer()),\n",
       "                           (&#x27;model&#x27;, NaiveForecaster())])</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "ForecastingPipeline(steps=[('imputer', Imputer(method='mean')),\n",
       "                           ('minmaxscaler',\n",
       "                            TabularToSeriesAdaptor(transformer=MinMaxScaler())),\n",
       "                           ('hampel', HampelFilter()),\n",
       "                           ('log', LogTransformer()),\n",
       "                           ('model', NaiveForecaster())])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cbdaaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002Q2    11477.868\n",
      "2002Q3    11477.868\n",
      "Freq: Q-DEC, Name: realgdp, dtype: float64\n",
      "Absolute FH: ForecastingHorizon(['2002Q2', '2002Q3', '2002Q4', '2003Q1', '2003Q2', '2003Q3',\n",
      "             '2003Q4', '2004Q1', '2004Q2', '2004Q3', '2004Q4', '2005Q1',\n",
      "             '2005Q2', '2005Q3', '2005Q4', '2006Q1', '2006Q2', '2006Q3',\n",
      "             '2006Q4', '2007Q1', '2007Q2', '2007Q3', '2007Q4', '2008Q1',\n",
      "             '2008Q2', '2008Q3', '2008Q4', '2009Q1', '2009Q2', '2009Q3'],\n",
      "            dtype='period[Q-DEC]', name='date', is_relative=False)\n",
      "\n",
      "Relative FH ahead: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "\n",
      "Relative FH in-sample: [-29, -28, -27, -26, -25, -24, -23, -22, -21, -20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.utils.plotting import plot_series\n",
    "import seaborn as sns\n",
    "\n",
    "#1 data\n",
    "df = sm.datasets.macrodata.load_pandas()['data']\n",
    "\n",
    "df['q_date'] = df.apply(lambda x: str(int(x['year'])) + '-Q' \n",
    "                                  + str(int(x['quarter'])), axis=1)\n",
    "df['date'] = pd.PeriodIndex(df['q_date'], freq='Q').to_timestamp()\n",
    "df = df.set_index('date').to_period(\"Q\")\n",
    "\n",
    "#creating lagged values for forecasting with exogenous variables\n",
    "df.loc[:, 'realinv_lagged'] = df.loc[:, 'realinv'].shift()\n",
    "df[['realinv_lagged']] = df[['realinv_lagged']].fillna(method='backfill')\n",
    "\n",
    "#2 univariate without X\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "\n",
    "\n",
    "y = df['realgdp']\n",
    "y_train, y_test = temporal_train_test_split(y, test_size=30)\n",
    "\n",
    "forecaster = NaiveForecaster()\n",
    "forecaster.fit(y_train)\n",
    "#y_pred = forecaster.predict(fh=1) #forecasting one step ahead\n",
    "y_pred = forecaster.predict(fh=[1, 2]) #forecasting two step ahead\n",
    "print(y_pred)\n",
    "\n",
    "#3 ForecastingHorizon chk\n",
    "#specific data points\n",
    "fh_abs = ForecastingHorizon(y_test.index, is_relative=False)  \n",
    "print(f\"Absolute FH: {fh_abs}\")\n",
    "\n",
    "#last point in the training series\n",
    "cutoff = pd.Period(\"2002-01-01\", freq=\"Q\")\n",
    "fh_rel = fh_abs.to_relative(cutoff)\n",
    "print(f\"\\nRelative FH ahead: {list(fh_rel)}\")\n",
    "\n",
    "#29 quarters after the last point in the training series\n",
    "cutoff_insample = pd.Period(\"2009-09-01\", freq=\"Q\")\n",
    "fh_rel_insample = fh_abs.to_relative(cutoff_insample)\n",
    "print(f\"\\nRelative FH in-sample: {list(fh_rel_insample)}\")\n",
    "\n",
    "forecaster = NaiveForecaster().fit(y_train)\n",
    "y_pred_rel = forecaster.predict(fh=fh_rel)\n",
    "y_pred_rel_insample = forecaster.predict(fh=fh_rel_insample)\n",
    "\n",
    "plot_series(y_train, y_test, y_pred_rel, y_pred_rel_insample, \n",
    "            labels=[\"y_train\", \"y_test\", \"y_pred\", \"y_pred in-sample\"]);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a22782b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeriodIndex(['2002Q2', '2002Q3', '2002Q4', '2003Q1', '2003Q2', '2003Q3',\n",
       "             '2003Q4', '2004Q1', '2004Q2', '2004Q3', '2004Q4', '2005Q1',\n",
       "             '2005Q2', '2005Q3', '2005Q4', '2006Q1', '2006Q2', '2006Q3',\n",
       "             '2006Q4', '2007Q1', '2007Q2', '2007Q3', '2007Q4', '2008Q1',\n",
       "             '2008Q2', '2008Q3', '2008Q4', '2009Q1', '2009Q2', '2009Q3'],\n",
       "            dtype='period[Q-DEC]', name='date')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n",
    "y_train\n",
    "y_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0504e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#v1.0 testing ts_forecast_models\n",
    "\n",
    "def date_parser(date_str):\n",
    "    from dateutil import parser\n",
    "    \n",
    "    dt = parser.parse(date_str)\n",
    "    \n",
    "    date = dt.date()\n",
    "    year = dt.year\n",
    "    month = dt.month\n",
    "    hour = dt.hour\n",
    "    minute = dt.minute\n",
    "    weekday = dt.weekday()\n",
    "    time = dt.time()\n",
    "    return date\n",
    "\n",
    "\n",
    "def ts_forecast_period_index(ts, periods=36, freq=\"M\"):\n",
    "    \n",
    "    last_index = ts.index.to_series().astype(str).values.tolist()[-1]\n",
    "    \n",
    "    from sktime.forecasting.base import ForecastingHorizon\n",
    "    fh = ForecastingHorizon(\n",
    "        pd.PeriodIndex(pd.date_range(last_index, periods=periods, freq=freq)), is_relative=False\n",
    "    )\n",
    "    fh = fh[1:] # drop last index\n",
    "    return fh\n",
    "\n",
    "#custom mape calculated on predictions with a ceiling\n",
    "# def custom_mape(y: np.array, y_pred: np.array, multioutput: str):\n",
    "#     metrics_dict = {'uniform_average': np.mean(np.abs((y - np.ceil(y_pred)) / y)),\n",
    "#                     'raw_values': np.abs((y - np.ceil(y_pred)) / y)}\n",
    "#     try:\n",
    "#         return metrics_dict[multioutput]\n",
    "#     except KeyError:\n",
    "#         print(\"multioutput not specified correctly - \"\n",
    "#               \"pick `raw_values` or `uniform_average`\")\n",
    "\n",
    "def custom_mape(y: np.array, y_pred: np.array):\n",
    "    metrics_dict = {'score_mean': np.mean(np.abs((y - np.ceil(y_pred)) / y)),\n",
    "                    'score_row': np.abs((y - np.ceil(y_pred)) / y)}\n",
    "    return metrics_dict\n",
    "\n",
    "def ts_forecast_models(data, target_col, index_col, feat_list, model_type, nested_params=None, train_type='TrainTest',\n",
    "        forecast_period=5, freq='M', regression_window=10, cv_type='ExpandingWindowSplitter',\n",
    "        pt_type='ForecastingRandomizedSearch', transformer_type='BoxCoxTransformer', PlotSeries=True):\n",
    "    '''\n",
    "    v1.1 Jan20,2023: added regression_window, cv_type, pt_type, forecasting score/CrossValidation\n",
    "    v1.0 Jan18,2023: initial version\n",
    "    ts_type: UniVariate/MultiVariate\n",
    "    train_type: TrainTest/CrossValidation/GridSearch/RandomizedSearch\n",
    "    freq: used to set index whern data is as df with date col\n",
    "    regression_window: when regression models are used\n",
    "    cv_type: used in cross_validation/GridSerach - ExpandingWindowSplitter/SlidingWindowSplitter\n",
    "    pt_type: used in parameter tuning - ForecastingGridSearch/ForecastingRandomizedSearch\n",
    "    \n",
    "    Test issues\n",
    "    Auto_Arima with cross_validation TypeError\n",
    "    TypeError: input must be a one of (<class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.series.Series'>), but found type: <class 'float'>\n",
    "    '''\n",
    "    print('entry ts_forecast_models')\n",
    "    \n",
    "    #import libs\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sktime.forecasting.base import ForecastingHorizon\n",
    "    from sktime.utils.plotting import plot_series\n",
    "    from sktime.forecasting.compose import make_reduction\n",
    "    from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "    \n",
    "    print('#1 data chk/split and chk UniVariate/MultiVariate')\n",
    "    print(f'data-typ:{type(data)}, data-shape:{data.shape}')\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        #set index\n",
    "        #data[index_col] = pd.to_datetime(data[index_col]) #TBC\n",
    "        if freq:\n",
    "            #Note1: convert partial str date(yyyy-mm or yy-q) to complete date format(yyyy-mm-dd)\n",
    "            data[index_col] = pd.PeriodIndex(data[index_col], freq=freq).to_timestamp()\n",
    "            #Note2: convert complete date format(yyyy-mm-dd) to date index w.r.t freq (yyyyqq)\n",
    "            data = data.set_index(index_col, drop=True).to_period(freq) #set index of \n",
    "        else:\n",
    "            data.set_index(index_col, drop=True, inplace=True) # convert to series\n",
    "            \n",
    "            \n",
    "        if feat_list:\n",
    "            X = data[feat_list]\n",
    "            y = data[target_col]\n",
    "            y_train, y_valid, X_train, X_valid = temporal_train_test_split(y, X, test_size=30)\n",
    "            ts_type = 'MultiVariate'\n",
    "            print(f'df-y_train:{y_train.shape}, y_valid:{y_valid.shape}, X_train:{X_train.shape}, X_valid:{X_valid.shape}')\n",
    "        else:\n",
    "            X = None\n",
    "            y = data[target_col]\n",
    "            y_train, y_valid = temporal_train_test_split(y, test_size=30)\n",
    "            ts_type = 'UniVariate'\n",
    "            print(f'df-srs-y_train:{y_train.shape}, y_valid:{y_valid.shape}')\n",
    "            \n",
    "    elif isinstance(data, pd.Series):\n",
    "        X = None\n",
    "        y = data\n",
    "        y_train, y_valid = temporal_train_test_split(y, test_size=30)\n",
    "        ts_type = 'UniVariate'\n",
    "        print(f'srs-y_train:{y_train.shape}, y_valid:{y_valid.shape}')\n",
    "    print('ts_type:', ts_type)\n",
    "       \n",
    "    print('target transformer')\n",
    "    if transformer_type:\n",
    "        if transformer_type == 'BoxCoxTransformer':\n",
    "            from sktime.transformations.series.boxcox import BoxCoxTransformer\n",
    "            transformer = BoxCoxTransformer(sp=4) #Box-Cox power transformation\n",
    "        elif transformer_type == 'LogTransformer':\n",
    "            from sktime.transformations.series.boxcox import LogTransformer\n",
    "            transformer = LogTransformer()\n",
    "        y_train = transformer.fit_transform(y_train)\n",
    "        \n",
    "    print('#create score dict')\n",
    "    score_dict = {}\n",
    "    score_dict['ts_type'] = ts_type\n",
    "    score_dict['model_type'] = model_type\n",
    "    \n",
    "    print('#get ForecastingHorizon')\n",
    "    fh_abs = ForecastingHorizon(y_valid.index, is_relative=False)\n",
    "    print('fh_abs:',fh_abs) # y_valid index\n",
    "    fh_re =[1,2,3] #next index forecast / not correct\n",
    "    \n",
    "    print(f'#executing:{model_type}')\n",
    "    if model_type == 'NaiveForecaster':\n",
    "        #Forecast based on naive assumptions about past trends continuing, supports univariate/multivariate\n",
    "        from sktime.forecasting.naive import NaiveForecaster\n",
    "        forecaster = NaiveForecaster()\n",
    "    elif model_type == 'PolynomialTrend':\n",
    "        #linear regression model with a 1st degree polynomial transformation of the feature.\n",
    "        from sktime.forecasting.trend import PolynomialTrendForecaster\n",
    "        forecaster = PolynomialTrendForecaster(degree=1)\n",
    "    elif model_type == 'AutoArima':\n",
    "        from sktime.forecasting.arima import AutoARIMA\n",
    "        forecaster = AutoARIMA(sp=4)\n",
    "    elif model_type == 'ThetaForecaster':\n",
    "        #it's equivalent to simple exponential smoothing (SES) with drift/tested for seasonality\n",
    "        from sktime.forecasting.theta import ThetaForecaster\n",
    "        forecaster = ThetaForecaster()\n",
    "        \n",
    "    else:\n",
    "        print(f'#executing Regression:{model_type}')\n",
    "        if regression_window:\n",
    "            forecaster = make_reduction(model_type, strategy=\"recursive\", window_length= regression_window)\n",
    "    \n",
    "    print('#cv_type:', cv_type)\n",
    "    from sktime.forecasting.model_selection import SlidingWindowSplitter, ExpandingWindowSplitter\n",
    "    \n",
    "    X_train = None if ts_type == 'UniVariate' else X_train # multivarite X_train will be used\n",
    "    window_size = round(y_train.size/4) #TBC\n",
    "    if cv_type == 'ExpandingWindowSplitter':\n",
    "         #note: initial_window + fh shold be smaller than the length of y\n",
    "        cv = ExpandingWindowSplitter(step_length=window_size, fh=list(range(1, window_size)), initial_window=window_size)\n",
    "    elif cv_type == 'SlidingWindowSplitter':\n",
    "        #note: initial_window must greater than `window_length`\n",
    "        cv = SlidingWindowSplitter(initial_window=window_size*2, window_length=window_size)\n",
    "        \n",
    "        \n",
    "    print('#train_type:',train_type, ts_type)\n",
    "    print(f'y_train:{y_train.shape}, y_valid:{y_valid.shape}')\n",
    "    if train_type == 'TrainTest':\n",
    "        if ts_type == 'UniVariate':\n",
    "            forecaster.fit(y_train)\n",
    "            y_pred = forecaster.predict(fh=fh_abs)\n",
    "        elif ts_type == 'MultiVariate':\n",
    "            print(f'X_train:{X_train.shape}, X_valid:{X_valid.shape}')\n",
    "            forecaster.fit(y_train, X_train)\n",
    "            y_pred = forecaster.predict(X=X_valid, fh=fh_abs)\n",
    "            \n",
    "        #score_dict update\n",
    "        fs_score_dict = custom_mape(y_valid, y_pred)\n",
    "        score_dict.update(fs_score_dict)\n",
    "        score_dict['y_pred'] = y_pred\n",
    "        \n",
    "    elif train_type == 'CrossValidation':\n",
    "        from sktime.forecasting.model_evaluation import evaluate\n",
    "        pred_df = evaluate(forecaster=forecaster, X=X_train, y=y_train, cv=cv, strategy=\"refit\", return_data=True)\n",
    "        \n",
    "        #score_dict update\n",
    "        score_dict['pred_df'] = pred_df\n",
    "        #y_pred TBC for PlotSeries\n",
    "        \n",
    "    elif train_type in ['GridSearch', 'RandomizedSearch']:\n",
    "        from sktime.forecasting.model_selection import ForecastingGridSearchCV, ForecastingRandomizedSearchCV\n",
    "        if train_type == 'GridSearch':\n",
    "            forecaster = ForecastingGridSearchCV(forecaster, strategy=\"refit\", cv=cv, param_grid=nested_params)\n",
    "        elif train_type == 'RandomizedSearch':\n",
    "            forecaster = ForecastingRandomizedSearchCV(forecaster, strategy=\"refit\", cv=cv, \n",
    "                        param_distributions=nested_params, n_iter=1, random_state=42)\n",
    "            \n",
    "        if ts_type == 'UniVariate':\n",
    "            forecaster.fit(y_train)\n",
    "            y_pred = forecaster.predict(fh=fh_abs)\n",
    "            #y_pred = forecaster.predict(np.arange(1, y_valid.size+1))\n",
    "        elif ts_type == 'MultiVariate':\n",
    "            print(f'X_train:{X_train.shape}, X_valid:{X_valid.shape}')\n",
    "            forecaster.fit(y_train, X_train)\n",
    "            y_pred = forecaster.predict(X=X_valid, fh=fh_abs)\n",
    "        \n",
    "        #score_dict update\n",
    "        score_dict['y_pred'] = y_pred\n",
    "    \n",
    "    print('#performance_metrics - forecasting score')\n",
    "    from sktime.performance_metrics.forecasting import make_forecasting_scorer\n",
    "    \n",
    "#     mape_changed = make_forecasting_scorer(func=custom_mape, multioutput = 'uniform_average')\n",
    "#     mape_changed_per_row = make_forecasting_scorer(func=custom_mape, multioutput = 'raw_values')\n",
    "#     mape_score = mape_changed(y_valid, y_pred)\n",
    "#     mape_score_df = mape_changed_per_row(y_valid, y_pred)\n",
    "        \n",
    "    #PlotSeries = False # temp dueto train_type == 'CrossValidation'\n",
    "    if PlotSeries:\n",
    "        #if train_type == 'TrainTest':\n",
    "        if train_type in ['TrainTest', 'GridSearch', 'RandomizedSearch']:\n",
    "            print(f'df-y_train:{y_train.shape}, y_valid:{y_valid.shape}, y_pred:{y_pred.shape}')\n",
    "            plot_series(y_train, y_valid, y_pred, labels=[\"y_train\", \"y_valid\", \"y_pred\"]);\n",
    "        elif train_type == 'CrossValidation':\n",
    "            n_cv = cv_df.shape[0]\n",
    "            plot_series(\n",
    "                y, *[cv_df[\"y_pred\"].iloc[x] for x in range(n_cv)],\n",
    "                markers=[\"o\", *[\".\"]*n_cv],\n",
    "                labels=[\"y_true\"] + [f\"cv: {x}\" for x in range(n_cv)]\n",
    "            );\n",
    "        \n",
    "    print('exit ts_forecast_models')\n",
    "    return score_dict\n",
    "        \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#1 univariate/series /done\n",
    "# from sktime.datasets import load_airline\n",
    "# data = load_airline()\n",
    "# target_col = None\n",
    "# index_col = None\n",
    "# feat_list = None\n",
    "\n",
    "#2 univariate/df / data issue\n",
    "# data_path='D:\\\\timeseries\\\\'\n",
    "\n",
    "# data = pd.read_csv(data_path+'shampoo-sales.csv')\n",
    "# target_col = 'Sales'\n",
    "# index_col = 'Month'\n",
    "# feat_list = None\n",
    "# data[index_col] = data[index_col].apply(lambda x:date_parser(x)) # convert date format\n",
    "\n",
    "#3 univariate/df\n",
    "# import statsmodels.api as sm\n",
    "# data = sm.datasets.macrodata.load_pandas()['data']\n",
    "\n",
    "# data['q_date'] = data.apply(lambda x: str(int(x['year'])) + '-Q' \n",
    "#                                   + str(int(x['quarter'])), axis=1)\n",
    "# target_col = 'realgdp'\n",
    "# index_col = 'q_date'\n",
    "# feat_list = None\n",
    "# #train_type='TrainTest'\n",
    "# train_type='CrossValidation'\n",
    "\n",
    "#4 multivariate/df/done\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data = sm.datasets.macrodata.load_pandas()['data']\n",
    "data['q_date'] = data.apply(lambda x: str(int(x['year'])) + '-Q' \n",
    "                                  + str(int(x['quarter'])), axis=1)\n",
    "target_col = 'realgdp'\n",
    "index_col = 'q_date'\n",
    "feat_list = ['realgdp', 'realcons', 'realinv', 'realgovt'] \n",
    "freq='Q'\n",
    "train_type_list = ['TrainTest', 'CrossValidation', 'GridSearch', 'RandomizedSearch']\n",
    "train_type = train_type_list[2]\n",
    "#cv_type = 'ExpandingWindowSplitter'\n",
    "cv_type = 'SlidingWindowSplitter'\n",
    "nested_params = {\"window_length\": list(range(2,21)), \n",
    "                 #\"estimator__max_depth\": list(range(5,16))\n",
    "                }\n",
    "param_grid = nested_params\n",
    "\n",
    "#testing\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#regressor = GradientBoostingRegressor()\n",
    "regressor = XGBRegressor()\n",
    "\n",
    "model_list = ['PolynomialTrend','NaiveForecaster', 'AutoArima', 'ThetaForecaster', regressor]\n",
    "model_type = model_list[4]\n",
    "\n",
    "score_dict = ts_forecast_models(data, target_col, index_col, feat_list, model_type, nested_params=param_grid, train_type=train_type,\n",
    "        forecast_period=5, freq=freq, regression_window=5, cv_type = cv_type, PlotSeries=True)\n",
    "print('score_dict:', score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce5fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b1198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-variate time series classification using a simple CNN/LSTM\n",
    "\n",
    "#https://github.com/sktime/sktime/blob/main/examples/02a_classification_multivariate_cnn.ipynb\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sktime.classification.deep_learning.cnn import CNNClassifier\n",
    "from sktime.datasets import load_basic_motions\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "X_train, y_train = load_basic_motions(split=\"train\", return_X_y=True)\n",
    "X_test, y_test = load_basic_motions(split=\"test\", return_X_y=True)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(type(X_train.iloc[1, 1]))\n",
    "X_train.head()\n",
    "\n",
    "# The class labels\n",
    "np.unique(y_train)\n",
    "\n",
    "#Train a deep neural network classifier\n",
    "network = CNNClassifier(n_epochs=200, verbose=True)\n",
    "network.fit(X_train, y_train)\n",
    "network.score(X_test, y_test)\n",
    "\n",
    "#Grid Search\n",
    "param_grid = {\"kernel_size\": [7, 9], \"avg_pool_size\": [3, 5]}\n",
    "grid = GridSearchCV(network, param_grid=param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Test set score: {:.2f}\".format(grid.score(X_test, y_test)))\n",
    "print(\"Best parameters: {}\".format(grid.best_params_))\n",
    "\n",
    "#Train a LSTM-FCN\n",
    "#https://github.com/sktime/sktime/blob/main/examples/02b_classification_multivariate_lstmfcn.ipynb\n",
    "\n",
    "from sktime.classification.deep_learning.lstmfcn import LSTMFCNClassifier\n",
    "network = LSTMFCNClassifier(n_epochs=200, verbose=0)\n",
    "network.fit(X_train, y_train)\n",
    "network.score(X_test, y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9738e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sktime - feature_extraction\n",
    "#https://github.com/sktime/sktime/blob/main/examples/feature_extraction_with_tsfresh.ipynb\n",
    "\n",
    "#Feature extraction with tsfresh transformer\n",
    "#!pip install --upgrade tsfresh\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sktime.datasets import load_arrow_head, load_basic_motions\n",
    "from sktime.transformations.panel.tsfresh import TSFreshFeatureExtractor\n",
    "\n",
    "#1 Univariate time series classification data\n",
    "X, y = load_arrow_head(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# tf = TsFreshTransformer()\n",
    "t = TSFreshFeatureExtractor(default_fc_parameters=\"efficient\", show_warnings=False)\n",
    "Xt = t.fit_transform(X_train)\n",
    "\n",
    "#Using tsfresh with sktime\n",
    "classifier = make_pipeline(\n",
    "    TSFreshFeatureExtractor(default_fc_parameters=\"efficient\", show_warnings=False),\n",
    "    RandomForestClassifier(),\n",
    ")\n",
    "classifier.fit(X_train, y_train)\n",
    "classifier.score(X_test, y_test)\n",
    "\n",
    "#2 Multivariate time series classification data\n",
    "X, y = load_basic_motions(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "t = TSFreshFeatureExtractor(default_fc_parameters=\"efficient\", show_warnings=False)\n",
    "Xt = t.fit_transform(X_train)\n",
    "\n",
    "#Using tsfresh for forecasting\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sktime.datasets import load_airline\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.forecasting.compose import make_reduction\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "\n",
    "y = load_airline()\n",
    "y_train, y_test = temporal_train_test_split(y)\n",
    "\n",
    "regressor = make_pipeline(\n",
    "    TSFreshFeatureExtractor(show_warnings=False, disable_progressbar=True),\n",
    "    RandomForestRegressor(),\n",
    ")\n",
    "forecaster = make_reduction(\n",
    "    regressor, scitype=\"time-series-regressor\", window_length=12\n",
    ")\n",
    "forecaster.fit(y_train)\n",
    "\n",
    "fh = ForecastingHorizon(y_test.index, is_relative=False)\n",
    "y_pred = forecaster.predict(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0b11bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sktime-Probabilistic Forecasting\n",
    "\n",
    "#https://github.com/sktime/sktime/blob/main/examples/01b_forecasting_proba.ipynb\n",
    "\n",
    "from sktime.datasets import load_airline\n",
    "from sktime.forecasting.arima import ARIMA\n",
    "\n",
    "# step 1: data specification\n",
    "y = load_airline()\n",
    "# step 2: specifying forecasting horizon\n",
    "fh = [1, 2, 3]\n",
    "# step 3: specifying the forecasting algorithm\n",
    "forecaster = ARIMA()\n",
    "# step 4: fitting the forecaster\n",
    "forecaster.fit(y, fh=[1, 2, 3])\n",
    "# step 5: querying predictions\n",
    "y_pred = forecaster.predict()\n",
    "\n",
    "# for probabilistic forecasting:\n",
    "forecast_intervals = forecaster.predict_interval(coverage=0.9)\n",
    "forecast_quantiles = forecaster.predict_quantiles(fh=None, X=None, alpha=[0.05, 0.95])\n",
    "forecast_variance = forecaster.predict_var(fh=None, X=None, cov=False)\n",
    "#forecast_distribution = forecaster.predict_proba(fh=None, X=None, marginal=True)\n",
    "\n",
    "# does it support probabilistic predictions now?\n",
    "forecaster.get_tag(\"capability:pred_int\")\n",
    "\n",
    "# from sktime.registry import all_estimators\n",
    "# all_estimators(\n",
    "#     \"forecaster\", filter_tags={\"capability:pred_int\": True}, as_dataframe=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1011c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8615670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_basic_motions(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fcf9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sktime pipeline component testing\n",
    "\n",
    "from sktime.transformations.series.adapt import TabularToSeriesAdaptor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sktime.datasets import load_airline\n",
    "\n",
    "from sktime.datasets import load_longley\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.forecasting.compose import ForecastingPipeline\n",
    "from sktime.transformations.series.adapt import TabularToSeriesAdaptor\n",
    "from sktime.transformations.series.impute import Imputer\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#1 TabularToSeriesAdaptor-univariate\n",
    "# y = load_airline()\n",
    "# transformer = TabularToSeriesAdaptor(MinMaxScaler())\n",
    "# y_hat = transformer.fit_transform(y)\n",
    "# y\n",
    "\n",
    "#https://www.sktime.org/en/latest/api_reference/auto_generated/sktime.forecasting.compose.ForecastingPipeline.html\n",
    "\n",
    "#2 ForecastingPipeline-string/estimator pairs/multivariate\n",
    "# y, X = load_longley()\n",
    "# y_train, _, X_train, X_test = temporal_train_test_split(y, X)\n",
    "# fh = ForecastingHorizon(X_test.index, is_relative=False)\n",
    "\n",
    "# pipe = ForecastingPipeline(steps=[\n",
    "#     (\"imputer\", Imputer(method=\"mean\")),\n",
    "#     (\"minmaxscaler\", TabularToSeriesAdaptor(MinMaxScaler())),\n",
    "#     (\"forecaster\", NaiveForecaster(strategy=\"drift\")),\n",
    "# ])\n",
    "# pipe.fit(y_train, X_train)\n",
    "\n",
    "# y_pred = pipe.predict(fh=fh, X=X_test)\n",
    "# y_pred\n",
    "\n",
    "#2 ForecastingPipeline-string/estimator pairs/uniivariate\n",
    "y, X = load_longley()\n",
    "X=None\n",
    "\n",
    "pipe = ForecastingPipeline(steps=[\n",
    "    (\"imputer\", Imputer(method=\"mean\")),\n",
    "    (\"minmaxscaler\", TabularToSeriesAdaptor(MinMaxScaler())),\n",
    "    (\"forecaster\", NaiveForecaster(strategy=\"drift\")),\n",
    "])\n",
    "pipe.fit(y, X)\n",
    "\n",
    "y_pred = pipe.predict(fh=fh)\n",
    "y_pred\n",
    "\n",
    "#3 without strings\n",
    "pipe = ForecastingPipeline([\n",
    "    Imputer(method=\"mean\"),\n",
    "    TabularToSeriesAdaptor(MinMaxScaler()),\n",
    "    NaiveForecaster(strategy=\"drift\"),\n",
    "])\n",
    "\n",
    "#4a dunder method Note: * (= apply to y) has precedence over ** (= apply to X)\n",
    "forecaster = NaiveForecaster(strategy=\"drift\")\n",
    "imputer = Imputer(method=\"mean\")\n",
    "pipe = (imputer * MinMaxScaler()) ** forecaster\n",
    "\n",
    "#4b using the dunder method, alternative\n",
    "pipe = imputer ** MinMaxScaler() ** forecaster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf62c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.sktime.org/en/stable/get_started.html\n",
    "\n",
    "#1 Time Series Classification\n",
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "from sktime.datasets import load_arrow_head\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# X, y = load_arrow_head()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "# classifier = TimeSeriesForestClassifier()\n",
    "# classifier.fit(X_train, y_train)\n",
    "# y_pred = classifier.predict(X_test)\n",
    "# accuracy_score(y_test, y_pred)\n",
    "\n",
    "#2 Time Series Clustering\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sktime.clustering.k_means import TimeSeriesKMeans\n",
    "from sktime.clustering.utils.plotting._plot_partitions import plot_cluster_algorithm\n",
    "from sktime.datasets import load_arrow_head\n",
    "\n",
    "X, y = load_arrow_head()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "k_means = TimeSeriesKMeans(n_clusters=5, init_algorithm=\"forgy\", metric=\"dtw\")\n",
    "k_means.fit(X_train)\n",
    "plot_cluster_algorithm(k_means, X_test, k_means.n_clusters)\n",
    "\n",
    "#Time Series Annotation\n",
    "from sktime.annotation.adapters import PyODAnnotator\n",
    "from pyod.models.iforest import IForest\n",
    "from sktime.datasets import load_airline\n",
    "y = load_airline()\n",
    "pyod_model = IForest()\n",
    "pyod_sktime_annotator = PyODAnnotator(pyod_model)\n",
    "pyod_sktime_annotator.fit(y)\n",
    "annotated_series = pyod_sktime_annotator.predict(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3717807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multivariate VAR, VMA, VARMA, and VARMAX\n",
    "\n",
    "# https://towardsdatascience.com/lets-forecast-your-time-series-using-classical-approaches-f84eb982212c\n",
    "#1 VAR\n",
    "\n",
    "# Import libraries\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from random import random\n",
    "# Generate a sample dataset with correlated variables\n",
    "# data = list()\n",
    "# for i in range(100):\n",
    "#     v1 = i + random()\n",
    "#     v2 = v1 + random()\n",
    "#     row = [v1, v2]\n",
    "#     data.append(row)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data = sm.datasets.macrodata.load_pandas()['data']\n",
    "data['q_date'] = data.apply(lambda x: str(int(x['year'])) + '-Q' \n",
    "                                  + str(int(x['quarter'])), axis=1)\n",
    "target_col = 'realgdp'\n",
    "index_col = 'q_date'\n",
    "feat_list = ['realgdp', 'realcons', 'realinv', 'realgovt'] \n",
    "\n",
    "X = data[feat_list]\n",
    "y = data[target_col]\n",
    "\n",
    "data1 = data [['realgdp', 'realcons', 'realinv', 'realgovt', 'realgdp']] # parallal forecasting\n",
    "\n",
    "#1 VAR  \n",
    "model = VAR(X)\n",
    "model_fit = model.fit()\n",
    "# make prediction\n",
    "#yhat = model_fit.forecast(y, steps=1) #TBC\n",
    "\n",
    "#2 VMA - generalized version of the moving average model, p parameter as 0.\n",
    "# model = VARMAX(X, order=(0, 1))\n",
    "# model_fit = model.fit(disp=False)\n",
    "# yhat = model_fit.forecast()\n",
    "\n",
    "#3 VARMA - combination of VAR and VMA, requires p and q parameters\n",
    "# model = VARMAX(X, order=(1, 1))\n",
    "# model_fit = model.fit(disp=False)\n",
    "# yhat = model_fit.forecast()\n",
    "\n",
    "#4 SARIMA with Exogenous Variables (SARIMAX)\n",
    "'''The Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX) is\n",
    "an extension of the SARIMA model.\n",
    "Suitable for univariate time series with trend and/or seasonal components and exogenous variables.\n",
    "'''\n",
    "# data1 = X[0:100]\n",
    "# data2 = X[100:]\n",
    "# model = SARIMAX(data1, exog=data2, order=(1, 1, 1), seasonal_order=(0, 0, 0, 0))\n",
    "# model_fit = model.fit(disp=False)\n",
    "# # make prediction\n",
    "# exog2 = [200 + random()]\n",
    "# yhat = model_fit.predict(len(data1), len(data1), exog=[exog2])\n",
    "#ValueError: The indices for endog and exog are not aligned\n",
    "\n",
    "#5 VARMAX-extension of the VARMA model\n",
    "#multivariate time series without trend and seasonal components with exogenous variables.\n",
    "\n",
    "#model = VARMAX(data, exog=X, order=(1, 1))\n",
    "# model_fit = model.fit(disp=False)\n",
    "# data_exog2 = [[100]]\n",
    "# yhat = model_fit.forecast(exog=data_exog2)\n",
    "#ValueError: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).\n",
    "\n",
    "#6 SimpleExpSmoothing\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "data = y\n",
    "model = SimpleExpSmoothing(data)\n",
    "model_fit = model.fit()\n",
    "# make prediction\n",
    "yhat = model_fit.predict(len(data), len(data))\n",
    "print(yhat)\n",
    "\n",
    "#7 Double Exponential Smoothing - seasonal=None\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "data = y\n",
    "model = ExponentialSmoothing(data, trend='add', seasonal=None, damped=True)\n",
    "model_fit = model.fit()\n",
    "yhat = model_fit.predict(len(data), len(data))\n",
    "\n",
    "#8 Triple Exponential Smoothing - seasonal=\"add\"\n",
    "data = y\n",
    "model = ExponentialSmoothing(data,trend=\"add\", seasonal=\"add\", seasonal_periods=12, damped=True)\n",
    "model_fit = model.fit()\n",
    "yhat = model_fit.predict(len(data), len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c468e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "bike_sharing = fetch_openml(\n",
    "    \"Bike_Sharing_Demand\", version=2, as_frame=True)\n",
    "df = bike_sharing.frame\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5d2b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/total-digital-factory/sktime-feature-importance-on-timeseriesforestclassifier-7cbf3d44e939\n",
    "\n",
    "from sktime.transformations.panel.compose import ColumnConcatenator\n",
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "config = {\n",
    "    \"model_params\": {\n",
    "        \"n_estimators\": 5,\n",
    "        \"min_interval\": 3,\n",
    "    }\n",
    "}\n",
    "\n",
    "steps = [\n",
    "  ('concatenate', ColumnConcatenator()),\n",
    "  ('classify', TimeSeriesForestClassifier(**config['model_params']))\n",
    "]\n",
    "clf = Pipeline(steps)\n",
    "\n",
    "# create normal df\n",
    "time_series_dataframe = pd.DataFrame({\n",
    "    \"feature_0\": [0., 0., 0., 1., 2., 3.],\n",
    "    \"feature_1\": [10., 20., 30., 40., 50., 60.],\n",
    "    \"feature_2\": [-4., -5., -6., -7., -8., -9.]\n",
    "    },\n",
    "    index=[\n",
    "        \"2022-01-01 00:00:00\",\n",
    "        \"2022-01-01 01:00:00\",\n",
    "        \"2022-01-01 02:00:00\",\n",
    "        \"2022-01-01 03:00:00\",\n",
    "        \"2022-01-01 04:00:00\",\n",
    "        \"2022-01-01 05:00:00\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "time_series_dataframe\n",
    "\n",
    "# create windowed_chunk_time_series_dataframe\n",
    "windowed_chunk_time_series_dataframe = pd.DataFrame({\n",
    "    \"dim_0\": [\n",
    "        pd.Series([0., 0., 0.]),\n",
    "        pd.Series([1., 2., 3.])\n",
    "    ],\n",
    "    \"dim_1\": [\n",
    "        pd.Series([10., 20., 30.]),\n",
    "        pd.Series([40., 50., 60.])\n",
    "    ],\n",
    "    \"dim_2\": [\n",
    "        pd.Series([-4., -5., -6.]),\n",
    "        pd.Series([-7., -8., -9.])\n",
    "    ],\n",
    "},\n",
    "    index=[\"frame_0\", \"frame_1\"]\n",
    ")\n",
    "\n",
    "\n",
    "windowed_chunk_time_series_dataframe_label = pd.DataFrame({\n",
    "        \"y\": [0, 1]\n",
    "    },\n",
    "    index=[\"frame_0\", \"frame_1\"]\n",
    ")\n",
    "\n",
    "windowed_chunk_time_series_dataframe\n",
    "\n",
    "#3 input_dataframe\n",
    "input_dataframe = pd.DataFrame({\n",
    "    \"dim_0\": [\n",
    "        pd.Series([0., 0., 0.]),\n",
    "        pd.Series([1., 2., 3.])\n",
    "    ],\n",
    "    \"dim_1\": [\n",
    "        pd.Series([10., 20., 30.]),\n",
    "        pd.Series([40., 50., 60.])\n",
    "    ],\n",
    "    \"dim_2\": [\n",
    "        pd.Series([-4., -5., -6.]),\n",
    "        pd.Series([-7., -8., -9.])\n",
    "    ],\n",
    "},\n",
    "    index=[\"frame_0\", \"frame_1\"]\n",
    ")\n",
    "\n",
    "input_dataframe\n",
    "\n",
    "# ColumnConcatenator\n",
    "concatenator = ColumnConcatenator()\n",
    "\n",
    "output_dataframe = concatenator.fit_transform(input_dataframe)\n",
    "\n",
    "expected_output_dataframe = pd.DataFrame({\n",
    "    0: [\n",
    "        pd.Series([0., 0., 0., 10., 20., 30., -4., -5., -6.]),\n",
    "        pd.Series([1., 2., 3., 40., 50., 60., -7., -8., -9.])\n",
    "    ]\n",
    "},\n",
    "    index=[\"frame_0\", \"frame_1\"]\n",
    ")\n",
    "expected_output_dataframe.index.names = [\"instances\"]\n",
    "\n",
    "pd.testing.assert_frame_equal(output_dataframe, expected_output_dataframe)\n",
    "\n",
    "expected_output_dataframe\n",
    "\n",
    "clf[\"classify\"].fit(\n",
    "    expected_output_dataframe, \n",
    "    windowed_chunk_time_series_dataframe_label.values.reshape(-1)\n",
    ")\n",
    "print(clf[\"classify\"].intervals_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c52353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing Sktime transform time sequence to supervise - AutoARIMA\n",
    "#https://mpatiparn-n.medium.com/xgboost-time-series-forcasting-with-sktime-ep-2-5a5525d902ab\n",
    "\n",
    "def ts_forecast_period_index(ts, periods=36, freq=\"M\"):\n",
    "    \n",
    "    last_index = ts.index.to_series().astype(str).values.tolist()[-1]\n",
    "    \n",
    "    from sktime.forecasting.base import ForecastingHorizon\n",
    "    fh = ForecastingHorizon(\n",
    "        pd.PeriodIndex(pd.date_range(last_index, periods=periods, freq=freq)), is_relative=False\n",
    "    )\n",
    "    fh = fh[1:] # drop last index\n",
    "    return fh\n",
    "    \n",
    "#1 load the data- monthly - shampoo-sales.csv\n",
    "data_path='D:\\\\timeseries\\\\'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sktime.datasets import load_airline\n",
    "from sktime.utils.plotting import plot_series\n",
    "\n",
    "y = load_airline()\n",
    "fh = ts_forecast_period_index(y, periods=36, freq=\"M\")\n",
    "\n",
    "#1 AutoARIMA\n",
    "from sktime.forecasting.arima import AutoARIMA\n",
    "forecaster = AutoARIMA(start_p=8, max_p=9, suppress_warnings=True)\n",
    "\n",
    "forecaster.fit(y)\n",
    "y_pred = forecaster.predict(fh)\n",
    "#plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"])\n",
    "\n",
    "#2 regression - transform the time series into tabular or panel data\n",
    "from sktime.forecasting.compose import make_reduction\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "regressor = GradientBoostingRegressor()\n",
    "forecaster = make_reduction(regressor, window_length=5, strategy=\"recursive\")\n",
    "forecaster.fit(y)\n",
    "y_pred = forecaster.predict(fh)\n",
    "\n",
    "#cross-validation\n",
    "fh1 = ['1961-01', '1961-02', '1961-03', '1961-04', '1961-05', '1961-06',\n",
    "             '1961-07', '1961-08', '1961-09', '1961-10', '1961-11', '1961-12',\n",
    "             '1962-01', '1962-02', '1962-03', '1962-04', '1962-05', '1962-06',\n",
    "             '1962-07', '1962-08', '1962-09', '1962-10', '1962-11', '1962-12',\n",
    "             '1963-01', '1963-02', '1963-03', '1963-04', '1963-05', '1963-06',\n",
    "             '1963-07', '1963-08', '1963-09', '1963-10', '1963-11']\n",
    "fh2 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "from sktime.forecasting.model_selection import ExpandingWindowSplitter\n",
    "cv = ExpandingWindowSplitter(step_length=12, fh=fh2, initial_window=52)\n",
    "#cv = ExpandingWindowSplitter(step_length=12, fh=fh1, initial_window=72)\n",
    "\n",
    "window_length = 10\n",
    "regressor = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "forecaster = make_reduction(regressor, strategy=\"recursive\", window_length= window_length)\n",
    "\n",
    "results = evaluate(\n",
    "    forecaster=forecaster, y=y, cv=cv, strategy=\"refit\", return_data=True\n",
    ")\n",
    "results\n",
    "\n",
    "#GridSearchCV-ExpandingWindowSplitter\n",
    "from sktime.forecasting.model_selection import (ForecastingGridSearchCV, SlidingWindowSplitter)\n",
    "\n",
    "param_grid = {\n",
    "    'estimator__max_depth': [3, 6, 10, 15],\n",
    "    'estimator__learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'estimator__colsample_bytree': np.arange(0.4, 1.0),\n",
    "    'estimator__n_estimators': [100, 500, 1000]\n",
    "}\n",
    "\n",
    "regressor = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "forecaster = make_reduction(regressor, strategy=\"recursive\")\n",
    "\n",
    "cv = ExpandingWindowSplitter(step_length=12, fh=fh2, initial_window=52)\n",
    "gscv = ForecastingGridSearchCV(\n",
    "    forecaster, cv=cv, param_grid=param_grid, strategy=\"refit\"\n",
    ")\n",
    "\n",
    "gscv\n",
    "#GridSearchCV-SlidingWindowSplitter\n",
    "\n",
    "cv = SlidingWindowSplitter(\n",
    "        initial_window=int(len(y_train) * 0.5),\n",
    "        start_with_window=True,\n",
    "    )\n",
    "# cv = SlidingWindowSplitter(fh = 3, window_length=3, initial_window=3)\n",
    "# cv = ExpandingWindowSplitter(fh = 1, window_length=5, initial_window=2, start_with_window=True)\n",
    "MidForecasterGSCV = ForecastingGridSearchCV(MidForecaster, cv=cv, param_grid=param_grid, scoring=MetricFunctionWrapper(mean_absolute_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c1aabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diff models of sktime\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "\n",
    "data_path='D:\\\\timeseries\\\\'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sktime.datasets import load_airline\n",
    "from sktime.utils.plotting import plot_series\n",
    "\n",
    "y = load_airline()\n",
    "\n",
    "fh_abs = ForecastingHorizon(y.index, is_relative=False)\n",
    "\n",
    "from sktime.forecasting.trend import PolynomialTrendForecaster\n",
    "forecaster = PolynomialTrendForecaster(degree=1)\n",
    "forecaster.fit(y)\n",
    "\n",
    "y_pred = forecaster.predict(fh=[1,2,3])\n",
    "#y_pred = forecaster.predict(fh=fh_abs)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176763ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sktime.registry import all_estimators\n",
    "\n",
    "# all_estimators(\n",
    "#     \"forecaster\", as_dataframe=True, return_tags=[\"scitype:y\", \"requires-fh-in-fit\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20acc02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "dt1 = parser.parse(\"06 April, 2019\")\n",
    "dt2 = parser.parse(\"1960-08\") #year month\n",
    "dt3 = parser.parse(\"1960\") #year\n",
    "dt4 = parser.parse(\"3-12\") #year\n",
    "\n",
    "date = dt1.date()\n",
    "year = dt1.year\n",
    "month = dt1.month\n",
    "hour = dt1.hour\n",
    "minute = dt1.minute\n",
    "weekday = dt1.weekday()\n",
    "time = dt1.time()\n",
    "\n",
    "print(f'dt1:{dt1}, dt2:{dt2}, dt3:{dt3}, dt4:{dt4}')\n",
    "print(f'year:{year}, month:{month}, date:{date}, hour:{hour}, minute:{minute}, weekday:{weekday}, time:{time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3fc9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e63ec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y = y_test\n",
    "y_hat = y_pred\n",
    "np.mean(np.abs((y - np.ceil(y_hat)) / y))\n",
    "\n",
    "np.abs((y - np.ceil(y_hat)) / y)\n",
    "\n",
    "metrics_dict = {'uniform_average': np.mean(np.abs((y - np.ceil(y_hat)) / y)),\n",
    "                    'raw_values': np.abs((y - np.ceil(y_hat)) / y)}\n",
    "multioutput = 'uniform_average'\n",
    "metrics_dict[multioutput]\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95465401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline testing\n",
    "\n",
    "#https://www.kaggle.com/code/forward469/sktime-baseline\n",
    "model = Pipeline(steps=[\n",
    "    ('polyfeatures', PolynomialFeatures(degree=1)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', KNeighborsRegressor(n_neighbors=2, p=1)),\n",
    "])\n",
    "\n",
    "LowForecaster = ReducedForecaster(model, scitype=\"regressor\", strategy='direct', window_length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab8308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# regressor = KNeighborsRegressor(n_neighbors=3)\n",
    "\n",
    "# forecaster = make_reduction(regressor, strategy=\"recursive\", window_length= window_length)\n",
    "# forecaster.fit(y_train)\n",
    "# y_pred = forecaster.predict(fh)\n",
    "\n",
    "from sktime.forecasting.compose import make_reduction\n",
    "from sktime.datasets import load_airline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "y = load_airline()\n",
    "regressor = GradientBoostingRegressor()\n",
    "forecaster = make_reduction(regressor, window_length=15, strategy=\"recursive\")\n",
    "forecaster.fit(y)\n",
    "\n",
    "y_pred = forecaster.predict(fh=[1,2,3])\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76505440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing Sktime transform time sequence to supervise - AutoARIMA\n",
    "\n",
    "#https://mpatiparn-n.medium.com/xgboost-time-series-forcasting-with-sktime-ep-2-5a5525d902ab\n",
    "\n",
    "#1 load the data- monthly - shampoo-sales.csv\n",
    "data_path='D:\\\\timeseries\\\\'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sktime.datasets import load_airline\n",
    "from sktime.utils.plotting import plot_series\n",
    "\n",
    "y = load_airline()\n",
    "\n",
    "#!pip install sktime\n",
    "from sktime.forecasting.arima import AutoARIMA\n",
    "\n",
    "forecaster = AutoARIMA(start_p=8, max_p=9, suppress_warnings=True)\n",
    "\n",
    "last_index = y.index.to_series().astype(str).values.tolist()[-1]\n",
    "\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "fh = ForecastingHorizon(\n",
    "    pd.PeriodIndex(pd.date_range(\"1961-01\", periods=36, freq=\"M\")), is_relative=False\n",
    ")\n",
    "fh = fh[1:] # drop last index\n",
    "\n",
    "forecaster.fit(y)\n",
    "y_pred = forecaster.predict(fh)\n",
    "#plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"])\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5fd558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#date features\n",
    "# convert object type column into datetime datatype column\n",
    "df['Time'] = pd.to_datetime(df['Time'])\n",
    "\n",
    "# Extrating 'Hour_of_Day' feature from the Time column\n",
    "new_df = df.copy()\n",
    "new_df['Hour_of_Day'] = new_df['Time'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d08d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing sktime\n",
    "\n",
    "#https://www.sktime.org/en/stable/examples/01_forecasting.html\n",
    "from sktime.datasets import load_airline\n",
    "from sktime.utils.plotting import plot_series\n",
    "\n",
    "y = load_airline()\n",
    "\n",
    "# plotting for visualization\n",
    "plot_series(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7d547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing Smoothing methods for Time Series Forecasting and ACF/PACF\n",
    "\n",
    "#https://medium.com/@manuktiwary/time-series-forecasting-concepts-and-methods-with-implementation-examples-edaf40dceee5\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "#1 load the data- monthly - shampoo-sales.csv\n",
    "data_path='D:\\\\timeseries\\\\'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(data_path+'shampoo-sales.csv')\n",
    "index_col = 'Month'\n",
    "value_col = 'Sales'\n",
    "target_col = 'Target'\n",
    "df = df.set_index(index_col)\n",
    "\n",
    "# Restructure the data\n",
    "df[target_col] = df[value_col].shift(-1).values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "y = df[target_col]\n",
    "X = df.drop(target_col, axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = \\\n",
    "      train_test_split(X, y, test_size = 0.12, \\\n",
    "                       random_state = 0, shuffle=False)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "acf = sm.tsa.acf(X)\n",
    "pacf = sm.tsa.pacf(X)\n",
    "\n",
    "#plot the ACF/PACF\n",
    "# sm.graphics.tsa.plot_acf(acf)\n",
    "# sm.graphics.tsa.plot_pacf(pacf)\n",
    "\n",
    "#2 load the data- yearly\n",
    "data = sm.datasets.sunspots.load_pandas().data\n",
    "index_col = 'YEAR'\n",
    "value_col = 'SUNACTIVITY'\n",
    "target_col = 'Target'\n",
    "data = data.set_index(index_col)\n",
    "\n",
    "#1 Simple moving average (SMA)\n",
    "window_size=3\n",
    "smoothed_data = data.rolling(window_size).mean()\n",
    "\n",
    "#2 Weighted moving average (WMA)\n",
    "weights = np.arange(1, 6) # weights for ast 5 time steps\n",
    "window_size=5\n",
    "smoothed_data = data.rolling(window_size).apply(lambda x: np.average(x, weights=weights))\n",
    "\n",
    "#3. Exponential smoothing\n",
    "alpha = 0.5\n",
    "smoothed_data = data.ewm(alpha).mean()\n",
    "\n",
    "#4 Holts Exponential Smoothing\n",
    "model = sm.tsa.Holt(data).fit()\n",
    "forecast = model.forecast(steps=10)\n",
    "\n",
    "#5 Holt-Winters exponential smoothing\n",
    "model = ExponentialSmoothing(data[value_col], trend='add', seasonal='add', seasonal_periods=12).fit()\n",
    "forecast = model.forecast(steps=10)\n",
    "forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669793fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TS as supervised problem\n",
    "\n",
    "def tt_cv_pt_regression(X, y, X_train, y_train, X_valid, y_valid, X_test, y_test, model, param_grid, cv=2, train_type='TrainTest',\n",
    "            test_type='XYZ'):\n",
    "    '''\n",
    "    ver 1.4 Oct01,2022 : updated rmse/mse\n",
    "    ver 1.3 Sep14,2022 : added train_type / test_type/pred_df check\n",
    "    ver 1.2 Sep12,2022 : added tt/cv/pt optionl\n",
    "    ver 1.1 : added mape_mean\n",
    "    ver 1.0\n",
    "    '''\n",
    "    print('entry tt_cv_pt_regression')\n",
    "    from sklearn.metrics import explained_variance_score, max_error, mean_absolute_error, mean_squared_error, \\\n",
    "            mean_squared_log_error\n",
    "    from sklearn.metrics import median_absolute_error, r2_score, explained_variance_score, mean_absolute_percentage_error\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold\n",
    "    from sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict\n",
    "\n",
    "    from numpy import argmax, sqrt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from imblearn.pipeline import Pipeline, make_pipeline\n",
    "    #from sklearn.pipeline import Pipeline, make_pipeline\n",
    "    \n",
    "    #1 get model name\n",
    "    if type(model) == Pipeline:\n",
    "        model_name = model['model'].__class__.__name__\n",
    "        print('if model: pipeline:', model_name)\n",
    "    else:\n",
    "        model_name = model.__class__.__name__\n",
    "        print('else model: direct model:', model_name)  \n",
    "        \n",
    "    print('model/pipeline start:', model)\n",
    "    #2 n_features\n",
    "    if X_train is not None:\n",
    "        n_features = X_train.shape[1]\n",
    "    else:\n",
    "        n_features = X.shape[1]\n",
    "    \n",
    "    #3 call train_test / cross_val / grid_search\n",
    "    if train_type == 'TrainTest':\n",
    "        print('2 entry TrainTest')\n",
    "        print(f'X_train:{X_train.shape}, y_train:{y_train.shape}, X:{X.shape}, y:{y.shape}')\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        trained_model = model # if pipeline passed then it's trained_pipeline\n",
    "        \n",
    "        y_preds = model.predict(X_valid)\n",
    "        y_actual = y_valid\n",
    "        #print(f'y_actual:{set(y_actual)}')\n",
    "        print('2 exit TrainTest')\n",
    "\n",
    "    elif (train_type == 'CrossValid') | (train_type == 'NestedCV'):\n",
    "        print('2 entry CrossValid / NestedCV')\n",
    "        # default cv does not work; ValueError: Cannot predict random effects from singular covariance structure.\n",
    "        trained_model = model #for prediction\n",
    "        X_valid = X\n",
    "        y_actual = y\n",
    "        #print('model-CrossValid:', model)\n",
    "        y_preds = cross_val_predict(model, X, y, cv=cv)\n",
    "        print('2 exit CrossValid')\n",
    "        \n",
    "    elif (train_type == 'RandomSearch') | (train_type == 'GridSearch'):\n",
    "        print('2 entry GridSearch')\n",
    "        if train_type == 'GridSearch':\n",
    "            estimator = GridSearchCV(model, param_grid=param_grid, cv=cv)\n",
    "        else:\n",
    "            estimator = RandomizedSearchCV(model, param_distributions=param_grid, cv=cv)\n",
    "            \n",
    "        print('get_params:', estimator.get_params().keys())\n",
    "        # estimator.fit(X, y)\n",
    "        estimator.fit(X_train, y_train)\n",
    "        best_score = estimator.best_score_\n",
    "        best_params = estimator.best_params_\n",
    "        best_estimator = estimator.best_estimator_\n",
    "        cv_results = estimator.cv_results_\n",
    "        gs_result_df = pd.DataFrame(cv_results)\n",
    "        \n",
    "        trained_model = best_estimator #for prediction\n",
    "        y_preds = best_estimator.predict(X_valid)\n",
    "        y_actual = y_valid\n",
    "        #print(f'y_actual:{set(y_actual)}')\n",
    "        print('2 exit GridSearch')\n",
    "        \n",
    "    #4 predcit df\n",
    "    pred_df = X_valid.copy()\n",
    "    if isinstance(pred_df, pd.DataFrame):\n",
    "        pass\n",
    "    elif isinstance(pred_df, (np.ndarray, np.generic)):\n",
    "        pred_df = pd.DataFrame(pred_df)\n",
    "        \n",
    "    pred_df['target'] = y_actual\n",
    "    pred_df['pred'] = y_preds\n",
    "    \n",
    "    #5 score\n",
    "    #mse = round(mean_squared_error(y_actual, y_preds), 2) # does not work for nan\n",
    "    #rmse = round(sqrt(mse), 2)\n",
    "    \n",
    "    mse = round(mean_squared_error(y_actual, y_preds, squared=True), 2)\n",
    "    rmse = round(mean_squared_error(y_actual, y_preds, squared=False), 2)\n",
    "    \n",
    "    mae = round(mean_absolute_error(y_actual, y_preds), 2)\n",
    "    mdae = round(median_absolute_error(y_actual, y_preds), 2)\n",
    "    mape_mean = round(mean_absolute_percentage_error(y_actual, y_preds), 2)\n",
    "    error_max = round(max_error(y_actual, y_preds), 2)\n",
    "    #error_log = round(mean_squared_log_error(y_actual, y_preds), 2)\n",
    "\n",
    "    variance_score = round(explained_variance_score(y_actual, y_preds), 2)\n",
    "    r2_score = round(r2_score(y_actual, y_preds), 2)\n",
    "    Adjst_R_Squre = round(1 - ((1 - r2_score) * (len(y_actual) - 1) / (len(y_actual) - n_features - 1)), 2)\n",
    "    \n",
    "    #5 save result_val in dict\n",
    "    report_dict = dict()\n",
    "    # report_dict['tweedie_deviance_mean'] = tweedie_deviance_mean\n",
    "    report_dict['model'] = model_name\n",
    "    report_dict['test_type'] = test_type\n",
    "    report_dict['train_type'] = train_type\n",
    "    \n",
    "    report_dict['mse'] = mse # does not work for nan\n",
    "    report_dict['rmse_mean'] = rmse\n",
    "    report_dict['mae_mean'] = mae\n",
    "    report_dict['mae_median'] = mdae\n",
    "    #report_dict['mape_mean'] = mape_mean\n",
    "    report_dict['error_max'] = error_max\n",
    "    #report_dict['error_log'] = error_log\n",
    "\n",
    "    report_dict['variance_score'] = variance_score\n",
    "    report_dict['r2_score'] = r2_score\n",
    "    report_dict['Adjst_R_Squre'] = Adjst_R_Squre\n",
    "\n",
    "    print('exit tt_cv_pt_regression')\n",
    "    return report_dict, pred_df, trained_model\n",
    "\n",
    "#1 data shampoo-sales.csv\n",
    "data_path='D:\\\\timeseries\\\\'\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(data_path+'shampoo-sales.csv')\n",
    "index_col = 'Month'\n",
    "target_shift_col = 'Sales'\n",
    "target_col = 'Target'\n",
    "df = df.set_index(index_col)\n",
    "\n",
    "# Restructure the data\n",
    "df[target_col] = df[target_shift_col].shift(-1).values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "y = df[target_col]\n",
    "X = df.drop(target_col, axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = \\\n",
    "      train_test_split(X, y, test_size = 0.12, \\\n",
    "                       random_state = 0, shuffle=False)\n",
    "import xgboost\n",
    "reg = xgboost.XGBRegressor(objective='reg:squarederror', n_estimators=1000)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "predictions_xgb = reg.predict(X_test)\n",
    "\n",
    "model = xgboost.XGBRegressor(objective='reg:squarederror', n_estimators=1000)\n",
    "param_grid = {}\n",
    "report_dict, pred_df, trained_model = tt_cv_pt_regression(X, y, X_train, y_train, X_valid, y_valid,\n",
    "        X_test, y_test, model, param_grid, cv=2, train_type='TrainTest', test_type='XYZ')\n",
    "\n",
    "report_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a1ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/tanmay111999/gdmbdf-ar-ma-arma-arima-sarima#Dataset-Information\n",
    "def SARIMA_regression(train_ts=None, test_ts=None, n_lags=7, trend='c', seasonal=False, p=2,\n",
    "           d=3, q=4, s=2, ts_type='AR'):\n",
    "    '''\n",
    "    ver 1.2 Jan13,2023 TBA simple exponential smoothing, Holt-Winters exponential smoothing\n",
    "    ver 1.1 Jan12,2023 added prediction and score\n",
    "    ver 1.0 Jan10,2023 initial ver\n",
    "    \n",
    "    ARIMA model is a combination of 3 models :(p,d,q) is known as the order of the ARIMA model.\n",
    "    AR (p) : Auto Regressive\n",
    "    I (d) : Integrated\n",
    "    MA (q) : Moving Average\n",
    "\n",
    "    p : Number of auto regressive terms.\n",
    "    d : Number of differencing orders required to make the time series stationary.\n",
    "    q : Number of lagged forecast errors in the prediction equation.\n",
    "    s : seasonality used in SARIMA\n",
    "    '''\n",
    "    \n",
    "    data = train_ts.copy()\n",
    "    print('entry SARIMA_regression')\n",
    "    from sklearn.metrics import explained_variance_score, max_error, mean_absolute_error, mean_squared_error, \\\n",
    "            mean_squared_log_error\n",
    "    from sklearn.metrics import median_absolute_error, r2_score, explained_variance_score, mean_absolute_percentage_error\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold\n",
    "    from sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict\n",
    "\n",
    "    from numpy import argmax, sqrt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    from statsmodels.tsa.ar_model import AutoReg\n",
    "    from statsmodels.tsa.arima.model import ARIMA\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    \n",
    "    if ts_type=='AR':\n",
    "        d=0; q=0\n",
    "        model = ARIMA(endog = data, order = (p, d, q))\n",
    "    elif ts_type=='AR1':\n",
    "        model = AutoReg(endog = data, \\\n",
    "                   lags = n_lags, \\\n",
    "                   trend=trend, \\\n",
    "                   seasonal = seasonal, \\\n",
    "                   exog = None, \\\n",
    "                   hold_back = None, \\\n",
    "                   period = None, \\\n",
    "                   missing = 'none')\n",
    "        \n",
    "    elif ts_type=='MA':\n",
    "        p=0; d=0\n",
    "        model = ARIMA(endog = data, order=(p, d, q))\n",
    "        \n",
    "    elif ts_type=='ARMA':\n",
    "        d=0\n",
    "        model = ARIMA(endog = data, order=(p, d, q))\n",
    "    \n",
    "    elif ts_type=='ARIMA':\n",
    "        model = ARIMA(endog = data, order=(p, d, q))\n",
    "        \n",
    "    elif ts_type=='SARIMA':\n",
    "        #p=0; d=0; q=0\n",
    "        #model = SARIMAX(endog = data, order = (p, d, q), seasonal_order=(p, d, q, s))\n",
    "        model = SARIMAX(endog = data, order = (0, 0, 0), seasonal_order=(p, d, q, s))\n",
    "        \n",
    "    elif ts_type=='ExponentialSmoothing':\n",
    "        from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "        model = ExponentialSmoothing(endog=data, \\\n",
    "                                        trend='add', \\\n",
    "                                        seasonal='add', \\\n",
    "                                        seasonal_periods=12, \\\n",
    "                                        damped=True)\n",
    "    elif ts_type=='AutoArima':\n",
    "        from pmdarima.arima import auto_arima\n",
    "        model = auto_arima(y = data, seasonal=False, stepwise=True)\n",
    "        \n",
    "    elif ts_type=='prophet':\n",
    "        #TBC\n",
    "        #from fbprophet import Prophet\n",
    "        model = Prophet( \\\n",
    "                           interval_width = 0.95, \\\n",
    "                           growth = \"linear\", \\\n",
    "                           daily_seasonality = False, \\\n",
    "                           weekly_seasonality = False, \\\n",
    "                           yearly_seasonality = False, \\\n",
    "                           seasonality_mode = \"multiplicative\")\n",
    "        \n",
    "    if ts_type in ['ExponentialSmoothing']:\n",
    "        model_fit = model.fit()\n",
    "        trained_model = model_fit\n",
    "        predictions = model_fit.predict(start=len(train_ts), end=len(train_ts)+len(test_ts)-1)\n",
    "    elif ts_type in ['AutoArima']:\n",
    "        trained_model = model\n",
    "        predictions = model.predict(n_periods=len(test_ts), X=None, return_conf_int=False, alpha=0.05)#array\n",
    "        predictions = pd.Series(predictions) # array to series\n",
    "    else:\n",
    "        model_fit = model.fit()\n",
    "        trained_model = model_fit\n",
    "        predictions = model_fit.predict(start=len(train_ts), end=len(train_ts)+len(test_ts)-1, dynamic=False)\n",
    "    \n",
    "    predictions.name = \"Predictions\"\n",
    "    \n",
    "    # predcit df\n",
    "    pred_df = test_ts.copy()\n",
    "    if isinstance(pred_df, pd.DataFrame):\n",
    "        pred_df['pred'] = predictions.values\n",
    "    elif isinstance(pred_df, (np.ndarray, np.generic, pd.Series)):\n",
    "        pred_df.name = 'target'\n",
    "        pred_df = pd.DataFrame(pred_df)\n",
    "        pred_df['pred'] = predictions.values\n",
    "        \n",
    "    #5 score\n",
    "    #mse = round(mean_squared_error(y_actual, y_preds), 2) # does not work for nan\n",
    "    #rmse = round(sqrt(mse), 2)\n",
    "    y_actual = test_ts.values\n",
    "    y_preds = predictions.values\n",
    "    n_features = 1\n",
    "    \n",
    "    mse = round(mean_squared_error(y_actual, y_preds, squared=True), 2)\n",
    "    rmse = round(mean_squared_error(y_actual, y_preds, squared=False), 2)\n",
    "    \n",
    "    mae = round(mean_absolute_error(y_actual, y_preds), 2)\n",
    "    mdae = round(median_absolute_error(y_actual, y_preds), 2)\n",
    "    mape_mean = round(mean_absolute_percentage_error(y_actual, y_preds), 2)\n",
    "    error_max = round(max_error(y_actual, y_preds), 2)\n",
    "    #error_log = round(mean_squared_log_error(y_actual, y_preds), 2)\n",
    "\n",
    "    variance_score = round(explained_variance_score(y_actual, y_preds), 2)\n",
    "    r2_score = round(r2_score(y_actual, y_preds), 2)\n",
    "    Adjst_R_Squre = round(1 - ((1 - r2_score) * (len(y_actual) - 1) / (len(y_actual) - n_features - 1)), 2)\n",
    "    \n",
    "    #5 save result_val in dict\n",
    "    report_dict = dict()\n",
    "    # report_dict['tweedie_deviance_mean'] = tweedie_deviance_mean\n",
    "    report_dict['model'] = ts_type\n",
    "    report_dict['test_type'] = 'test_type' #TBC\n",
    "    report_dict['train_type'] = 'train_type' #TBC\n",
    "    \n",
    "    report_dict['mse'] = mse # does not work for nan\n",
    "    report_dict['rmse_mean'] = rmse\n",
    "    report_dict['mae_mean'] = mae\n",
    "    report_dict['mae_median'] = mdae\n",
    "    #report_dict['mape_mean'] = mape_mean\n",
    "    report_dict['error_max'] = error_max\n",
    "    #report_dict['error_log'] = error_log\n",
    "\n",
    "    report_dict['variance_score'] = variance_score\n",
    "    report_dict['r2_score'] = r2_score\n",
    "    report_dict['Adjst_R_Squre'] = Adjst_R_Squre\n",
    "\n",
    "    print('exit SARIMA_regression')\n",
    "    return report_dict, pred_df, trained_model\n",
    "    \n",
    "\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "#1 data shampoo-sales.csv\n",
    "data_path='D:\\\\timeseries\\\\'\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(data_path+'shampoo-sales.csv')\n",
    "index_col = 'Month'\n",
    "target_col = 'Sales'\n",
    "df = df.set_index(index_col)\n",
    "target_ts = df[target_col]\n",
    "total_data = df[target_col].count()\n",
    "\n",
    "#split = (df.shape[0] * 0.90).astype(np.int32)\n",
    "split = (total_data * 0.90).astype(np.int32)\n",
    "train = df[0:split] \n",
    "test = df[split:]\n",
    "df1 = test.copy(deep = True)\n",
    "\n",
    "test = test['Sales']\n",
    "report_dict, pred_df, trained_model = SARIMA_regression(train_ts=train, test_ts=test, n_lags=7, trend='c', seasonal=False,\n",
    "                   p=2, d=3, q=4, s=2, ts_type='ExponentialSmoothing')\n",
    "report_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e062f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing ARIMA\n",
    "\n",
    "#1 data shampoo-sales.csv\n",
    "data_path='D:\\\\timeseries\\\\'\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(data_path+'shampoo-sales.csv')\n",
    "index_col = 'Month'\n",
    "target_col = 'Sales'\n",
    "df = df.set_index(index_col)\n",
    "target_ts = df[target_col]\n",
    "total_data = df[target_col].count()\n",
    "\n",
    "#split = (df.shape[0] * 0.90).astype(np.int32)\n",
    "split = (total_data * 0.90).astype(np.int32)\n",
    "train = df[0:split] \n",
    "test = df[split:]\n",
    "df1 = test.copy(deep = True)\n",
    "\n",
    "p=2; d=1; q=1; s=2;\n",
    "#model = SARIMAX(endog = data, order = (0, 0, 0), seasonal_order=(p, d, q, s))\n",
    "model = ARIMA(endog = train, order=(p, d, q))\n",
    "model_fit = model.fit()\n",
    "train_ts = train; test_ts=test\n",
    "#predictions = model_fit.predict(start=len(train_ts), end=len(train_ts)+len(test_ts)-1, dynamic=False)\n",
    "#predictions = model_fit.forecast(steps=4) # same as above\n",
    "\n",
    "# from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "# model = ExponentialSmoothing(endog=data, \\\n",
    "#                                 trend='add', \\\n",
    "#                                 seasonal='add', \\\n",
    "#                                 seasonal_periods=12, \\\n",
    "#                                 damped=True)\n",
    "#predictions = model_fit.predict(start=len(train_ts), end=len(train_ts)+len(test_ts)-1)\n",
    "#predictions = model_fit.forecast(steps=4)\n",
    "\n",
    "\n",
    "# from pmdarima.arima import auto_arima\n",
    "# model = auto_arima(y = data, seasonal=False, stepwise=True)\n",
    "\n",
    "# predictions = model.predict(n_periods=len(test_ts), \\\n",
    "#                                         X=None, \\\n",
    "#                                         return_conf_int=False, \\\n",
    "#                                         alpha=0.05)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6031025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(model_fit)\n",
    "model_fit.forecast(steps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa1a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(['a', 'b', 'c', 'd', 'e'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2be0f9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_features(df, fldname, drop_date=True, drop_year=True, state_col='ca'):\n",
    "    '''\n",
    "    1.3 : commented df1[targ_pre + '_' + 'Is_weekend'] / TBC\n",
    "    1.2 : added target col / rename to target\n",
    "    1.1 : added holiday option\n",
    "    ver1.0: initial\n",
    "    \n",
    "    '''\n",
    "    import re\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    from datetime import datetime as dt\n",
    "\n",
    "    #import holidays\n",
    "    #holidays = holidays.UnitedStates(state=state)\n",
    "    df1 = df.copy()\n",
    "\n",
    "    fld = df1[fldname]\n",
    "    if not np.issubdtype(fld.dtype, np.datetime64):\n",
    "        df1[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n",
    "    targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "    for n in ('Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "              'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end',\n",
    "              'Is_year_start'):\n",
    "        df1[targ_pre + '_' + n] = getattr(fld.dt, n.lower())\n",
    "    #df1[targ_pre + '_' + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
    "\n",
    "    df1['months_old'] = df1[fldname].apply(lambda x: (dt.today().year - x.year) * 12 + (dt.today().month - x.month))\n",
    "\n",
    "    # add other features\n",
    "    # df1[targ_pre+'_'+'Is_holiday'] = df1[fldname].apply(lambda x: 1 if holidays.get(x) is not None else 0)\n",
    "#     df1[targ_pre + '_' + 'Is_holiday'] = df1.apply(\n",
    "#         lambda x: 1 if holidays.UnitedStates(state=x[state_col]).get(x[fldname]) is not None else 0, axis=1)\n",
    "    \n",
    "    df1[targ_pre + '_' + 'quarter'] = df1[fldname].dt.quarter\n",
    "    df1[targ_pre + '_' + 'semester'] = np.where(df1[targ_pre + '_' + 'quarter'].isin([1, 2]), 1, 2)\n",
    "    \n",
    "    # convert binary features\n",
    "    start_end_cols = [d for d in df1.columns if 'start' in d] + [d for d in df1.columns if 'end' in d]\n",
    "    bl_map = {True: 1, False: 0}\n",
    "\n",
    "    for col in start_end_cols:\n",
    "        df1[col] = df1[col].map(bl_map)\n",
    "\n",
    "    if drop_date: df1.drop(fldname, axis=1, inplace=True)\n",
    "    if drop_year: df1.drop(fldname + '_' + 'Year', axis=1, inplace=True)\n",
    "        \n",
    "    df1[targ_pre + '_' + 'Is_weekend'] = np.where(df1[targ_pre + '_' + 'Dayofweek'].isin([5, 6]), 1, 0) # shifted from top\n",
    "    return df1\n",
    "\n",
    "#1 data shampoo-sales.csv\n",
    "data_path='D:\\\\timeseries\\\\'\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(data_path+'shampoo-sales.csv')\n",
    "index_col = 'Month'\n",
    "target_col = 'Sales'\n",
    "\n",
    "fldname = index_col\n",
    "date_col_to_force = index_col\n",
    "#df_date = date_features(df, fldname, drop_date=True, drop_year=True, state_col='ca')\n",
    "\n",
    "#pd.to_datetime(date_col_to_force, errors = 'coerce')\n",
    "\n",
    "df[index_col] = df[index_col].apply(lambda x: pd.Period(x, freq='ms'))\n",
    "\n",
    "#df_date = date_features(df, fldname, drop_date=True, drop_year=True, state_col='ca')\n",
    "\n",
    "fld = df[fldname]\n",
    "#pd.to_datetime(fld, infer_datetime_format=True)\n",
    "\n",
    "#pd.to_datetime(fld, errors = 'coerce')\n",
    "\n",
    "#df[fldname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d31955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "BOMoffset = pd.tseries.offsets.MonthBegin()\n",
    "BOMoffset\n",
    "\n",
    "pd.Timestamp.min,  pd.Timestamp.max\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#create date range\n",
    "some_dates = ['1/1/2000', '1/1/2150', '1/1/2300']\n",
    "\n",
    "#convert date range to datetime and automatically coerce errors\n",
    "some_dates = pd.to_datetime(some_dates, errors = 'coerce')\n",
    "\n",
    "#show datetimes\n",
    "print(some_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8be8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_f = [x for x in df['Sales']]\n",
    "f1 = []\n",
    "for t in range(len(df1)):\n",
    "    \n",
    "    model = ARIMA(history_f, order = (0,0,0)) # AR\n",
    "    model = ARIMA(history_f, order = (0,0,0)) # MA\n",
    "    model = ARIMA(history_f,order = (0,0,0)) # ARMA\n",
    "    model = ARIMA(history_f, order = (0,1,0)) # ARIMA\n",
    "    #model = sm.tsa.statespace.SARIMAX(history_f,order = (0,1,0),seasonal_order = (0,1,0,12)) # SARIMA\n",
    "    model_fit = model.fit()\n",
    "    \n",
    "    output = model_fit.forecast()[0]\n",
    "    \n",
    "    history_f.append(output)\n",
    "    f1.append(output)\n",
    "\n",
    "df1['forecast'] = 0\n",
    "for i in range(len(f1)):\n",
    "    df1.iloc[i, 1] = f1[i]\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a241db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://cprosenjit.medium.com/10-time-series-forecasting-methods-we-should-know-291037d2e285\n",
    "\n",
    "data_path='D:\\\\timeseries\\\\'\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(data_path+'shampoo-sales.csv')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "total_data = dataset[\"Month\"].count()\n",
    "split = (total_data * 0.90).astype(np.int32)\n",
    "\n",
    "train = dataset[0:split] \n",
    "test = dataset[split:]\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(train.Month, train.Sales, label='Train')\n",
    "plt.plot(test.Month, test.Sales, label='Test')\n",
    "plt.xticks(dataset[\"Month\"], dataset[\"Month\"], rotation='vertical')\n",
    "plt.legend(loc='best')\n",
    "plt.title(\"Train Test Split\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('\\n #1. Nave Approach')\n",
    "'''\n",
    "1. Nave Approach\n",
    "This is one of the simplest methods. It says that the forecast for any period equals the last observed value.\n",
    "If the time series data contain seasonality, itll be better to take forecasts equal to the value from last season. \n",
    "'''\n",
    "predictions_nv = test.copy()\n",
    "# Copy the last observed Sales from training data\n",
    "predictions_nv[\"Predictions\"] = train.tail(1).iloc[0][\"Sales\"]\n",
    "print (predictions_nv)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rmse = sqrt(mean_squared_error(test[\"Sales\"], \\\n",
    "                               predictions_nv[\"Predictions\"]))\n",
    "print(\"Naive Mean Square Error (RMSE): %.3f\" % rmse)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(train.Month, train.Sales, label='Train')\n",
    "plt.plot(test.Month, test.Sales, label='Test')\n",
    "plt.plot(predictions_nv.Month, predictions_nv.Predictions, label='Prediction')\n",
    "plt.xticks(dataset[\"Month\"], dataset[\"Month\"], rotation='vertical')\n",
    "plt.legend(loc='best')\n",
    "plt.title(\"Predictions by Naive model\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('\\n #2 AR Model')\n",
    "'''\n",
    "2. AR (AutoRegressive)\n",
    "Autoregressive AR-X(p) model follows linear regression. It makes one prediction at a time and feed the output back\n",
    "into the model. Here, p specifies the order of the model e.g, AR(1) i.e. first-order Autoregression model.\n",
    "The output variable depends linearly on its previous values (called lags or orders) at previous time steps i.e.\n",
    "regression with self-values. The lag length must be specified when creating the model.\n",
    "'''\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "model_ag = AutoReg(endog = train[\"Sales\"], \\\n",
    "                   lags = 7, \\\n",
    "                   trend='c', \\\n",
    "                   seasonal = False, \\\n",
    "                   exog = None, \\\n",
    "                   hold_back = None, \\\n",
    "                   period = None, \\\n",
    "                   missing = 'none')\n",
    "# endog: dependent variable, response variable or y (endogenous)\n",
    "# exog: independent variable, explanatory variable or x (exogenous)\n",
    "# lags: the no. of lags to include in the model\n",
    "#       [1, 4] will only include lags 1 and 4 \n",
    "#       while lags=4 will include lags 1, 2, 3, and 4\n",
    "# trend: trend to include in the model\n",
    "#        {n, c, t, ct}\n",
    "#        n - No trend.\n",
    "#        c - Constant only.\n",
    "#        t - Time trend only.\n",
    "#        ct - Constant and time trend.\n",
    "# seasonal: whether to include seasonal dummies in the model\n",
    "fit_ag = model_ag.fit()\n",
    "print(\"Coefficients:\\n%s\" % fit_ag.params)\n",
    "\n",
    "predictions = fit_ag.predict(start=len(train), \\\n",
    "                             end=len(train)+len(test)-1, \\\n",
    "                             dynamic=False)\n",
    "predictions.name = \"Predictions\"\n",
    "result  = pd.concat([test, predictions], axis=1).reindex(test.index)\n",
    "print (result)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rmse = sqrt(mean_squared_error(test[\"Sales\"], predictions))\n",
    "print(\"AR - Root Mean Square Error (RMSE): %.3f\" % rmse)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(train.Month, train.Sales, label='Train')\n",
    "plt.plot(test.Month, test.Sales, label='Test')\n",
    "plt.plot(result.Month, result.Predictions, label='Prediction')\n",
    "plt.xticks(dataset[\"Month\"], dataset[\"Month\"], rotation='vertical')\n",
    "plt.legend(loc='best')\n",
    "plt.title(\"Predictions by AutoRegressive (AR) model\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('\\n 3. MA (Moving Average)')\n",
    "'''\n",
    "Moving Average process is an approach to model univariate time series. This is used to remove any seasonal trend in time series\n",
    "to allow us to see any trend in data. This is represented as MA(q) where q specifies the order of the model\n",
    "e.g., MA(2) i.e. second-order Moving Average model.\n",
    "'''\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "model_ma = ARIMA(endog = train[\"Sales\"], \\\n",
    "                   order=(0, 0, 2))\n",
    "# endog: dependent variable, response variable or y (endogenous)\n",
    "# order: order of the model for the autoregressive, \n",
    "#        differences & moving average components.\n",
    "fit_ma = model_ma.fit()\n",
    "print(\"Coefficients:\\n%s\" % fit_ma.params)\n",
    "\n",
    "predictions_ma = fit_ma.predict(start = len(train), \\\n",
    "                                end = len(train)+len(test)-1, \\\n",
    "                                dynamic = False)\n",
    "predictions_ma.name = \"Predictions\"\n",
    "result_ma  = pd.concat([test, predictions_ma], axis=1).reindex(test.index)\n",
    "print (result_ma)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rmse_ma = sqrt(mean_squared_error(test[\"Sales\"], predictions_ma))\n",
    "print(\"MA - Root Mean Square Error (RMSE): %.3f\" % rmse_ma)\n",
    "\n",
    "\n",
    "print('\\n 4. AutoRegressive Moving Average (ARMA)')\n",
    "'''\n",
    "ARMA process combines both AutoRegression (AR) and Moving Average (MA) models. This is usually referred to as the\n",
    "ARMA(p,q) model where p is the order of the AR part and q is the order of the MA part.\n",
    "'''\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "model_arma = ARIMA(endog = train[\"Sales\"], \\\n",
    "                   order = (2, 0, 1))\n",
    "# endog: dependent variable, response variable or y (endogenous)\n",
    "# order: order of the model for the autoregressive, \n",
    "#        differences & moving average components.\n",
    "fit_arma = model_arma.fit()\n",
    "print(\"Coefficients:\\n%s\" % fit_arma.params)\n",
    "\n",
    "predictions_arma = fit_arma.predict(start = len(train), \\\n",
    "                                end = len(train)+len(test)-1, \\\n",
    "                                dynamic = False)\n",
    "predictions_arma.name = \"Predictions\"\n",
    "result_arma  = pd.concat([test, predictions_arma], axis=1).reindex(test.index)\n",
    "print (result_arma)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rmse_arma = sqrt(mean_squared_error(test[\"Sales\"], predictions_arma))\n",
    "print(\"ARMA - Root Mean Square Error (RMSE): %.3f\" % rmse_arma)\n",
    "\n",
    "\n",
    "print('\\n 5. AutoRegressive Integrated Moving Average (ARIMA)')\n",
    "'''ARIMA model has three components:\n",
    "(i) Auto regressive component, AR(p) i.e. linear regression on its previous values or lags(p).\n",
    "(ii) Integrated component (I) indicates that the data have been replaced with the difference\n",
    "    between the current observation and the previous time step.\n",
    "(iii) Moving average, MA(q) i.e. consider moving average with order of q.\n",
    "This model is represented as ARIMA(p, d, q) where p, d and q specifies the order of the AR(p), I(d) and MA(q)\n",
    "        models respectively.'''\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "model_arima = ARIMA(endog = train[\"Sales\"], \\\n",
    "                   order = (1, 1, 1))\n",
    "# endog: dependent variable, response variable or y (endogenous)\n",
    "# order: order of the model for the autoregressive, \n",
    "#        differences & moving average components.\n",
    "fit_arima = model_arima.fit()\n",
    "print(\"Coefficients:\\n%s\" % fit_arima.params)\n",
    "\n",
    "predictions_arima = fit_arima.predict(start = len(train), \\\n",
    "                                end = len(train)+len(test)-1, \\\n",
    "                                dynamic = False)\n",
    "predictions_arima.name = \"Predictions\"\n",
    "result_arima  = pd.concat([test, predictions_arima], axis=1).reindex(test.index)\n",
    "print (result_arima)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rmse_arima = sqrt(mean_squared_error(test[\"Sales\"], predictions_arima))\n",
    "print(\"ARIMA - Root Mean Square Error (RMSE): %.3f\" % rmse_arima)\n",
    "\n",
    "\n",
    "print('\\n 6. Seasonal AutoRegressive Integrated Moving Average (SARIMA)')\n",
    "'''SARIMA extends the ARIMA model with the ability to perform the same AR, I, and MA modeling at the seasonal level.\n",
    "Seasonal ARIMA models are denoted as ARIMA(p,d,q)(P,D,Q)m, where m refers to the number of periods in each season and\n",
    "P, D, Q (uppercase) refer to the autoregressive, differencing, and moving average terms for the seasonal part of the\n",
    "ARIMA model respectively.'''\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "model_sarima = SARIMAX(endog = train[\"Sales\"], \\\n",
    "                        order = (1, 1, 1), \\\n",
    "                        seasonal_order=(0, 0, 0, 0))\n",
    "# endog: dependent variable, response variable or y (endogenous)\n",
    "# order: order of the model for the autoregressive, \n",
    "#        differences & moving average components.\n",
    "# seasonal_order: (P,D,Q,s) order of the seasonal component of \n",
    "#     the model for the AR parameters, differences, \n",
    "#     MA parameters, and periodicity. s is the periodicity \n",
    "#     (number of periods in season), often it is 4 for \n",
    "#     quarterly data or 12 for monthly data (default, no \n",
    "#     seasonal effect).\n",
    "fit_sarima = model_sarima.fit()\n",
    "print(\"Coefficients:\\n%s\" % fit_sarima.params)\n",
    "\n",
    "predictions_sarima = fit_sarima.predict(start = len(train), \\\n",
    "                                end = len(train)+len(test)-1, \\\n",
    "                                dynamic = False)\n",
    "predictions_sarima.name = \"Predictions\"\n",
    "result_sarima  = pd.concat([test, predictions_sarima], axis=1) \\\n",
    "                   .reindex(test.index)\n",
    "print (result_sarima)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rmse_sarima = sqrt(mean_squared_error(test[\"Sales\"], \\\n",
    "                                      predictions_sarima))\n",
    "print(\"SARIMA - Root Mean Square Error (RMSE): %.3f\" % rmse_sarima)\n",
    "\n",
    "\n",
    "print('\\n 7. Auto ARIMA')\n",
    "'''Automatically discover the optimal order for an ARIMA model. The auto-ARIMA process seeks to identify the most\n",
    "optimal parameters for an ARIMA model, settling on a single fitted ARIMA model. This process is based on the\n",
    "commonly-used R function, forecast::auto.arima\n",
    "Pmdarima wraps statsmodels under the hood. This can be installed from PyPI.'''\n",
    "\n",
    "from pmdarima.arima import auto_arima\n",
    "model_aarima = auto_arima (y = train[\"Sales\"], \\\n",
    "                           seasonal=False, \\\n",
    "                           stepwise=True)\n",
    "# seasonal : default=True, whether to fit \n",
    "#            a seasonal ARIMA.\n",
    "# stepwise : default=True, the auto_arima \n",
    "#            function has two modes: stepwise \n",
    "#            & parallelized (slower)\n",
    "\n",
    "predictions_aarima = model_aarima.predict(n_periods=test[\"Month\"].size, \\\n",
    "                                        X=None, \\\n",
    "                                        return_conf_int=False, \\\n",
    "                                        alpha=0.05)\n",
    "predictions_aarimaDf = pd.DataFrame({'Predictions': \\\n",
    "                                     predictions_aarima})\n",
    "result_aarima = pd.concat([test.reset_index(drop=True), \\\n",
    "                           predictions_aarimaDf], axis=1)\n",
    "print (result_aarima)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rmse_aarima = sqrt(mean_squared_error(test[\"Sales\"], \\\n",
    "                                      predictions_aarima))\n",
    "print(\"Auto ARIMA - Root Mean Square Error (RMSE): %.3f\" % rmse_aarima)\n",
    "\n",
    "\n",
    "print('\\n 8. Prophet')\n",
    "'''Prophet is a time series forecasting procedure developed by Facebook. This can handle the following scenarios:\n",
    "hourly, daily, or weekly observations with at least a few months (preferably a year) of history\n",
    "strong multiple human-scale seasonalities: day of week and time of year\n",
    "important holidays that occur at irregular intervals that are known in advance (e.g. the Super Bowl)\n",
    "a reasonable number of missing observations or large outliers\n",
    "historical trend changes, for instance due to product launches or logging changes\n",
    "trends that are non-linear growth curves, where a trend hits a natural limit or saturates\n",
    "The input to Prophet is always a dataframe with two columns: ds (datestamp, either YYYY-MM-DD or YYYY-MM-DD HH:MM:SS formats) and y (numeric, represents the measurement we wish to forecast).\n",
    "Prophet can be installed from CRAN and PyPI.'''\n",
    "\n",
    "from fbprophet import Prophet\n",
    "# instantiate the model and set parameters\n",
    "model_fb = Prophet( \\\n",
    "                   interval_width = 0.95, \\\n",
    "                   growth = \"linear\", \\\n",
    "                   daily_seasonality = False, \\\n",
    "                   weekly_seasonality = False, \\\n",
    "                   yearly_seasonality = False, \\\n",
    "                   seasonality_mode = \"multiplicative\"\n",
    ")\n",
    "train_fb = train.copy()\n",
    "train_fb.columns = [\"y\", \"ds\"]\n",
    "# fit the model to historical data\n",
    "model_fb.fit(train_fb)\n",
    "\n",
    "future_pd = model_fb.make_future_dataframe(\n",
    "    periods = 6,\n",
    "    freq = 'm',\n",
    "    include_history=True\n",
    ")\n",
    "# predict over the dataset\n",
    "predictions_fb = model_fb.predict(future_pd)\n",
    "\n",
    "predict_fig = model_fb.plot(predictions_fb, \\\n",
    "                            xlabel='Month', \\\n",
    "                            ylabel='Sales')\n",
    "\n",
    "\n",
    "print('\\n 9. XGBoost')\n",
    "'''\n",
    "XGBoost (Extreme Gradient Boosting) is an implementation of gradient boosting for classification and regression problems.\n",
    "This can be used for time series forecasting by restructuring the input dataset to look like a supervised learning problem.'''\n",
    "\n",
    "dataXGB = dataset.copy()\n",
    "# Restructure the data\n",
    "dataXGB[\"Target\"] = dataXGB.Sales.shift(-1)\n",
    "# Drop the last null column because of shifting\n",
    "dataXGB.dropna(inplace=True)\n",
    "# Extract features & labels\n",
    "X = dataXGB.iloc[:,0:1].values\n",
    "y = dataXGB.iloc[:, -1].values\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "      train_test_split(X, y, test_size = 0.12, \\\n",
    "                       random_state = 0, shuffle=False)\n",
    "import xgboost\n",
    "reg = xgboost.XGBRegressor(objective='reg:squarederror', \\\n",
    "                           n_estimators=1000)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "predictions_xgb = reg.predict(X_test)\n",
    "predictions_xgb = pd.DataFrame({'Predictions': \\\n",
    "                                     predictions_xgb})\n",
    "result_xgb = pd.concat( \\\n",
    "                       [dataXGB.tail(len(X_test)) \\\n",
    "                               .reset_index(drop=True), \\\n",
    "                        predictions_xgb], axis=1)\n",
    "print (result_xgb)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rmse_xgb = sqrt(mean_squared_error(y_test, predictions_xgb))\n",
    "print(\"XGBoost - Root Mean Square Error (RMSE): %.3f\" % rmse_xgb)\n",
    "\n",
    "\n",
    "print('#\\n 9. TBATS')\n",
    "'''\n",
    "BATS and TBATS are time series forecasting algorithms and work well with multiple seasonal periods.\n",
    "TBATS is preferred where seasonality is complex. TBATS is the acronym for:\n",
    "Trigonometric seasonality\n",
    "Box-Cox transformation\n",
    "ARMA error\n",
    "Trend\n",
    "Seasonal components\n",
    "'''\n",
    "\n",
    "from tbats import TBATS\n",
    "# /databricks/python/bin/pip install tbats==1.1.0\n",
    "model_tbats = TBATS(seasonal_periods=(12, 28),\\\n",
    "              use_arma_errors=False,\\\n",
    "              use_box_cox=False,\\\n",
    "              n_jobs=1,\\\n",
    "              use_trend=None,\\\n",
    "              use_damped_trend=None)\\\n",
    "          .fit(train.Sales)\n",
    "\n",
    "predictions_tbats = model_tbats.forecast(steps=4)\n",
    "predictions_tbatsDF = pd.DataFrame()\n",
    "predictions_tbatsDF[\"Predictions\"] = predictions_tbats.tolist()\n",
    "result_tbats = pd.concat([test.reset_index(drop=True), \\\n",
    "                           predictions_tbatsDF], axis=1)\n",
    "print (result_tbats)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rmse_tbats = sqrt(mean_squared_error(test[\"Sales\"], \\\n",
    "                                      predictions_tbatsDF))\n",
    "print(\"TBATS - Root Mean Square Error (RMSE): %.3f\" % rmse_tbats)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(train.Month, train.Sales, label='Train')\n",
    "plt.plot(test.Month, test.Sales, label='Test')\n",
    "plt.plot(test.Month, predictions_tbatsDF.Predictions, label='Prediction')\n",
    "plt.xticks(dataset[\"Month\"], dataset[\"Month\"], rotation='vertical')\n",
    "plt.legend(loc='best')\n",
    "plt.title(\"Predictions by TBATS model\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('\\n 10. ETS')\n",
    "'''ETS (Error, Trend, Seasonality) are exponential smoothing state space models (a general family of forecasting models)\n",
    "for univariate time series analysis. Unlike the simple moving average where the past observations are weighted equally,\n",
    "exponential functions are used in ETS models to assign exponentially decreasing weights over time.\n",
    "In this blog well use statsmodels library for python implementation. statsmodels implements all combinations of additive &\n",
    "multiplicative error model, additive & multiplicative trend, possibly dampened  additive & multiplicative seasonality.\n",
    "Refer here for the available models. Here, well use the Holt-Winters seasonal method which can incorporate a trend and\n",
    "a seasonal component.'''\n",
    "\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "model_es = ExponentialSmoothing(endog=train[\"Sales\"], \\\n",
    "                                trend='add', \\\n",
    "                                seasonal='add', \\\n",
    "                                seasonal_periods=12, \\\n",
    "                                damped=True)\n",
    "# Parameters\n",
    "# endog : The time series to model.\n",
    "# trend : Type of trend component (optional); \n",
    "#         options: \"add\", \"mul\", \"additive\", \n",
    "#                  \"multiplicative\", None\n",
    "# seasonal : Type of seasonal component (optional); \n",
    "#         options: \"add\", \"mul\", \"additive\", \n",
    "#                  \"multiplicative\", None\n",
    "# damped_trend : Should the trend component \n",
    "#                be damped (optional)\n",
    "\n",
    "fit_es = model_es.fit(optimized=True, \\\n",
    "                      use_boxcox=False, \\\n",
    "                      remove_bias=False)\n",
    "predictions_es = fit_es.predict(start=test.index[0], \\\n",
    "                                end=test.index[-1])\n",
    "predictions_es.name = \"Predictions\"\n",
    "result_es  = pd.concat([test, predictions_es], axis=1) \\\n",
    "                   .reindex(test.index)\n",
    "print (result_es)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rmse_es = sqrt(mean_squared_error(test[\"Sales\"], \\\n",
    "                                      predictions_es))\n",
    "print(\"Exponential Smoothing - Root Mean Square Error (RMSE): %.3f\" % rmse_es)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a1b5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_arima_model(train, test, order, maxlags=8, ic='aic'):\n",
    "    # feature Scaling\n",
    "    stdsc = StandardScaler()\n",
    "    train_std = stdsc.fit_transform(train.values.reshape(-1, 1))\n",
    "    test_std = stdsc.transform(test.values.reshape(-1, 1))\n",
    "    # prepare training dataset\n",
    "    history = [x for x in train_std]\n",
    "    # make predictions\n",
    "    predictions = list()\n",
    "    # rolling forecasts\n",
    "    for t in range(len(test_std)):\n",
    "        # predict\n",
    "        model = ARIMA(history, order=order)\n",
    "        model_fit = model.fit(maxlags=maxlags, ic=ic, disp=0)\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        # invert transformed prediction\n",
    "        predictions.append(yhat)\n",
    "        # observation\n",
    "        history.append(test_std[t])\n",
    "    # inverse transform\n",
    "    predictions = stdsc.inverse_transform(np.array(predictions).reshape((-1)))\n",
    "    # calculate mse\n",
    "    mse = mean_squared_error(test, predictions)\n",
    "    return predictions, mse\n",
    "\n",
    "def evaluate_arima_models(train, test, p_values, d_values, q_values):\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    pdq = list(itertools.product(p_values, d_values, q_values))\n",
    "    for order in pdq:\n",
    "        try:\n",
    "            predictions, mse = evaluate_arima_model(train, test, order)\n",
    "            if mse < best_score:\n",
    "                best_score, best_cfg = mse, order\n",
    "            print('Model(%s) mse=%.3f' % (order,mse))\n",
    "        except:\n",
    "            continue\n",
    "    print('Best Model(%s) mse=%.3f' % (best_cfg, best_score)) \n",
    "    return best_cfg\n",
    "\n",
    "def predict_arima_model(train, period, order, maxlags=8, ic='aic'):\n",
    "    # Feature Scaling\n",
    "    stdsc = StandardScaler()\n",
    "    train_std = stdsc.fit_transform(train.values.reshape(-1, 1))\n",
    "    # fit model\n",
    "    model = ARIMA(train_std, order=order)\n",
    "    model_fit = model.fit(maxlags=maxlags, ic=ic, disp=0)\n",
    "    # make prediction\n",
    "    yhat = model_fit.predict(len(train), len(train) + period -1, typ='levels')\n",
    "    # inverse transform\n",
    "    yhat = stdsc.inverse_transform(np.array(yhat).flatten())\n",
    "    return yhat\n",
    "\n",
    "model_name='AR Model'\n",
    "# evaluate parameters\n",
    "p_values = range(1, 4)\n",
    "d_values = [0]\n",
    "q_values = [0]\n",
    "\n",
    "model_name='MA Model'\n",
    "# evaluate parameters\n",
    "p_values = [0]\n",
    "d_values = [0]\n",
    "q_values = range(1, 4)\n",
    "\n",
    "model_name='ARMA Model'\n",
    "# evaluate parameters\n",
    "p_values = range(0, 1, 2)\n",
    "d_values = [0]\n",
    "q_values = range(0, 1, 2)\n",
    "\n",
    "model_name='ARIMA Model'\n",
    "# evaluate parameters\n",
    "p_values = [1, 2, 4, 6, 8, 10]\n",
    "d_values = range(0, 3)\n",
    "q_values = range(1, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d4f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ts310]",
   "language": "python",
   "name": "conda-env-ts310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
