{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0be6d3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\spacy\\util.py:895: UserWarning: [W094] Model 'en_core_web_md' (2.0.0) specifies an under-constrained spaCy version requirement: >=2.0.0. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the \"spacy_version\" in your meta.json to a version range, with a lower and upper pin. For example: >=3.5.2,<3.6.0\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CoherenceModel\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#1 Collecting the data\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m reports \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://github.com/sg-tarek/Python/raw/main/cordis-h2020reports.gz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m reports\u001b[38;5;241m.\u001b[39mhead()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#df = pd.read_excel(data_path+'User_Journey_11.xlsx', sheet_name ='RAW_Data')\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#2 Preprocess the data\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Our spaCy model:\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\common.py:713\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    710\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    712\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[1;32m--> 713\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    721\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    722\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\common.py:363\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[0;32m    362\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[0;32m    364\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    366\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\common.py:265\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\nlp397\\lib\\urllib\\request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\nlp397\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\nlp397\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\nlp397\\lib\\urllib\\request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    560\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\nlp397\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\nlp397\\lib\\urllib\\request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "#https://towardsdatascience.com/topic-modelling-in-python-with-spacy-and-gensim-dc8f7748bdbf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#sns.set()\n",
    "import spacy\n",
    "\n",
    "#import pyLDAvis.gensim_models #issue in installation pip install past\n",
    "#pyLDAvis.enable_notebook()# Visualise inside a notebook\n",
    "\n",
    "import en_core_web_md\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#1 Collecting the data\n",
    "#reports = pd.read_csv('https://github.com/sg-tarek/Python/raw/main/cordis-h2020reports.gz')\n",
    "reports.head()\n",
    "\n",
    "#df = pd.read_excel(data_path+'User_Journey_11.xlsx', sheet_name ='RAW_Data')\n",
    "\n",
    "#2 Preprocess the data\n",
    "# Our spaCy model:\n",
    "nlp = en_core_web_md.load()\n",
    "# Tags I want to remove from the text\n",
    "removal= ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE', 'NUM', 'SYM']\n",
    "tokens = []\n",
    "for summary in nlp.pipe(reports['summary']):\n",
    "    proj_tok = [token.lemma_.lower() for token in summary if token.pos_ not in removal and not token.is_stop and token.is_alpha]\n",
    "    tokens.append(proj_tok)\n",
    "    \n",
    "reports['tokens'] = tokens\n",
    "\n",
    "#3 Create dictionary and corpus \n",
    "#Dictionary: The idea of the dictionary is to give each token a unique ID.\n",
    "#Corpus: Having assigned a unique ID to each token, the corpus simply contains each ID and its frequency \n",
    "\n",
    "# I will apply the Dictionary Object from Gensim, which maps each word to their unique ID:\n",
    "dictionary = Dictionary(reports['tokens'])\n",
    "print('dictionary:', dictionary.token2id)\n",
    "\n",
    "#filter out low-frequency and high-frequency tokens, also limit the vocabulary to a max of 1000 words:\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)\n",
    "\n",
    "#4 Model building\n",
    "# LdaMulticore, uses all CPU cores to parallelize and speed up model training.\n",
    "lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=50, num_topics=10, workers = 4, passes=10)\n",
    "#lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics = 5, id2word = dic, passes = 10, workers = 2)\n",
    "#if does not work LdaMulticore then use an equivalent single-core model LdaModel\n",
    "#gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, iterations=50, num_topics=10, workers = 4, passes=10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21fdfd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ajverma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ajverma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ajverma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ajverma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'extend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m data \u001b[38;5;241m=\u001b[39m df1[text_col]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     32\u001b[0m new_stopwords_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msir\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwelcome\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlisten\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 33\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[43mtext_preprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_stopwords_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m corpus\n",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m, in \u001b[0;36mtext_preprocessing\u001b[1;34m(data, new_stopwords_list)\u001b[0m\n\u001b[0;32m      8\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124momw-1.4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m stopwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(nltk\u001b[38;5;241m.\u001b[39mcorpus\u001b[38;5;241m.\u001b[39mstopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 11\u001b[0m \u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m(\u001b[38;5;28mset\u001b[39m(new_stopwords_list))\n\u001b[0;32m     13\u001b[0m corpus\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     15\u001b[0m lem \u001b[38;5;241m=\u001b[39m WordNetLemmatizer() \u001b[38;5;66;03m# For Lemmatization\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'set' object has no attribute 'extend'"
     ]
    }
   ],
   "source": [
    "def text_preprocessing(data, new_stopwords_list):\n",
    "    \n",
    "    import nltk\n",
    "    #from nltk.stem import *\n",
    "    nltk.download('punkt') # For Stemming\n",
    "    nltk.download('wordnet') # For Lemmatization\n",
    "    nltk.download('stopwords') # For Stopword Removal\n",
    "    nltk.download('omw-1.4')\n",
    "\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    stopwords.extend(set(new_stopwords_list))\n",
    "    \n",
    "    corpus=[]\n",
    "    \n",
    "    lem = WordNetLemmatizer() # For Lemmatization\n",
    "    for news in data:\n",
    "        words=[w for w in nltk.tokenize.word_tokenize(news) if (w not in stopwords)] # word_tokenize function tokenizes text on each word by default\n",
    "        words=[lem.lemmatize(w) for w in words if len(w)>2]\n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "data_path='D:\\\\projects\\\\CAI\\\\'\n",
    "df = pd.read_excel(data_path+'User_Journey_11.xlsx', sheet_name ='RAW_Data')\n",
    "\n",
    "text_col = 'utterance'\n",
    "\n",
    "df1 = df.loc[df.utterance.notnull()][[text_col]]\n",
    "df1[text_col] = df1[text_col].apply(lambda x: str(x))\n",
    "\n",
    "#df1= df1.head(10)\n",
    "data = df1[text_col].values.tolist()\n",
    "new_stopwords_list = ['sir', 'welcome', 'please', 'hello', 'listen', 'name']\n",
    "corpus = text_preprocessing(data, new_stopwords_list)\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4595817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328\n",
      "328\n",
      "328\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Print the set of spaCy's default stop words (remember that sets are unordered):\n",
    "#print(nlp.Defaults.stop_words)\n",
    "\n",
    "print(len(nlp.Defaults.stop_words))\n",
    "\n",
    "# Make list of word you want to add to stop words\n",
    "list = [\"welcome\",\"hello\"]\n",
    "\n",
    "# Iterate this in loop\n",
    "\n",
    "for item in list:\n",
    "    # Add the word to the set of stop words. Use lowercase!\n",
    "    nlp.Defaults.stop_words.add(item)\n",
    "    print(len(nlp.Defaults.stop_words))\n",
    "    \n",
    "    # Set the stop_word tag on the lexeme\n",
    "    nlp.vocab[item].is_stop = True\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3007e8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ajverma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ajverma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ajverma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ajverma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps?:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS*\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,text)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#df.text = df.text.apply(remove_url)\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m df1\u001b[38;5;241m.\u001b[39mutterance \u001b[38;5;241m=\u001b[39m \u001b[43mdf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutterance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#2 Remove mentions and hashtags\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\apply.py:1105\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\apply.py:1156\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1154\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1155\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1156\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1163\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[2], line 27\u001b[0m, in \u001b[0;36mremove_url\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_url\u001b[39m(text):\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps?:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mS*\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\nlp397\\lib\\re.py:210\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path='D:\\\\projects\\\\CAI\\\\'\n",
    "#df = pd.read_excel(data_path+'User_Journey.xlsx', sheet_name ='ChatRawData')\n",
    "df = pd.read_excel(data_path+'User_Journey_11.xlsx', sheet_name ='RAW_Data')\n",
    "\n",
    "\n",
    "df1 = df.loc[df.utterance.notnull()][['utterance']]\n",
    "df1.head()\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import *\n",
    "nltk.download('punkt') # For Stemming\n",
    "nltk.download('wordnet') # For Lemmatization\n",
    "nltk.download('stopwords') # For Stopword Removal\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "text = 'utterance'\n",
    "#1 Remove URLs\n",
    "import re # Import regular expression package\n",
    "def remove_url(text):\n",
    "    return re.sub(r'https?:\\S*','',text)\n",
    "\n",
    "#df.text = df.text.apply(remove_url)\n",
    "#df1.utterance = df1.utterance.apply(remove_url)\n",
    "\n",
    "#2 Remove mentions and hashtags\n",
    "import re\n",
    "def remove_mentions_and_tags(text):\n",
    "    text = re.sub(r'@\\S*','',text)\n",
    "    return re.sub(r'#\\S*','',text)\n",
    "\n",
    "#3 preprocessing\n",
    "def text_preprocessing(data):\n",
    "    corpus=[]\n",
    "    \n",
    "    lem = WordNetLemmatizer() # For Lemmatization\n",
    "    for news in data:\n",
    "        words=[w for w in nltk.tokenize.word_tokenize(news) if (w not in stopwords)] # word_tokenize function tokenizes text on each word by default\n",
    "        words=[lem.lemmatize(w) for w in words if len(w)>2]\n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "#df1= df1.head(10)\n",
    "data = df1['utterance'].values.tolist()\n",
    "corpus = text_preprocessing(data)\n",
    "\n",
    "import gensim\n",
    "# Transform to gensim dictionary - \n",
    "#4 Creating the term dictionary of our courpus, where every unique term is assigned an index.\n",
    "dic = gensim.corpora.Dictionary(corpus)\n",
    "\n",
    "#5 Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "bow_corpus = [dic.doc2bow(doc) for doc in corpus]\n",
    "import pickle # Useful for storing big datasets\n",
    "pickle.dump(bow_corpus, open('corpus.pkl', 'wb'))\n",
    "dic.save('dictionary.gensim')\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus,\n",
    "                                   num_topics = 5,\n",
    "                                    id2word = dic,\n",
    "                                      passes = 10,\n",
    "                                      workers = 2)\n",
    "lda_model.save('model4.gensim')\n",
    "\n",
    "# We print words occuring in each of the topics as we iterate through them\n",
    "for idx, topic in lda_model.print_topics(num_words=10):\n",
    "    #print(f'idx:{idx}, topic:{topic}, type:{type(topic)}')\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f98419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://raw.githubusercontent.com/pycaret/pycaret/master/datasets/kiva.csv\n",
    "    \n",
    "df = pd.read_csv('https://raw.githubusercontent.com/pycaret/pycaret/master/datasets/kiva.csv')\n",
    "df1 = df[['en']]\n",
    "df1\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import *\n",
    "nltk.download('punkt') # For Stemming\n",
    "nltk.download('wordnet') # For Lemmatization\n",
    "nltk.download('stopwords') # For Stopword Removal\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Remove URLs\n",
    "import re # Import regular expression package\n",
    "def remove_url(text):\n",
    "    return re.sub(r'https?:\\S*','',text)\n",
    "\n",
    "#df.text = df.text.apply(remove_url)\n",
    "df1.en = df1.en.apply(remove_url)\n",
    "\n",
    "# Remove mentions and hashtags\n",
    "import re\n",
    "def remove_mentions_and_tags(text):\n",
    "    text = re.sub(r'@\\S*','',text)\n",
    "    return re.sub(r'#\\S*','',text)\n",
    "\n",
    "def text_preprocessing(data):\n",
    "    corpus=[]\n",
    "    \n",
    "    lem = WordNetLemmatizer() # For Lemmatization\n",
    "    for news in data:\n",
    "        words=[w for w in nltk.tokenize.word_tokenize(news) if (w not in stopwords)] # word_tokenize function tokenizes text on each word by default\n",
    "        words=[lem.lemmatize(w) for w in words if len(w)>2]\n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "#df1= df1.head(10)\n",
    "data = df1['en'].values.tolist()\n",
    "corpus = text_preprocessing(data)\n",
    "\n",
    "import gensim\n",
    "# Transform to gensim dictionary\n",
    "dic = gensim.corpora.Dictionary(corpus) \n",
    "bow_corpus = [dic.doc2bow(doc) for doc in corpus]\n",
    "import pickle # Useful for storing big datasets\n",
    "pickle.dump(bow_corpus, open('corpus.pkl', 'wb'))\n",
    "dic.save('dictionary.gensim')\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus,\n",
    "                                   num_topics = 5,\n",
    "                                    id2word = dic,\n",
    "                                      passes = 10,\n",
    "                                      workers = 2)\n",
    "lda_model.save('model4.gensim')\n",
    "\n",
    "# We print words occuring in each of the topics as we iterate through them\n",
    "for idx, topic in lda_model.print_topics(num_words=10):\n",
    "    #print(f'idx:{idx}, topic:{topic}, type:{type(topic)}')\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69f1e9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ajverma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ajverma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ajverma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ajverma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.083*\"need\" + 0.083*\"password\" + 0.083*\"login\" + 0.083*\"access\" + 0.083*\"issue\" + 0.083*\"always\" + 0.083*\"alwyas\" + 0.083*\"system\" + 0.083*\"working\" + 0.083*\"issus\"\n",
      "Topic: 1 \n",
      "Words: 0.083*\"password\" + 0.083*\"login\" + 0.083*\"access\" + 0.083*\"need\" + 0.083*\"issue\" + 0.083*\"always\" + 0.083*\"detail\" + 0.083*\"alwyas\" + 0.083*\"issus\" + 0.083*\"system\"\n",
      "Topic: 2 \n",
      "Words: 0.083*\"password\" + 0.083*\"access\" + 0.083*\"need\" + 0.083*\"issue\" + 0.083*\"login\" + 0.083*\"always\" + 0.083*\"alwyas\" + 0.083*\"system\" + 0.083*\"problem\" + 0.083*\"issus\"\n",
      "Topic: 3 \n",
      "Words: 0.083*\"password\" + 0.083*\"issue\" + 0.083*\"login\" + 0.083*\"access\" + 0.083*\"need\" + 0.083*\"always\" + 0.083*\"detail\" + 0.083*\"alwyas\" + 0.083*\"issus\" + 0.083*\"system\"\n",
      "Topic: 4 \n",
      "Words: 0.292*\"login\" + 0.153*\"always\" + 0.153*\"problem\" + 0.153*\"detail\" + 0.153*\"need\" + 0.014*\"password\" + 0.014*\"access\" + 0.014*\"issue\" + 0.014*\"issus\" + 0.014*\"system\"\n",
      "Topic: 5 \n",
      "Words: 0.339*\"password\" + 0.177*\"alwyas\" + 0.177*\"working\" + 0.177*\"issus\" + 0.016*\"need\" + 0.016*\"login\" + 0.016*\"issue\" + 0.016*\"access\" + 0.016*\"detail\" + 0.016*\"system\"\n",
      "Topic: 6 \n",
      "Words: 0.339*\"access\" + 0.177*\"need\" + 0.177*\"system\" + 0.177*\"issue\" + 0.016*\"password\" + 0.016*\"login\" + 0.016*\"always\" + 0.016*\"alwyas\" + 0.016*\"problem\" + 0.016*\"issus\"\n",
      "Topic: 7 \n",
      "Words: 0.083*\"login\" + 0.083*\"access\" + 0.083*\"need\" + 0.083*\"issue\" + 0.083*\"password\" + 0.083*\"always\" + 0.083*\"detail\" + 0.083*\"alwyas\" + 0.083*\"issus\" + 0.083*\"system\"\n",
      "Topic: 8 \n",
      "Words: 0.083*\"password\" + 0.083*\"login\" + 0.083*\"access\" + 0.083*\"need\" + 0.083*\"always\" + 0.083*\"issue\" + 0.083*\"detail\" + 0.083*\"alwyas\" + 0.083*\"issus\" + 0.083*\"system\"\n",
      "Topic: 9 \n",
      "Words: 0.083*\"need\" + 0.083*\"password\" + 0.083*\"access\" + 0.083*\"login\" + 0.083*\"always\" + 0.083*\"issue\" + 0.083*\"detail\" + 0.083*\"alwyas\" + 0.083*\"issus\" + 0.083*\"system\"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import *\n",
    "nltk.download('punkt') # For Stemming\n",
    "nltk.download('wordnet') # For Lemmatization\n",
    "nltk.download('stopwords') # For Stopword Removal\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Remove URLs\n",
    "import re # Import regular expression package\n",
    "def remove_url(text):\n",
    "    return re.sub(r'https?:\\S*','',text)\n",
    "#df.text = df.text.apply(remove_url)\n",
    "\n",
    "# Remove mentions and hashtags\n",
    "import re\n",
    "def remove_mentions_and_tags(text):\n",
    "    text = re.sub(r'@\\S*','',text)\n",
    "    return re.sub(r'#\\S*','',text)\n",
    "\n",
    "def text_preprocessing(data):\n",
    "    corpus=[]\n",
    "    \n",
    "    lem = WordNetLemmatizer() # For Lemmatization\n",
    "    for news in data:\n",
    "        words=[w for w in nltk.tokenize.word_tokenize(news) if (w not in stopwords)] # word_tokenize function tokenizes text on each word by default\n",
    "        words=[lem.lemmatize(w) for w in words if len(w)>2]\n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "doc1 = 'password is not working. password alwyas has issus'\n",
    "doc2 = 'access has issue, i need my system access'\n",
    "doc3 = 'login is always problem, i need login details'\n",
    "documents = [doc1, doc2, doc3]\n",
    "\n",
    "data = documents\n",
    "corpus = text_preprocessing(data)\n",
    "\n",
    "import gensim\n",
    "# Transform to gensim dictionary\n",
    "dic = gensim.corpora.Dictionary(corpus) \n",
    "bow_corpus = [dic.doc2bow(doc) for doc in corpus]\n",
    "import pickle # Useful for storing big datasets\n",
    "pickle.dump(bow_corpus, open('corpus.pkl', 'wb'))\n",
    "dic.save('dictionary.gensim')\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus,\n",
    "                                   num_topics = 10,\n",
    "                                    id2word = dic,\n",
    "                                      passes = 10,\n",
    "                                      workers = 2)\n",
    "lda_model.save('model4.gensim')\n",
    "\n",
    "# We print words occuring in each of the topics as we iterate through them\n",
    "for idx, topic in lda_model.print_topics(num_words=10):\n",
    "    #print(f'idx:{idx}, topic:{topic}, type:{type(topic)}')\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5a7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#lda_model.get_topic_terms()\n",
    "lda_model.get_term_topics(5), lda_model.get_topic_terms(5)\n",
    "\n",
    "dir(lda_model)\n",
    "\n",
    "lda_model.show_topics(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a84f458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/let-us-extract-some-topics-from-text-data-part-i-latent-dirichlet-allocation-lda-e335ee3e5fa4\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import *\n",
    "nltk.download('punkt') # For Stemming\n",
    "nltk.download('wordnet') # For Lemmatization\n",
    "nltk.download('stopwords') # For Stopword Removal\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "fetch20newsgroups = fetch_20newsgroups(subset='train')\n",
    "\n",
    "# Store in a pandas dataframe\n",
    "df = pd.DataFrame(fetch20newsgroups.data, columns=['text'])\n",
    "\n",
    "# Remove URLs\n",
    "import re # Import regular expression package\n",
    "def remove_url(text):\n",
    "    return re.sub(r'https?:\\S*','',text)\n",
    "df.text = df.text.apply(remove_url)\n",
    "\n",
    "# Remove mentions and hashtags\n",
    "import re\n",
    "def remove_mentions_and_tags(text):\n",
    "    text = re.sub(r'@\\S*','',text)\n",
    "    return re.sub(r'#\\S*','',text)\n",
    "\n",
    "df.text = df.text.apply(remove_mentions_and_tags)\n",
    "\n",
    "#Pre-Processing\n",
    "# def text_preprocessing(df):\n",
    "#     corpus=[]\n",
    "    \n",
    "#     lem = WordNetLemmatizer() # For Lemmatization\n",
    "#     for news in df['text']:\n",
    "#         words=[w for w in nltk.tokenize.word_tokenize(news) if (w not in stopwords)] # word_tokenize function tokenizes text on each word by default\n",
    "#         words=[lem.lemmatize(w) for w in words if len(w)>2]\n",
    "#         corpus.append(words)\n",
    "#     return corpus\n",
    "# # Apply this function on our data frame\n",
    "# corpus = text_preprocessing(df)\n",
    "# corpus\n",
    "\n",
    "def text_preprocessing1(data):\n",
    "    corpus=[]\n",
    "    \n",
    "    lem = WordNetLemmatizer() # For Lemmatization\n",
    "    for news in data:\n",
    "        words=[w for w in nltk.tokenize.word_tokenize(news) if (w not in stopwords)] # word_tokenize function tokenizes text on each word by default\n",
    "        words=[lem.lemmatize(w) for w in words if len(w)>2]\n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "doc1 = 'password is not working. password alwyas has issus'\n",
    "doc2 = 'access has issue, i need my system access'\n",
    "doc3 = 'login is always problem, i need login details'\n",
    "documents = [doc1, doc2, doc3]\n",
    "\n",
    "# documents = ''' password is not working. password alwyas has issus.\n",
    "# access has issue. i need my system access.\n",
    "# login is always problem. i need login details\n",
    "# '''\n",
    "data = documents\n",
    "corpus = text_preprocessing1(data)\n",
    "\n",
    "import gensim\n",
    "# Transform to gensim dictionary\n",
    "dic = gensim.corpora.Dictionary(corpus) \n",
    "bow_corpus = [dic.doc2bow(doc) for doc in corpus]\n",
    "import pickle # Useful for storing big datasets\n",
    "pickle.dump(bow_corpus, open('corpus.pkl', 'wb'))\n",
    "dic.save('dictionary.gensim')\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus,\n",
    "                                   num_topics = 4,\n",
    "                                    id2word = dic,\n",
    "                                      passes = 10,\n",
    "                                      workers = 2)\n",
    "lda_model.save('model4.gensim')\n",
    "\n",
    "# We print words occuring in each of the topics as we iterate through them\n",
    "for idx, topic in lda_model.print_topics(num_words=10):    \n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410591ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch20newsgroups\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05\n",
    "\n",
    "#LSA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "#documents = [\"doc1.txt\", \"doc2.txt\", \"doc3.txt\"] \n",
    "\n",
    "doc1 = 'password is not working. password alwyas has issus'\n",
    "doc2 = 'access has issue, i need my system access'\n",
    "doc3 = 'login is always problem, i need login details'\n",
    "documents = [doc1, doc2, doc3]\n",
    "  \n",
    "# raw documents to tf-idf matrix: \n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             use_idf=True, \n",
    "                             smooth_idf=True)\n",
    "# SVD to reduce dimensionality: \n",
    "svd_model = TruncatedSVD(n_components=4,   #num dimensions\n",
    "                         algorithm='randomized',\n",
    "                         n_iter=10)\n",
    "\n",
    "# pipeline of tf-idf + SVD, fit to and applied to documents:\n",
    "svd_transformer = Pipeline([('tfidf', vectorizer), \n",
    "                            ('svd', svd_model)])\n",
    "svd_matrix = svd_transformer.fit_transform(documents)\n",
    "print('svd_matrix:', svd_matrix)\n",
    "\n",
    "#LDA\n",
    "#from gensim.corpora.dictionary import load_from_text, doc2bow\n",
    "from gensim.corpora.dictionary import Dictionary \n",
    "from gensim.corpora import MmCorpus\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "document = \"This is some document...\"\n",
    "# load id->word mapping (the dictionary)\n",
    "id2word = Dictionary.load_from_text('wiki_en_wordids.txt')\n",
    "# load corpus iterator\n",
    "mm = MmCorpus('wiki_en_tfidf.mm')\n",
    "# extract 100 LDA topics, updating once every 10,000\n",
    "lda = LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=1, chunksize=10000, passes=1)\n",
    "# use LDA model: transform new doc to bag-of-words, then apply lda\n",
    "doc_bow = Dictionary.doc2bow(document.split())\n",
    "doc_lda = lda[doc_bow]\n",
    "# doc_lda is vector of length num_topics representing weighted presence of each topic in the doc\n",
    "print('LDA:', doc_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefd5a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2023/03/detect-cyberbullying-using-topic-modeling-and-sentiment-analysis/\n",
    "\n",
    "#1 define pre-processing function to model topics based on annotation\n",
    "def preprocess_topic(df, topic):\n",
    "    \"\"\" Preprocessing function to model text data based on give topics.\n",
    "    \n",
    "    args:\n",
    "    df = input dataframe\n",
    "    topic = input topic \"none\", \"sexism\", or \"racism\"\n",
    "    \n",
    "    returns:\n",
    "    corpus of words under given topic\n",
    "    \"\"\"\n",
    "    corpus=[]\n",
    "    # topic wise division\n",
    "    if topic == 'none':\n",
    "        for doc in ndf[ndf['Annotation'] == 'none']['cleaned_text']:\n",
    "            stop_word_removal = remove_stowords(doc)\n",
    "            lemmmatized_sample = lemma_clean_text(stop_word_removal)\n",
    "            words = lemmmatized_sample.split()\n",
    "            corpus.append(words)\n",
    "            \n",
    "    elif topic == 'sexism':\n",
    "        for doc in ndf[ndf['Annotation'] == 'sexism']['cleaned_text']:\n",
    "            stop_word_removal = remove_stowords(doc)\n",
    "            lemmmatized_sample = lemma_clean_text(stop_word_removal)\n",
    "            words = lemmmatized_sample.split()\n",
    "            corpus.append(words)\n",
    "                \n",
    "    elif topic == 'racism':\n",
    "        for doc in ndf[ndf['Annotation'] == 'racism']['cleaned_text']:\n",
    "            stop_word_removal = remove_stowords(doc)\n",
    "            lemmmatized_sample = lemma_clean_text(stop_word_removal)\n",
    "            words = lemmmatized_sample.split()\n",
    "            corpus.append(words) \n",
    "            \n",
    "    return corpus\n",
    "\n",
    "#2 corpus of the words\n",
    "corpus = preprocess_topic(ndf, 'sexism')\n",
    "\n",
    "# creat BOW model from corpus\n",
    "dic=gensim.corpora.Dictionary(corpus)\n",
    "bow_corpus = [dic.doc2bow(doc) for doc in corpus]\n",
    "\n",
    "# create LDA model using gensim library\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus,\n",
    "                                   num_topics = 4,\n",
    "                                   id2word = dic,\n",
    "                                   passes = 10,\n",
    "                                   workers = 2)\n",
    "\n",
    "lda_model.show_topics()\n",
    "\n",
    "#3 Visual Interpretation of Topic Modeling Output\n",
    "def plot_lda_vis(lda_model, bow_corpus, dic):\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dic)\n",
    "    return vis\n",
    "\n",
    "plot_lda_vis(lda_model, bow_corpus, dic)\n",
    "\n",
    "#4 Calculating the Coherence Metric of the Model\n",
    "#  assessing coherenece metric of the model\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "topics = [['prophet', 'slavery', 'violence', 'fear'],\n",
    "           ['people', 'religion', 'slave', 'hate', 'like'],\n",
    "           ['like', 'murder', 'people', 'prophet'],\n",
    "           ['war', 'humanity', 'religion', 'salon', 'world']]\n",
    "\n",
    "# Coherence model\n",
    "cm = CoherenceModel(topics=topics, \n",
    "                    texts=corpus,\n",
    "                    coherence='c_v',  \n",
    "                    dictionary=dic)\n",
    "\n",
    "coherence_per_topic = cm.get_coherence_per_topic()\n",
    "print('coherence_per_topic:', coherence_per_topic)\n",
    "\n",
    "#visualize the coherence score of each topic using the seaborn library.\n",
    "# plotting coherenece score\n",
    "topics_str = [ '\\n '.join(t) for t in topics ]\n",
    "data_topic_score = pd.DataFrame( data=zip(topics_str, coherence_per_topic), \n",
    "                                columns=['Topic', 'Coherence'] )\n",
    "data_topic_score = data_topic_score.set_index('Topic')\n",
    "\n",
    "# plottinh using matplotlib heatmap\n",
    "fig, ax = plt.subplots( figsize=(2,6) )\n",
    "ax.set_title(\"Topics coherence\\n $C_v$\")\n",
    "sns.heatmap(data=data_topic_score, annot=True, square=True,\n",
    "            cmap='Reds', fmt='.2f',\n",
    "            linecolor='black', ax=ax )\n",
    "plt.yticks( rotation=0 )\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfcbe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the Coherence Metric of the Model\n",
    "#  assessing coherenece metric of the model\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "topics = [['prophet', 'slavery', 'violence', 'fear'],\n",
    "           ['people', 'religion', 'slave', 'hate', 'like'],\n",
    "           ['like', 'murder', 'people', 'prophet'],\n",
    "           ['war', 'humanity', 'religion', 'salon', 'world']]\n",
    "\n",
    "# Coherence model\n",
    "cm = CoherenceModel(topics=topics, \n",
    "                    texts=corpus,\n",
    "                    coherence='c_v',  \n",
    "                    dictionary=dic)\n",
    "\n",
    "coherence_per_topic = cm.get_coherence_per_topic()\n",
    "coherence_per_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa24fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp397]",
   "language": "python",
   "name": "conda-env-nlp397-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
