{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86ad65c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error:     0.125 +/- 0.014\n",
      "Root Mean Squared Error: 0.166 +/- 0.020\n"
     ]
    }
   ],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html#sphx-glr-auto-examples-applications-plot-cyclical-feature-engineering-py\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "bike_sharing = fetch_openml(\"Bike_Sharing_Demand\", version=2, as_frame=True)\n",
    "df = bike_sharing.frame\n",
    "\n",
    "X = df.drop(\"count\", axis=\"columns\")\n",
    "y = df[\"count\"] / df[\"count\"].max()\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "ts_cv = TimeSeriesSplit(\n",
    "    n_splits=5,\n",
    "    gap=48,\n",
    "    max_train_size=10000,\n",
    "    test_size=1000,\n",
    ")\n",
    "\n",
    "all_splits = list(ts_cv.split(X, y))\n",
    "train_0, test_0 = all_splits[0]\n",
    "\n",
    "#Trigonometric features\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def sin_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
    "\n",
    "def cos_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n",
    "\n",
    "def evaluate(model, X, y, cv):\n",
    "    cv_results = cross_validate(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"],\n",
    "    )\n",
    "    mae = -cv_results[\"test_neg_mean_absolute_error\"]\n",
    "    rmse = -cv_results[\"test_neg_root_mean_squared_error\"]\n",
    "    print(\n",
    "        f\"Mean Absolute Error:     {mae.mean():.3f} +/- {mae.std():.3f}\\n\"\n",
    "        f\"Root Mean Squared Error: {rmse.mean():.3f} +/- {rmse.std():.3f}\"\n",
    "    )\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "alphas = np.logspace(-6, 6, 25)\n",
    "\n",
    "#ColumnTransformer\n",
    "categorical_columns = [\n",
    "    \"weather\",\n",
    "    \"season\",\n",
    "    \"holiday\",\n",
    "    \"workingday\",\n",
    "]\n",
    "\n",
    "cyclic_cossin_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"categorical\", one_hot_encoder, categorical_columns),\n",
    "        (\"month_sin\", sin_transformer(12), [\"month\"]),\n",
    "        (\"month_cos\", cos_transformer(12), [\"month\"]),\n",
    "        (\"weekday_sin\", sin_transformer(7), [\"weekday\"]),\n",
    "        (\"weekday_cos\", cos_transformer(7), [\"weekday\"]),\n",
    "        (\"hour_sin\", sin_transformer(24), [\"hour\"]),\n",
    "        (\"hour_cos\", cos_transformer(24), [\"hour\"]),\n",
    "    ],\n",
    "    remainder=MinMaxScaler(),\n",
    ")\n",
    "cyclic_cossin_linear_pipeline = make_pipeline(\n",
    "    cyclic_cossin_transformer,\n",
    "    RidgeCV(alphas=alphas),\n",
    ")\n",
    "evaluate(cyclic_cossin_linear_pipeline, X, y, cv=ts_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57acd5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABSnElEQVR4nO2dd3gU1frHP286hBAICS2UQAi9BIj0KoKAV0GvqKiIFUFQLNefetVrb9deKBe7WLGjovQqHQwdUiCQUEISCC0ESHJ+f8xEY9iElN3M7O75PM8+uzvnzJzv1HdOe19RSqHRaDQa78XHagEajUajsRZtCDQajcbL0YZAo9FovBxtCDQajcbL0YZAo9FovBxtCDQajcbL0YbAxYhIXxHZZbWOqsad91tEnhSRT83fTUTkpIj4uqCcaiLyk4gcE5Gvnb19s4w/98XJ271ZRFY4e7sVQUTqicgyETkhIq9arccd0YagApgPhsJPgYicLvL/hqJ5lVLLlVKtrNLqCkREiUiL0vJ4yn4rpfYppWoopfJdsPmrgXpAHaXUqMpuTEQGiEha5WWVuP2PRORmV23/AmWniEhUCcnjgEygplLqgUqW85GIPFuZbbgjflYLcEeUUjUKf4tICnC7UmpB8Xwi4qeUyqtKbXbAW/e7AjQFEipyrLzlGJdxP5sC25UNZse663nRNQInUvhGJiIPicgh4MPib2ki0kVE/jCrsV+LyFdF30BE5A4RSRKRIyIyW0QaFklTIjJeRBJF5KiITBERMdN8ROQxEdkrIodF5BMRCTXTosx1bxGRVHPd8SJykYhsFpFsEXmn2L7cKiI7zLxzRaSpuXyZmWWTWQO6toz73VhEvhORDBHJKl5ekXw+IvKwiCSb+WaJSFix/RgrIvtEJFNEHi2yrq+I/Ntc94SIbBCRxmZaLxFZZzbDrBORXkXWayYiS8115gPhRdIKy/Qz/y8RkWdE5Hcz/zwRKZr/JvMcZInI4+ab7CUO9vMp4D/AteZxvK2M5/A2EdkHLCq2vWDgV6Ch/FU7Lbx2AsxtnRCRbSISV2S9hiLyrXle9ojIPY7OS0mIyCvmNbJHRIYV2+5s8zpOEpE7iqT97a3bwbWSYl5Lm4FThce+hPI/AsYC/2fu8yWlXUPmOl+LyCHzWlgmIu3M5eOAG4ps6ydz+d9qwEX1l3Dtl1q+LVFK6U8lPkAKcIn5ewCQB7wEBALVzGVpZnoAsBeYDPgDVwFngWfN9IsxqrhdzPXfBpYVKUsBPwO1gCZABjDUTLsVSAKaAzWA74CZZlqUue50IAgYAuQCPwB1gUjgMNDfzD/S3FYbjFrjY8DKYjpaFPl/of32BTYBrwPBpoY+JRzPe4HVQCNzW/8Dvii2H++aZXQCzgBtzPQHgS1AK0DM9DpAGHAUGGPuz2jzfx1zvVXAa2Z5/YATwKfFyvQz/y8BkoGWpoYlwItmWlvgJNDHPNevAOcwrw8H+/pkYTnlOIefmMewmoPt/XnMi5WRCww3z8MLwGozzQfYgGGQAsxydwOXluG6v9nctzvM7U4ADgBipi8FpprnOhbjWh1kpn2Eec070o1xT8UDjR3tpwMtxbd3LyVcQ0WOc4iZ9gYQX9K2Srje/8yD42u/1PLt+LFcgLt/ON8QnAWCiqT/eZFjPGT2F94s5rIVRS6q94H/FkmrYd5sUeZ/RZEHKDALeNj8vRC4q0haK3NdP/56iEQWSc8Cri3y/1vgXvP3r8BtRdJ8gBygaREdxQ1BafvdE+NB4FeG47kD84Fh/m/gYD8aFUlfC1xn/t4FjHCwzTHA2mLLVmE8zJqYN3JwkbTPKd0QPFYk713Ab+bv//D3B05187iU1RCU5Rw2L+XY/XnMi5WxoMj/tsBp83d3YF+x/I8AH5bhPN0MJBXbVwXUx3iA5wMhRdJfAD4yf3/EhQ3BreW4B4tvr8RryMG6tUzdoY62VcL1/mceHF/7ZS7fLh/dR+B8MpRSuSWkNQT2K/PqMEktlr6x8I9S6qSIZGG8saeYiw8VyZ+DYSwK191bJG0vxgOkXpFl6UV+n3bwv3BbTYE35e8jMMTUUbSMopS2342BvapsbadNge9FpKDIsnz+vh8lHYPGGG/rxSl+bDD/R5ppR5VSp4qlNS5FY2nn4M/zqZTKMc9fWSnLOUyl/BTXG2Q2tzTFaErKLpLuCywv73bNfQXjWNQBjiilThTJuxeIo+xUZD8LKfEaMptvngNGARFAYZ5w4FgFyyt+7Zd2De+vYBkuRfcROJ/SOqwOApFi3jEmRR84BzAuIuDPdt86lO3i+du6/PWmm+44e6mkAncqpWoV+VRTSq0sZZ3S9jsVaFJaW2+xvMOKlR2klCrLMUgFoh0sL35swDg++zHOSW3zWBdNqwgHMZoDAGN4KMb5KytlOYelHefS0hyRCuwpdqxDlFLDy7md4hwAwkQkpMiywuMNcAqjBlFIfQfbKO++FKW0a+h6YARwCRCKUdMC40WnpHJzLqC3+DqVuYYtQRuCqmUVxpvBJBHxE5ERQLci6Z8Dt4hIrIgEAs8Da5RSKWXY9hfAfWbHZw1z3a/K+BZenOnAI0U60UJFpOjwxnSM9uSyshbjIfmiiASLSJCI9C6l7Ofkr87pCPM4lYX3gGdEJEYMOopIHWAO0FJErjeP+7UYTSQ/K6X2AuuBp0QkQET6AJeXY9+K8g1wuRgd0wHAU/z1gCkLlT2H6UCdwg7mMrAWOG52dFYTo7O9vYhcVA7N56GUSgVWAi+Y57ojcBvwmZklHhguImEiUh+jTd2ZlHYNhWD0K2VhPNyfL7auo2s7HrjePD5Dgf6VKN+WaENQhSilzmJ0EN8GZAM3YnT+njHTFwKPY7TXH8R4u72ujJv/AJgJLAP2YHQQ3l1Bnd9jdH59KSLHga3AsCJZngQ+FmO00TVl2F4+xsO1BbAPSAOuLSH7m8BsYJ6InMDodOteRumvYfSbzAOOY/S5VFNKZQH/AB7AeAD8H/APpVSmud71ZhlHgCcwOmTLjVJqG8Yx/xLj/J3A6IQ/U8ZNVOocKqV2YhiT3ea5aXiB/IXnJdYsLxPDmJbVkJTGaIy37QPA98ATSqn5ZtpMjMEDKRjn6isnlFeU0q6hTzCaqfYD2820orwPtDWP3w/msskYxykbY1TRD5ROZa5hSyjs4ddYhIisAaYrpT60WovGuZhv9dlAjFJqj8VyNJoS0TWCKkZE+otIfbOJYizQEfjNal0a5yAil4tIdbPP4RWM4awp1qrSaEpHG4KqpxVGtfgYRlPF1Uqpg9ZK0jiRERjNIQeAGIyhrbrarbE1umlIo9FovBxdI9BoNBovxy0nlIWHh6uoqCirZWg0Go1bsWHDhkylVETx5W5pCKKioli/fr3VMjQajcatEBGHngF005BGo9F4OdoQaDQajZejDYFGo9F4OdoQaDQajZejDYFGo9F4OU4xBCLygRih9baWkC4i8pYYIes2i0iXImlDRWSXmfawM/RoNBqNpuw4q0bwETC0lPRhGNPtY4BxwDQwYswCU8z0tsBoEWnrJE0ajUajKQNOmUeglFomIlGlZBkBfGL6XFktIrVEpAGGm9okpdRuABH50sy73Rm6ziNhHmQmQPTFULcNSHlcxXs+R0+dZUVSJhknztCjeR3aNAhB9DHSaJzPmROQsgIyE6FJD2jYBXytm9ZVVSVH8vfQc2nmMkfLHfrtFpFxGLUJmjSpYACpxHmw7l3jd0hDwyC0uBiaD4TqYRXbphtzLr+A+NRsliVksCwhg837j1HU9VRESCB9Y8Lp3zKC3i3CCa8RaJ1YjcadKSiAQ5sheSEkLYLUNVBw7q/0oFBoPgCiB0GLQRDaqMRNuYKqMgSOXitVKcvPX6jUDGAGQFxcXMU85V32CvSeDMmLjBOy8yeI/9SQ0bCzcQKiB0GjOPD1r1ARdif1SA5LzQf/quQsTpzJw0cgtnEtJg+KoV/LCOrVDOL3pEyWJWSwaOdhvttoRNhrH1mTfjER9GsZQZcmtQnw02MNNJoSOZH+17MmeTHkmHGQ6nWAnncZz5qIVrD3d8M4JC+E7T8aecJb/mUUmvaGgOoll+MEnOZ91Gwa+lkp1d5B2v+AJUqpL8z/u4ABGE1DTyqlLjWXPwKglHqhtLLi4uKUU1xM5OfBgT9MK70Q9q8HVQCBNaFZP6PGEDMEapUWx9ze5J7LZ0ViJssSM1iemMmeTCNGe2StavRrGU7fmAh6R4cTWt2x4csvUGzdf8yoNSRmsHFfNvkFiuAAX3pG16FfywgGtqpL4zDXXqgaje3JzzMe6oVv/elbjOXVw41nSeEnpJ7j9ZWCjJ3Gsyh5IexdCXm54BsITXv+ZRjqtq1ws7aIbFBKxZ23vIoMwWXAJGA4RtPPW0qpbmYw8wRgEEbouHXA9WbIvxJxmiEozumjsHvpXyfyeBqILwx7CS663e36FBLTT3DHJ+tJycohyN+HHs3r/PlGHx0RXKH2/+O551iZlMXyRMMwpB45jY/AQ0NbM65fc92noPFOjh+EWWMgbR34+EHjHkazc/QgqN8RfCpQez532jQsiw3jkLHDWH7NJ9C2YiGQXWoIROQLjDf8cIzgz08A/gBKqeliPB3ewRhZlAPcopRab647HHgD8AU+UEo9d6HyXGYIiqKU0ZEz/3FI+A06j4HLXgU/92gnn789nXu//INqAX68cFUH+rUMJ9DP16llKKVIycrh5bk7mbPlECNiG/LSPzsS5O/ccjQaW5O2Hr68wegAHv5f4yEdGOL8co7tN5qaWl9W4T5Nl9cIqpIqMQSFFBTA4udg+SvQqBtcOxNC6ldN2RWgoEDxzuIkXpufQMdGofxvTFcahFZzaZlKKaYuSeaVebto17Am/xsTR2Qt15ap0diCPz6Dn++FkAYw+guo185qRaVSkiHQvX0XwscHBj0Ooz6C9K0wYyDs32C1KoecOpPHxM838tr8BK7sHMmsO3u63AgAiAgTB7bg3TFxpGTmMOKdFaxLOeLycjUay8jPg18fhh/vMoZ/jltieyNQGtoQlJV2V8Jt84z2vw+GQfwXViv6G6lHcvjntJXM3XaIxy5rw2vXdKryJppL2tbjh4m9CAny5/p3V/PZGoeuzzUa9ybnCHx6FayZBt0nwI3fu/3wc20IykP9Doblb9wNfhgPcx813gws5vekTC5/ZwUHj+Xy8a3duL2vdZ22LeqG8MPE3vSKDufR77fy6PdbOJtXYIkWjcbppG+DGQNg3yoYMRWGvWjpRDBnoQ1BeQmuA2O+h253wqp34LOrjTcEC1BK8eHve7jpg7VE1Ajkx4m96RtzXhS6Kie0mj8f3HwRd/Zvzmdr9nHje2vIPHnGalkaTeXY/iO8NxjyzsAtv0LnG6xW5DS0IagIvv7G6IAr3jGGd707EA7vqFIJuefyefCbzTz103YGta7L9xN7ExUeXKUaSsPXR3hkWBvevC6WTWnZXPH2CrbuP2a1LI2m/BQUwKLnYNZNhmuacUuMSacehDYElaHLGLj5F2O873uXwI6fq6TY9OO5XDdjNd9sSGPyoBim39iVGoH2rJ6OiI3k2wm9ALh6+kp+jN9vsSKNphzkHoevboBl/4XYG437vWYDq1U5HW0IKkvjbsYbQnhL44JZ8iK4cEjuH/uOcvnbK0hIP8H0G7tw3+CW+PjYexJX+8hQZt/dh46RtZj8ZTwv/LqD/AL3G7as8TKykuH9wZAwF4b9F0a8A/5BVqtyCdoQOIOaDY02w07Xw5IXYPmrLikmOeMkY95fS5C/L9/d1Yuh7d3nzSS8RiCf3t6dG3s04X9Ld/PqvF1WS9JoSuZUFnwyAk4eNvoEu9/pdp4FyoM92xPcEf8gGDnV8Ci4+DmI7ArRA522+VNn8hg/cwMBfj58Oa4HDd1wwlaAnw/PjuxAfoExAa1zk9oMbluC3xWNxioK8uG72w0jcNtcwyGlh6NrBM5EBC5/E8Jbwbe3GVPCnYBSike+20JyxkneHt3ZLY1AUZ64vB0dIkO5f1Y8e7NOWS1Ho/k7S18yXDkMf9krjABoQ+B8AoINNxR5Z+DrsZB3ttKb/GTVXmZvOsADQ1rRu0W4E0RaS5C/L1Nv6IKPCOM/3cjps/lWS9JoDBLmGYYg9kbocpPVaqoMbQhcQXgMjJhieCKc92ilNrVh71Ge/WU7l7Spy4T+0U4SaD2Nw6rzxnWx7Dx0nMd+2Io7+rzSeBhHU+C7O4yJo5e94tF9AsXRhsBVtBsJPSfB2hmweVaFNpF58gwTP9tIg9BqvDoq1vajg8rLwFZ1uefiGL7dmMYXa1MvvIJG4yrO5RrzBFBwzUzwd+/m1/KiDYErueRJaNILfpoM6eULw5xfoLjniz84mnOWaTd2KTFwjLtzjxkV7cnZ29iclm21HI238uuDcHATXDkDwppZrabK0YbAlfj6w6gPDd/ks8YYk1PKyKvzdrEyOYtnR7anXcNQF4q0Fl8f4c1rY4kICWTCpxs5eqryfSoaTbnYOBM2fgJ9/wWthlqtxhK0IXA1IfUNF9ZH9hgua8vQFj5/ezpTlyQzultjRsW5b5jMslI7OICpN3Qh48QZJn8VryebaaqOg5tgzr+MwPED/221GstwiiEQkaEisktEkkTkYQfpD4pIvPnZKiL5IhJmpqWIyBYzrYqizVQxTXvB4Kdhx0+w8u1Ss6ZknuL+WfF0iAzlicvd1795eenUuBZPXtGOZQkZvLUw0Wo5Gm/g9FH4agxUrwP/fB98vDeyXqUNgYj4AlOAYUBbYLSItC2aRyn1slIqVikVCzwCLFVKFXXZOdBM9yxPTkXpOdEIYbfgSUhZ4TDL6bP5jP90A74+wtQbunhdyMfR3Rrzzy6NeGtRIot3HbZajsaTKSiA78fD8QNGDOBg9x+WXRmcUSPoBiQppXYrpc4CXwKlRVYeDdgrqktVIGJ4Kw1rDl/fAicO/S1ZKcVjP2xlV/oJ3rg2lsZh1S0Sah0iwrMj29O6fk3u+yqe1CM5VkvSeCorXjVikQ99weM8iVYEZxiCSKDo2L80c9l5iEh1jAD23xZZrIB5IrJBRMaVVIiIjBOR9SKyPiMjwwmyLSCopjHZ7OxJ+PpmyD/3Z9IXa1P5dmMa91wcw4BWda3TaDHVAnyZdkMX8gsUd322kdxzerKZxskkLzLcSne4Bi663Wo1tsAZhsDR4PaSevsuB34v1izUWynVBaNpaaKI9HO0olJqhlIqTikVFxFhffCVClO3DVzxthHhaMGTAGxOy+bJ2dvo1zKCewbFWKvPBkSFB/PaNbFs2X+Mp34q37BbjaZUjqXBt7cb9+Hlb3jVpLHScIYhSAOKDm1pBBwoIe91FGsWUkodML8PA99jNDV5Nh2u/jPC2cmNXzPh041EhATy5rWx+HrYpLGKMrhtPe4aEM0Xa/fx9Xo92UzjBPLOwCzT7cs1Mw13MBrAOYZgHRAjIs1EJADjYT+7eCYRCQX6Az8WWRYsIiGFv4EhwFYnaLI/Q55FNeqG7093U+PEHqbe0IXawQFWq7IV9w9uSa/oOjz2w1a2Hyj7HAyNxiHzHoP96w0vweEtrFZjKyptCJRSecAkYC6wA5illNomIuNFZHyRrFcC85RSRd1N1gNWiMgmYC3wi1Lqt8pqcgv8Avi19QvkFvjycd3P6dTIcyeNVRQ/Xx/eGt2ZmtX8eeS7zRTo+QWairJ3leHupcdEaHuF1Wpsh1PmESil5iilWiqlopVSz5nLpiulphfJ85FS6rpi6+1WSnUyP+0K1/UGjuee4z9LsvkyZCz1j66Hbd9ZLcmWhNcI5N/DW7Mp7Rhfb9BNRJoKUJAPcx6Emo3g4so5gfRU9Mxii3hzQSJZp87Q55p/QYNOMPcxOHPSalm2ZGRsJBdF1eal33ZxLOfchVfQaIqy/gNI3wKXPqf7BUpAGwIL2HXoBB+tTGF0tyZ0aBIGw1+BEwdg2ctWS7MlIsJTV7QnO+csr87XIS415eBUJix6Bpr1NyZ0ahyiDUEVo5TiidlbCQny48EhrYyFjbtB7A2wagpkavcKjmjbsCZjejTl09V72XbgmNVyNO7Cwqfg7Ckj+LweKloi2hBUMT9vPsjq3Uf415BWfx8ldMmT4F8dfv2/Mjmm80buH9yK2tUDeOLHbTqQjebCpG0wPIt2Hw91W1utxtZoQ1CFnDqTx3O/7KB9ZE1Gd2vy98QadQ3vh8mLYOcv1gi0OaHV/XloaGvW7z3KD/HOiQet8VAKCgyvojXqQf+HrFZje7QhqELeXpTEoeO5PHVFe8cTxy66Heq2hd8egbPaz44jru7aiE6Na/H8nJ2cyNUdx5oSiP8UDmyEIc8Yrl00paINQRWRnHGS91fs5uqujejatLbjTL5+RsfxsX3w+xtVqs9d8PERnhnRjsyTZ3hzge5P0Tjg9FHDfUuTntBhlNVq3AJtCKoApRRPzt5GkJ8vDw29QFtlVG/j4l3xhhHMRnMeHRvV4rqLmvDhyhQS0k9YLUdjNxY9ZxiD4S/rDuIyog1BFTBvezrLEzO5b3BLIkICL7zC4GeMMJdzvTdi0oV48NJW1Aj048nZuuNYU4SDm2H9+0Yza/0OVqtxG7QhcDGnz+bz9E/baVUvhJt6Ni3bSjUbQP//g11zIGGeawW6KWHBAfzr0lasTM7ily0HrZajsQNKGTOIq4XBQD2DuDxoQ+Bipi1NZn/2aZ4a0Q4/33Ic7u4TILylMZz0XK7rBLox13drQruGNXnulx2cOpNntRyN1WyeBamrjaHY1WpZrcat0IbAhezLymH60mSu6NSQHs3rlG9lvwAY9hIc3QOr3nGNQDfH10d4ekQ7Dh7LZcriJKvlaKwk9zjMfxwiuxqTMzXlQhsCF/L0z9vx9xH+PbxNxTYQfTG0uQKWvQLZ2uGaI7o2DeOfXRrx7vLd7M7Qvpq8lqUvwcnDxqg7H/1YKy/6iLmIxTsPs2BHOvcMiqF+aFDFN3Tp88b3PN3mWRIPD2tNkJ8vT/20XXcceyOHd8DqadB1LER2sVqNW6INgQvIPZfPkz9to3lEMLf0bla5jdVqDP0egO0/QvJi5wj0MCJCArlvcEuWJmQwf3u61XI0VYlSRj9aYAhc/B+r1bgt2hC4gPdX7GFvVg5PXt6OAD8nHOKed0PtZsYFn3e28tvzQG7q2ZRW9UJ4+uftOuC9N7H9B9izDAY9DsHl7IfT/IlTDIGIDBWRXSKSJCIPO0gfICLHRCTe/PynrOu6G/uzT/P2okSGtqtPv5YRztmof5DRcZyZAGv/55xtehh+vj48eUU70o6eZvrSZKvlaKqCMydh7qPGfIGut1itxq2ptCEQEV9gCjAMaAuMFpG2DrIuV0rFmp+ny7mu2/DcL9sBeOwfFewgLomWl0LLobDkRTiux807omd0HS7v1JBpS5JJPaJ9NXk8y1+F4/th+Kvg42u1GrfGGTWCbkCSGXbyLPAlUNYIEJVZ13as2Z3FnC2HmDigBY1qV3d+AUNfgPxzRqANjUMeHd4GXx/hhV93WC1F40qO7jWGVXe6Hpp0t1qN2+MMQxAJFB3bmGYuK05PEdkkIr+KSLtyrouIjBOR9SKyPiMjwwmynYtSilfnJVA3JJA7+jV3TSFhzaHbHbDpCx3ApgTqhwZxe9/mzNlySAew8WSW/RcQo29AU2mcYQgceXUqPoZvI9BUKdUJeBv4oRzrGguVmqGUilNKxUVEOKnt3Yn8npTF2pQjTBzYgiB/F1ZTe98LftWMcdMah9zWpxk1g/x4fb42lh5JVjLEfwFxt0LNhlar8QicYQjSgMZF/jcCDhTNoJQ6rpQ6af6eA/iLSHhZ1nUHlFK8On8XDUODuK5b4wuvUBlqRED3cbDlG2P8tOY8Qqv5M65fcxbsSGdTarbVcjTOZul/wTcA+txntRKPwRmGYB0QIyLNRCQAuA6YXTSDiNQXMfzBikg3s9yssqzrDizZlcEf+7KZdHEMgX5V0GnV6x4IqAFLXnB9WW7Kzb2bUbu6P6/NT7BaisaZZCTAlllGE2lIPavVeAyVNgRKqTxgEjAX2AHMUkptE5HxIjLezHY1sFVENgFvAdcpA4frVlZTVaKU4rX5CTSqXY2ruzaqmkKrh0GPCcYks4Obq6ZMN6NGoB939o9maUIGG/YesVqOxlksecFoGu092WolHoVT5hEopeYopVoqpaKVUs+Zy6Yrpaabv99RSrVTSnVSSvVQSq0sbV13Yv72dLbsP8Y9g2KcM3msrPScCEGhulZQCjf1bEp4jQBenadrBR5B+jbY9h30GA/B4Var8Sj0zOJKUFBg1AaahQdzVWeHg51cR7VaxozjXXNg/8aqLdtNqB7gx4QBLViZnMWq5Cyr5Wgqy5IXILAm9JxktRKPQxuCSvDr1kPsPHSCyYNiyhdrwFl0vxOq1YbFz1d92W7CDd2bUK9mIK/PT9AO6dyZg5tgx09GTbh6mNVqPA5tCCpIfoHijQUJtKhbg8s7WTSELaim0VaaNB9S11qjweYE+fsycWAL1qYcYUVSptVyNBVl8fMQVMvoG9M4HW0IKsjPmw+QePgk914Sg6+PhQGyu42D4AhY9Kx1GmzOtRc1pmFoEK/O07UCtyRtPST8Br3uNvrFNE5HG4IKkJdfwBsLEmldP4Th7RtYKyYg2BhPvWcppKywVotNCfTz5e5BMcSnZrNkl/1mpWsuwOLnoXod6D7+wnk1FUIbggrw/R/72ZN5ivsGt8THytpAIXG3Qo36xg2j33gdcnXXRjQOq8Zruq/Avdi3GpIXGjPqA2tYrcZj0YagnJzLL+CtRYm0j6zJkLY2mdDiXw36PgB7f4fdS6xWY0v8fX245+IYtuw/xjwdvMZ9WPQsBNeFi263WolHow1BOflmQxqpR05z/+CWmJOl7UHXsVAzEhY/p2sFJXBl50iahQfz+vwECgr0MbI9e5ZBynLoez8EuMCbr+ZPtCEoB2fy8nl7YSKxjWsxsFVdq+X8Hb9A6PcvSFsHSQusVmNL/Hx9mDwohp2HTvDr1kNWy9GUhlJGU2dIQx10pgrQhqAczFqXyoFjuTwwxGa1gUJib4RaTXStoBQu79SQmLo1eH1BAvm6VmBfkhfBvlVGvG7/IKvVeDzaEJSR3HP5vLM4iYuiatOnhU2nt/sFQP+H4MAfxoxjzXn4+gj3XtKSpMMn+WmT2zm69Q6UMl5mQhtD5zFWq/EKtCEoI5+t2Uf68TPcP7iVPWsDhXS8zghgs/h5KCiwWo0tGda+Pq3rh/DmwkTy8vUxsh0Jc2H/Buj3oNHkqXE52hCUgZyzeUxbkkSv6Dr0jK5jtZzS8fWD/g9D+lbY4XYevasEHx/hvsEt2ZN5iu//2G+1HE1RCmsDtaMg9nqr1XgN2hCUgZmr9pJ58iz3D25ptZSy0eFqCG9lOOkqyLdajS0Z0rYeHSJDeWtRIud0rcA+7PwZDm02XmZ8/a1W4zVoQ3ABTp7JY/rSZPq1jCAuyk2cXfn4woCHIWMnbP3OajW2RES4f3BLUo+c5uv1aVbL0YDRlLn4eajTAjqMslqNV+EUQyAiQ0Vkl4gkicjDDtJvEJHN5meliHQqkpYiIltEJF5E1jtDjzP56Pc9HM055z61gULajoS67YxaQX6e1WpsyYBWEcQ2rsU7ixI5k6drTpaz/Xs4vB0GPGI0cWqqjEobAhHxBaYAw4C2wGgRaVss2x6gv1KqI/AMMKNY+kClVKxSKq6yepzJsdPnmLFsN5e0qUts41pWyykfPj4w8BE4kgybv7JajS0RER4Y0pIDx3L5cm2q1XK8m4J8WPIiRLSBdldZrcaWFBQo5m075JIBDs6oEXQDkpRSu5VSZ4EvgRFFMyilViqljpp/V2MEqbc9H69M4XhuHvde4ma1gUJa/wMadIJl/9W1ghLo0yKcblFhTF2SpGsFVrLte8hMMJo0fXSLtSMW7TzMuJkb+G2b8ydDOuOIRwJFX6fSzGUlcRvwa5H/CpgnIhtEZFxJK4nIOBFZLyLrMzJc70Hy1Jk8Pvh9D5e0qUv7SDd1fStiDME7mgLbf7BajS0REe4e1IL042f4bqMeQWQJBQWw/FWIaA1trrBajS1RSvHO4iQa1a7G0Hb1nb59ZxgCR4PqHU7ZFJGBGIbgoSKLeyulumA0LU0UkX6O1lVKzVBKxSml4iIiIiqr+YJ8sXYf2TnnuGtgC5eX5VJaXWaMIFr+qp5XUAJ9WoTTsVEo05cm63kFVpA41+gb6HO/rg2UwKrdWcSnZnNn/2iXREN0xhbTgMZF/jcCzpuyKSIdgfeAEUqpPwPIKqUOmN+Hge8xmpos5UxePu8u303P5nXo0qS21XIqh4+P4bTr8HbjhtOch4hw14AW7M3K4ZctB62W410oZbyk1GoC7f9ptRrbMnVxMuE1AhnV1TWt6s4wBOuAGBFpJiIBwHXA32YyiUgT4DtgjFIqocjyYBEJKfwNDAG2OkFTpfhu437Sj5/hroHRVktxDu3/CaFNjBtO+yByyJC29WhRtwbTliTreAVVScoKw1Fi78l6pFAJxKdmsyIpkzv6NiPI39clZVTaECil8oBJwFxgBzBLKbVNRMaLSGFIof8AdYCpxYaJ1gNWiMgmYC3wi1Lqt8pqqgx5+QVMX5pMx0ah9vUpVF58/aH3PcYNp6OYOcTHR5jQP5qdh06waOdhq+V4D8tfNeINxN5otRLbMnVxEjWD/LihR1OXleGUxial1BylVEulVLRS6jlz2XSl1HTz9+1KqdrmENE/h4maI406mZ92hetayZyth9iblcNdA1rY26dQeel8o3HDLX/VaiW25YrYhkTWqsY7i5N0raAq2L8Bdi+GXpO0h9ESSEw/wbzt6dzcuxk1Al1XY9I9M0VQSjF1cRIt6tawT/QxZ+FfDXpONG68/RusVmNL/H19GN+/OX/sy2b17iNWy/F8lr9mBKOPu9VqJbZl2pJkqgf4ckuvKJeWow1BERbtPMzOQye4a0C0PWIRO5u4W40bb/lrViuxLaPiGhNeI5CpS5KsluLZHN5p+BXqPh4CQ6xWY0tSj+Tw46YDjO7WhNrBAS4tSxsCk6LjdC/v1NBqOa4hqCZ0u9O4AQ/vtFqNLQny9+X2vs1YnpjJ5rRsq+V4LiteB//qhiHQOOR/y5LxEbijb3OXl6UNgcnq3Uf4Y182d/Zrjr8Lxunahu7jjRvw9zesVmJbbujehJpBfkxdnGy1FM/k6F7Y8rURgrK6mzhyrGIOH89l1vo0ru7aiPqhru8/8eAnXvmYuiTJGKcb1/jCmd2Z4DrQ9WbYPMu4ITXnERLkz9heUfy27RCJ6SesluN5rHwLxMfoJNY45P0Ve8jLL+DOflUzhF0bAmBzWjbLEzO53YXjdG1Fz0nGjbjyLauV2JZbejejmr8v05bqWoFTOZEOG2caQWdqemgTbCU5lnOOT1fv5R8dGxIVHlwlZWpDgDFrr2aQHzd0b2K1lKohNBJiRxs35Il0q9XYkrDgAEZ3a8KP8QdIPZJjtRzPYfUUKDhnTCDTOOTjVSmcOpvPhAFVN6HV6w1BYvoJftt2iJt7RRES5EURkXrfa9yQq6dYrcS23NGvGT4CM5bttlqKZ3D6KKx7H9pdCXU8ZNa+kyl0djmodV3aNKhZZeV6vSGYtjSZav6+3Ny7mdVSqpY60cYNue594wbVnEeD0Gr8s0sjvlqfyuETuVbLcX/WvgtnTxrO5TQOscrZpVcbgtQjOfwYf4DruzchzMXjdG1Jn/uNG3Ptu1YrsS139o8mL7+A91fssVqKe3P2FKyeBi2HQv32VquxJYXOLns0D6Nr06p1dunVhmDGst34CNze18tqA4XUb2/cmKunGTeq5jyahQdzWceGfLZ6H8dyzlktx33Z8DGcPgJ9H7BaiW0pdHY50QLX915rCA6fyOWr9an8s0sjGoRWs1qOdfS537hBN3xstRLbMqF/NCfP5PHJqhSrpbgneWdg5dsQ1RcaW+5l3pZY7ezSaw3Bn+N0+3t5p1WT7tC0j3Gj5p2xWo0taduwJhe3rssHv+8h56wO+VluNn0JJw4YcTE0DvnL2WW0Jc4uvdIQHMs5x2er93FZx4Y0q6Jxuram7/3GjbrpS6uV2JaJA6M5mnOOL3SQ+/KRn2e4k2gQC80HWq3GlhQ6u4yOCGZIW+eHoSwLXmkIPlmVwskzedxVheN0bU30xcaNuuJ1HeS+BLo2DaN7szDeXbZbB7kvD9t/gKN7jL4BT3Lr7kT+cnbZwjJnl15nCHLOWjNO19aIGDfq0T06yH0pTBzYgkPHc/leB7kvG0oZLxfhLaH1P6xWY0sKnV1G1qrGFbHWzbR2iiEQkaEisktEkkTkYQfpIiJvmembRaRLWdd1Nl+sTeVozjnPCUPpLFr/w7hhV7yuw1mWQN+YcDpEGkHu8wv0MbogifMgfasOSl8Khc4ux/e31tllpUsWEV9gCjAMaAuMFpG2xbINA2LMzzhgWjnWdRpn8vJ5d9luujcLo2tT7fXwb/j4GDds+lbjBtach4gwcWA0KVk5zNFB7ktHKVj2ihEru8PVVquxLXZxdukME9QNSDLDTp4FvgRGFMszAvhEGawGaolIgzKu6zS+37ifQ8dzLRmn6xZ0uNq4cZe9omsFJTCkbX2iI4KZosNZls7e3yFtrREr29eLXLeUg0Jnl7f1sd7ZpTMMQSRQdChFmrmsLHnKsi4AIjJORNaLyPqMjIwKCc04cYa4prXpG+MhQemdzZ9B7tcaN7LmPHx8hAkDWrDz0AkW79JB7ktk+asQHGHEytY4ZOriZEKC/Lixh/XOLp1hCBx1cxd/VSopT1nWNRYqNUMpFaeUiouIiCinRIO7B8Xw1Z09PSsovbPpfKNxA+sg9yUyojDI/SJdK3DI/o2QvMiIke3vxZM1S8Fuzi6dYQjSgKINXI2AA2XMU5Z1nYqvJ8YidiaFQe6TF8GBP6xWY0v8fX24s39zNu7LZs0eHeT+PFa8BoGhEHeb1UpsS6Gzy1ts4uzSGYZgHRAjIs1EJAC4DphdLM9s4CZz9FAP4JhS6mAZ19VUNXG3GTeyDnJfItfENSa8RgBTl+jANX8jYxfs+Am6jzNiZGvOo9DZ5ehu9nF2WWlDoJTKAyYBc4EdwCyl1DYRGS8ihZGp5wC7gSTgXeCu0tatrCZNJQmqadzIO34ybmzNeQT5+3Jbn+YsS8hgS9oxq+XYhxVvmEHpJ1itxLa8u9xwdnlHP3vUBsBJ8wiUUnOUUi2VUtFKqefMZdOVUtPN30opNdFM76CUWl/auhob0H2C0Uy04g2rldiWG3s0ISTIj6lLkqyWYg+O7oXNXxkxsYPrWK3Glhw+kcuX6+zn7FLP8tA4JrgOdBlr3Ng6yL1DQoL8GdvTCHKfdFgHuWfl20Ys7J46KH1JfLAixZbOLrUh0JRMr8Ig929brcS23NI7ikA/H6Yt8fJwlifSYeMn0Ok6Iya25jwKg9IP79DAds4utSHQlExoI+PG/mMmnNRj5h1Rp0agGeR+P2lHvTjI/eqpRgzsPvdZrcS2/OXs0n4TWrUh0JROn/sg/6xxo2scckff5ojAu94a5L4wKH3bkToofQnknM3jw5UpXNy6Lm0b2m80lTYEmtKpE23c4Gvfg9PZVquxJQ1rVeOqzo34cl0qGSe8MLjP2vfg7AkdeKYUvlybypFTZ5loU2eX2hBoLkzf+40bfZ0Ocl8S4wdEcy6/gA9+97Ig92dPwZppEHMp1O9gtRpbcjavgBk2d3apDYHmwtTvADFDdJD7UmgWHsywDg2YuWovx057UZD7jZ9ATpauDZTC93+kceh4LnfZ2NmlNgSastH3AeOG3/iJ1Upsy10DjCD3M70lyH3eWfj9LWjaG5r0sFqNLckvUExfupv2kTXpZ2Nnl9oQaMpGkx7GDb/ybeMBoDmPdg1DGdgqgg9+T+H0WS8IZ7lZB6W/EL9uPciezFNMHNDC1s4utSHQlJ2+98Px/cYkM41DJg5swZFTZ/ly3T6rpbiWgnwzKH0niB5ktRpbopRiyuJkoiOCubSdNUHpy4o2BJqyEz3IuPFXvG48CDTnERcVRrdmYcxYtpuzeQVWy3Ed23+AI7t1UPpSWLIrgx0HjzPBwqD0ZUUbAk3ZKQxyfyQZtv9otRrbMnFgCw4ey+WHPzw0yL1SsLwwKP3lVquxJUWD0o+wMCh9WdGGQFM+Wl9uPACWv6bDWZZAv5hw2kfWZJqnBrlPnA/pW4zJhjoovUPW7jnChr1HudPioPRlxf4KNfbCxwd632s8CBLnW63GlogIdw1owZ7MU/y61cOC3CsFy1+B0MbQYZTVamzLlCXJhNcI4BqLg9KXFW0INOWn4zXGg2CFDlxTEpe2q0/ziGCmLk72rHCWe1dC6hropYPSl8TW/cdYlpDBrTYISl9WtCHQlB9ff+NBsG+V8WDQnIevjzChfzTbDx5nSUKG1XKcR2FQ+i5jrFZiW6YuSTKD0je1WkqZqZQhEJEwEZkvIonmd20HeRqLyGIR2SEi20RkcpG0J0Vkv4jEm5/hldGjqUK6jNFB7i/AyM6RRNaqxtTFHhK45sAfkLwQetylg9KXQNLhk/y69RBje0ZR0wZB6ctKZWsEDwMLlVIxwELzf3HygAeUUm2AHsBEEWlbJP11pVSs+ZlTST2aqsK/mvFASFoAB+KtVmNL/H19GNevOetSjrLWE4LcLzeD0l+kg9KXxPSlyQT6+XBL7yirpZSLyhqCEcDH5u+PgZHFMyilDiqlNpq/T2DEJtaRKzyBi8wg97qvoESuvcgIcj/F3WsFhUHpu90BQaFWq7ElaUdz+OGP/Yzu1oQ6NQKtllMuKmsI6imlDoLxwAfqlpZZRKKAzsCaIosnichmEfnAUdNSkXXHich6EVmfkeFBba7uTFAodLsdts+GwzutVmNLgvx9uaV3M5YmZLA5LdtqORVn+WvgFwQ9dFD6kvjf0t2IGPEp3I0LGgIRWSAiWx18RpSnIBGpAXwL3KuUOm4ungZEA7HAQaDEBmel1AylVJxSKi4iIqI8RWtcSY+JEBAMS1+0WoltualnU2pV9+f1+QlWS6kYGQmwZZZRAwy2r+M0K9mffZqv1qUyKq4xDWu5X//JBQ2BUuoSpVR7B58fgXQRaQBgfjuMZygi/hhG4DOl1HdFtp2ulMpXShUA7wLdnLFTmiokuA50Hw/bvodDW61WY0tCgvwZ1685i3dlsGHvUavllJ+lL4JfNWP+iMYh7yxKBGCSjV1Nl0Zlm4ZmA2PN32OB8/wOiOFy731gh1LqtWJpDYr8vRLQTxJ3pNcko69gyQtWK7EtY3tGUSc4wP1qBenbYet30H0c1NA1cUfsy8rh6/VpjO7mnrUBqLwheBEYLCKJwGDzPyLSUEQKRwD1BsYAFzsYJvpfEdkiIpuBgYCOfO2OVKsNPSfCzp+NIYaa8wgO9GPCgGhWJGWyZneW1XLKzpIXIKCGMW9E45C3FiXi6yNMdNPaAFTSECilspRSg5RSMeb3EXP5AaXUcPP3CqWUKKU6Fh8mqpQao5TqYKZdUdjxrHFDeoyHoFqwWNcKSuKG7k2JCAnktfkJ7jHb+OBm2DEbet4F1e0ZYtFqdmec5LuNaYzp0ZS6NYOsllNh9MxijXMICoXe90DiXEhdZ7UaW1ItwJeJA6JZs+cIK5PdoFaw+HnjvPa4y2oltuXNhYkE+vkyfoA9g9KXFW0INM6j251QPRwWP2e1EttyXbcmNAgN4tV5u+xdK9i/ARJ+hZ53Q7VaVquxJQnpJ5i96QBje0UR7mbzBoqjDYHGeQTWgD73wu7F2gdRCQT5+zLp4hZs3JfNUjv7IFr8PFQLM5r8NA55c0EiwQF+3NnP/eYNFEcbAo1zibsNatQzHiQah4zq2phGtavZt69g3xrDdUjvyRAYYrUaW7L9wHF+2XKQW3tHUTs4wGo5lUYbAo1zCagOfe6HlOWwe6nVamxJgJ8P91wcw+a0YyzY4XDqjbUsfs5wKNjtDquV2JbXFyQQEuTHbX3cvzYA2hBoXEHXmyGkofFAseMbrw24qkskTetU57X5CRTYKYpZygrYs9SIPhYQbLUaW7I5LZv529O5o29zQqu7j4fR0tCGQON8/IOg3wNGAJPkhVarsSV+vj5MHhTDjoPHmbvtkNVyDJSCRc9BSAOIu9VqNbbl9fkJ1Kru73YeRktDGwKNa+h8E4Q2MR4sulbgkBGxkURHBPP6ggR7xDbevRj2rYS+D+h4AyWwYe9RFu/K4M5+0YS4UbyBC6ENgcY1+AVA/wfhwEZI+M1qNbbE10e495KWJKSf5OfNB6wVo5TRwV+zEXS5yVotNub1+QnUCQ7gpp7uE32sLGhDoHEdnUZD7WZGX0FBgdVqbMllHRrQql4Iby5IJC/fwmOUOB/S1kG/f4Gfe4+JdxVrdmexIimTCQOiCQ70s1qOU9GGQOM6fP2h/0NwaIvhh0hzHj4+wn2DY9ideYof4y2qFShlGOtaTaHzjdZosDlKKV6dn0DdkEC3ikVcVrQh0LiWjtdAnRjDeZmuFTjk0nb1adewJm8uTOScFbWCXXPgYLxhtH09p93bmaxMzmLtniNMHNiCIH9fq+U4HW0INK7FxxcGPAyHt8O27y6c3wsREe4f3JJ9R3L4dkNa1RZeUGD0DYRFQ8drq7ZsN0EpxavzdtEgNIjrujW2Wo5L0IZA43raXQURbWDJi5CfZ7UaW3Jx67p0alyLtxclcSYvv+oK3vEjpG81jLWvZ7V7O4slCRls3JfNpItbEOjnebUB0IZAUxX4+MDARyArEbZ+Y7UaW1JYK9iffZpZ66uoVlCQb7gNj2gN7f9ZNWW6GUopXp+fQKPa1RjV1TNrA1BJQyAiYSIyX0QSzW+HwedFJMUMQBMvIuvLu77GA2h9OdTvYNYKzlmtxpb0iwknrmltpixKIvdcFdQKtn4HmbuM2oCPZ77pVpYFOw6zOe0Y9wyKIcDPc9+bK7tnDwMLlVIxwELzf0kMNIPSxFVwfY074+MDAx+Fo3tg0xdWq7ElIsL9Q1py6Hgun6/Z59rC8vOMDvx67aHNCNeW5aYUFChem59AVJ3qXNU50mo5LqWyhmAE8LH5+2NgZBWvr3EnWg6Fhl1g6cuQd9ZqNbakV3Q4PZqHMXVJMqfPurBWsPkrOJIMAx4xjLTmPH7bdogdB48z+ZIY/Hw9+xhVdu/qFYaXNL/rlpBPAfNEZIOIjKvA+ojIOBFZLyLrMzJs7MddUzIiRq3g2D5Y967VamzLA0NakXnyDO8u3+2aAs7mGLWBBrHQ+jLXlOHmnM0r4JW5u2hRtwZXdPLs2gCUwRCIyAIR2ergU576ZG+lVBdgGDBRRPqVV6hSaoZSKk4pFRcREVHe1TV2ocUgaHGJ0VdwIt1qNbbkoqgwhneoz9QlSaQdzXF+ASteh2OpMPQFwzhrzuOD3/ewO/MUj13WBl8fzz9GFzQESqlLlFLtHXx+BNJFpAGA+e3QubpS6oD5fRj4HuhmJpVpfY0HIQJDX4Jzp2HBk1arsS2PXtYWgOd+2eHcDR/ZDb+/CR2ugaa9nLttD+HgsdO8tTCRwW3rMaBViY0UHkVlm4ZmA2PN32OBH4tnEJFgEQkp/A0MAbaWdX2NBxLeAnpNgk2fG9GwNOcRWasakwa24Neth1ie6MSm0N8eMWYPD37aedv0MJ6fs5P8AsV//tHWailVRmUNwYvAYBFJBAab/xGRhiIyx8xTD1ghIpuAtcAvSqnfSltf4wX0exBqRsKcB4zx7JrzuKNfc6LqVOeJ2ds4m+cE1xO7fjM8wQ54GGo2qPz2PJBVyVn8tOkAEwZE0zisutVyqoxKGQKlVJZSapBSKsb8PmIuP6CUGm7+3q2U6mR+2imlnrvQ+hovICAYhjxrOKTb8KHVamxJoJ8vT1zejt0Zp/jw9z2V29i5XPjtIQhvBd11QHpHnMsv4InZW2lUuxrj+0dbLadK8ewxURp70+5KiOoLC5+BU1lWq7ElA1vX5ZI2dXlrYSKHjuVWfEMr34ajKTDsJe1YrgQ+WbWXhPST/OcfbT3SsVxpaEOgsQ4RGP4ynD0JC5+yWo1t+c8/2nGuQPH8nAp2HGfvg+WvQtsRED3QueI8hMMncnljfgL9W0YwuG09q+VUOdoQaKylbhujqWLjJ7B/g9VqbEmTOtUZ3z+a2ZsOsHp3BWpOcx81jO6Q5y6c10t56dddnMkr4Mkr2iFeOKRWGwKN9fR/CIIjYM6DOmZBCUzoH01krWo88eO28kUyS14EO2YbcYhrea7TtMqwYe8Rvt2Yxu19m9EsPNhqOZagDYHGeoJqwpBnjBpB/KdWq7El1QJ8efwfbdmVfoJPVu0t20p5Z2HO/0FYc+h1t2sFuin5BYrHf9hGg9AgJl3cwmo5lqENgcYedLwWGvcwJpmdPmq1Gltyabt69I0J5/X5CWScOHPhFdZMM1x/D31JxyEugc/X7mP7weM8elkbqgd4bzwGbQg09qCw4/j0USNiluY8RIQnr2hHbl4+L/22s/TMxw/Akpeg1XBoOaRqBLoZR06d5ZW5u+gVXYfLOnj3vAptCDT2oUFHiLsN1r1nzC/QnEd0RA1u69OcbzaksWFvKTWneY9DQR5cqo1qSbw8dyenzuTxlJd2EBdFGwKNvRj4b6hW2+g4VspqNbbk7otbUK9mIE/M3kp+gYNjlLLCiATX514Ia1bl+tyBTanZfLkulZt7RRFTL8RqOZajDYHGXlQPg0FPwL5VsHmW1WpsSXCgH49e1pat+4/zxdpiAWzyzxlGNLQJ9L7XEn12p6BA8Z/Z2wivEcjkS2KslmMLtCHQ2I/OY4wANvMfh9zjVquxJZd3bECP5mG8Mm8XR08VCfKz7j04vN1wMR3gPb5yysPXG1LZlJrNv4e3JiRIz7IGbQg0dsTHBy57BU4ehqUvWa3GlogIT13RnhO5ebw8b5ex8ORho6M9epAOOFMCx3LO8dJvu7goqjYjYz0/4ExZ0YZAY08iu0KXMbBmOhy+wAgZL6VV/RDG9ozii7X72JyWbQy9PXfa8Cfk5Z2fJfHa/F1k55z12hnEJaENgca+DHrC8FL6q+44Lol7B8dQJziQz775BuI/g54TIVy3ezti24FjzFy9lxt7NKVdw1Cr5dgKj5lBce7cOdLS0sjNrYSHRi8mKCiIRo0a4e9vozbT4HC4+HGY8y/Y8jV0vMZqRbajZpA/jw5pTsuf7yenWl2q93vQakm2JC+/gMd/2Eqt6gE8MLiV1XJsh8cYgrS0NEJCQoiKitJVvnKilCIrK4u0tDSaNbPZcMO4W2HzV/DzfUaw9YiWViuyHSMPT0F89nJXzgNMzMqnXUOrFdmPV+YlsHFfNm9cG0todRu97NiESjUNiUiYiMwXkUTzu7aDPK1EJL7I57iI3GumPSki+4ukDa+oltzcXOrUqaONQAUQEerUqWPP2pSPL4z6yHCRMGsMnDlptSJ7sekrZP175MRNYGO13kz4dCPHTp+zWpWtmLvtENOXJnN99yaM7Kw7iB1R2T6Ch4GFSqkYYKH5/28opXYppWKVUrFAVyAHI4B9Ia8Xpiul5hRfvzxoI1BxbH3sQhvB1R9AZgL8NFn3FxSSvs04Hk17U33Ys0y5oQsHsk/zwKx4ChxNNPNC9mSe4l+zNtGxUShPXO49MYjLS2UNwQjgY/P3x8DIC+QfBCQrpcroPlGjMWk+AAY+asyYXTvDajXWk3sMvhpjeG69+gPw9aNr09o8dlkbFuw4zLSlyVYrtJzTZ/OZ8OkGfH2FqTd0IdDPu6KOlYfKGoJ6SqmDAOZ33Qvkvw74otiySSKyWUQ+cNS0VIiIjBOR9SKyPiMjo3KqNe5Jn/uh5TCY+29IXWu1GutQCn64ywg9OeojCKn/Z9LYXlFc0akhr87bxe9JmZZJtBqlFI9+v4Vd6Sd487rONKqtJ9eVxgUNgYgsEJGtDj4jylOQiAQAVwBfF1k8DYgGYoGDwKslra+UmqGUilNKxUVERJSn6CojJSWF9u3bW6ph+PDhZGdnW6rBZfj4wJXTjaaiWWPhpJe+EKx8C3b+DIOfhqa9/pYkIrxwVQeiI2pwzxd/cPDYaYtEWstna/bx3R/7uXdQS/q3tOfzwk5ccNSQUuqSktJEJF1EGiilDopIA+BwKZsaBmxUSqUX2fafv0XkXeDnsskunad+2sb2A851TdC2YU2euLydU7dZFvLy8vDzK/vgrjlzKtXNYn+q1YJrZsL7g+GbW2DMD+DrMYPfLsye5cbEsbYjjDkDDggO9GPajV0Z8c4K7vpsI1+N60mAn/dMGYpPzebpn7YzoFUEd3txsJnyUNmrYzYw1vw9FvixlLyjKdYsZBqPQq4EtlZSj+Xk5+dzxx130K5dO4YMGcLp06eJj4+nR48edOzYkSuvvJKjRw33wQMGDGD9+vUAZGZmEhUVBcBHH33EqFGjuPzyyxkyxLEv+YMHD9KvXz9iY2Np3749y5cvByAqKorMzExSUlJo06bNeVo8ggYd4bLXIGU5LPaiOLzHD8I3t0JYNIyYUurs4RZ1a/DyqE78sS+74kHv3ZAjp85y16cbqFszkDeujcXHx8aDIOyEUqrCH6AOxmihRPM7zFzeEJhTJF91IAsILbb+TGALsBnDqDQoS7ldu3ZVxdm+fft5y6qaPXv2KF9fX/XHH38opZQaNWqUmjlzpurQoYNasmSJUkqpxx9/XE2ePFkppVT//v3VunXrlFJKZWRkqKZNmyqllPrwww9VZGSkysrKKrGsV155RT377LNKKaXy8vLU8ePHlVJKNW3aVGVkZJSopTTscAzLxex7lHqiplI7frZaievJO6vUe0OUeraBUuk7yrzaMz9tU00f+ln98EeaC8XZg7z8AnXje6tVzKNz1ObUbKvl2BJgvXLwTK1UnVoplYUxEqj48gPA8CL/c0yjUTzfmMqUb0eaNWtGbGwsAF27diU5OZns7Gz69+8PwNixYxk1atQFtzN48GDCwsJKTL/ooou49dZbOXfuHCNHjvyzzNK0pKSklHd37M3Ql+BAPHw/AcYthjrRVityHfOfgNTV8M/3oW7rMq/20LDWbErL5uFvt9CmQU1aerDv/TcXJrI8MZMXr+pAh0bahUR58J6GwyoiMPCv2LC+vr6ldtz6+flRUFAAcN5kruDg4FLL6devH8uWLSMyMpIxY8bwySefXFBLXl5eWXbBffAPgms+MTqRZ90EZ3OsVuQatn0Pq6dAtzuhw9XlWtXf14cp13chONCP8TM3cCLXMyebLd55mLcWJjKqayOuvaix1XLcDm0IXExoaCi1a9f+sw1/5syZf9YOoqKi2LBhAwDffPNNuba7d+9e6tatyx133MFtt93Gxo0bnSvcXajdFK56z5hc9csDnjfZLGMX/DgJGnWDIc9WaBN1awYx5frO7D2Sw/99s7mwWdZjSD2Sw71fxdO2QU2eGdne3pMjbYo2BFXAxx9/zIMPPkjHjh2Jj4/nP//5DwD/+te/mDZtGr169SIzs3xjvpcsWUJsbCydO3fm22+/ZfLkya6Q7h7EXAL9H4JNn8OGj6xW4zzOnDQmjfkFmW42Aiq8qe7N6/Dw0Nb8uvUQ76/Y4zyNFpN7Lp8Jn21AKcX0G7sS5K8njVUEcce3g7i4OFU42qaQHTt20KZNG4sUeQZufQwLCuDzUbBnGdz6mxHPwJ1RyhghtP0HY4hs8/5O2KRiwqcbmb8jnc9v70735ud127kdD3+7mS/XpfL+2DgGtalntRzbIyIblFJxxZfrGoHGM/DxgavehRr1jclmOUesVlQ51vwPtn1nuOF2ghEAY7LZy6M60jSsOpO++IPDx23oZLAczFqXypfrUpk0sIU2ApVEGwKbs2XLFmJjY//26d69u9Wy7En1MLjmYziZDp9eBccPWK2oYmz4COY9Cq0ugz73OXXTIUH+TLuxKydz87jpg7WkHnHPDvYf4/fz2I9b6dMinPsGa9fklUUbApvToUMH4uPj//ZZs2aN1bLsS2QXGPUxZCbCjAHu5ZMo/5zR4f3TZGjWz3Cn4YKOz1b1Q5hxU1f2Z59mxJTfWZWc5fQyXEV+geL5OTuY/GU8sY1r8fbozvjqSWOVRhsCjefRejjcNh/8q8FHl8HGmVYrujAnM+CTEbDuPeh1D9zwjeFZ1EX0jYngx4m9qV3dnxvfX8PHK1NsP5roWM45bvloHTOW7eamnk357Pbu1A6ueAe65i+0IdB4JvXawh2LoWlvmD0J5vyf8cZtRw5ugncHwv4NRj/HkGeMgDwupnlEDX6Y2JuBrSJ4YvY2Hv52C2fy8l1ebkVITD/BiCkrWJWcyQtXdeDpEe3x99WPL2ehj6TGc6keZrxZ95wEa/8HM6+EUzZrBtn6Lbx/KagCY7RTFcdlDgnyZ8aYOO6+uAVfrU9l9IzVtutEnrftECOn/M7JM/l8cUcPRndrYrUkj0MbAo1n4+sHlz4HV/7P6C94dwAc2mK1KijIN7yIfnMrNOgE45ZAw86WSPHxER4Y0oqpN3Rhx8ETXP7OCuJTsy3RUpSCAsVbCxMZN3MD0XVr8NPdvYmLKtntiqbiaEPgROwQj0BTAp2ug1t/hfw8eH+I4bbBKnKPwRfXwYrXoevNMPYnqHGhmE6uZ3iHBnw7oRf+vj5c879VfLshzTItp87kMfHzjbw2P4ErO0cy686eNAitZpkeT8czHbn/+rDz3/rqd4BhLzp3m2WgvPEINKUQ2dV48541Br6+GQ5tNcJf+lTh+1BGAnw52ogudtlrcNFtVVd2GWjbsCazJ/Vh4mcbeeDrTWw/eJxHhrXGrwrb4/dl5TBu5noS0k/w2GVtuK1PM+02wsXoGoGTqap4BAD//e9/6dChA506deLhhx8GKLGst956i7Zt29KxY0euu+46Fx4BmxNSz3gD7zwGlr9iPJRzj1VN2Qlz4b1BcDobbpptOyNQSFhwAJ/c1o2be0Xx/oo93PzhOrJzzlZJ2b8nZXLFlBUcPJbLx7d24/a+zbURqAoc+aa2+0fHI1Bqzpw5qmfPnurUqVNKKfVn3pLKatCggcrNzVVKKXX06FGH27TDMawyCgqUWjNDqafClHo7Tql0F+77uTNKLX1ZqSdClZrWR6mj+1xXlpP5at0+FfPvOarvS4vUljTX+fg/m5ev3lu+WzV/5Bd1yatL1J6Mky4ry5vBFfEINOdTVfEIFixYwC233EL16kZQ7rCwMI4dO1ZiWR07duSGG25g5MiRjBw5shJ76CGIQLc7IKI1fD0WpvaA8JYQfTFED4Ko3hBQuivwElEKjuyGpIWQvNAIL3nuFLT/J1zxDgS4TyD1a+Ia06JuDcbP3MA/3l5BVJ3q9GsZQb+YCHpG1yE4sOKPkH1ZOSxNzGB5QgarkrM4cSaPIW3r8dq1sdSoxHY15adSR1tERgFPAm2Abkqp9SXkGwq8CfgC7ymlXjSXhwFfAVFACnCNUupoZTRZTVXFI1BKlavK/Msvv7Bs2TJmz57NM888w7Zt23TfA0CzvjBhJWz9znhob/gI1kwH3wBo0tMwDC0GQb32pc/yzT1mOLxLWgjJiyB7r7G8djOIHQ0xQ4yPGzZzdGlSmzmT+/LzpgMsS8zk6/VpfLJqL/6+QpcmtenXMoL+LSNo26BmqaEhT57JY3VyFssSM1iWkEFKluHeIrJWNf7RqSEDW0VwSZt6OrykBVT2SbAVuAr4X0kZRMQXmAIMBtKAdSIyWym1HXgYWKiUelFEHjb/P1RJTbaiaDyCvn37OoxH0K1bt3LHIxgyZAhPP/00119/PdWrV+fIkSOEhYU5LKugoIDU1FQGDhxInz59+Pzzzzl58iS1atVywR67ISH1oeddxudcLuxb+dcDfcETxqdGvb9qC80HGHMUDsQbeZIXGkNTVT4E1DDcQ/S62zAgYc2t3junEF4jkJt7N+Pm3s04k5fPhpSj5tt8Ji/P3cXLc3dRJziAPjHh9IuJoG/LcMKDA9l+8DhLE4wH/8Z9RzmXr6jm70vP6Drc3CuKfi0jaBYerPsBLKayoSp3ABc6id2AJKXUbjPvl8AIYLv5PcDM9zGwBA8zBGDEIxg/fjw5OTk0b96cDz/8EDDiEVxzzTXMnDmTiy++uFzbHDp0KPHx8cTFxREQEMDw4cN5/vnnHZaVn5/PjTfeyLFjx1BKcd9992kjUBL+QeYD3zwfxw8YD/ukhZDwG2z6wlgeWBPOHDd+N4iF3pONB3+jbpWKG+AOBPr50qtFOL1ahPPIMDh8IpcViZksS8hgeWImP8Ybzv5Cgvw4kWtExWvToCa39mlG/5gIukbVJtBPxw2wE06JRyAiS4B/OWoaEpGrgaFKqdvN/2OA7kqpSSKSrZSqVSTvUaVU7RLKGAeMA2jSpEnXvXv3/i3drX3p2wR9DC9AQT4cjIekRXAsFaL6QvRACA63WpltKChQf9YC9mXl0K1ZGH1bhlM3JMhqaRpKjkdwwRqBiCwA6jtIelQp9WNZynawrNzWRyk1A5gBRmCa8q6v0VQaH19jLoK7B71xIT4+QvvIUNpH6uDx7sQFDYFS6pJKlpEGFI0m3QgodBSfLiINlFIHRaQBcLiSZXkcW7ZsYcyYMX9bFhgYqF1RazQap1EVw0bWATEi0gzYD1wHXG+mzQbGAi+a32WpYZRIeUfSuAOF8QhcjTOaCDUajXtSqZnFInKliKQBPYFfRGSuubyhiMwBUErlAZOAucAOYJZSapu5iReBwSKSiDGqqMI+HIKCgsjKytIPtAqglCIrK4ugIN2Oq9F4Ix4TvP7cuXOkpaWdNx5fUzaCgoJo1KgR/v7+VkvRaDQuosKdxe6Cv78/zZo1s1qGRqPRuB3a6ZxGo9F4OdoQaDQajZejDYFGo9F4OW7ZWSwiGcDeC2Z0TDiQ6UQ57oDeZ+9A77N3UJl9bqqUiii+0C0NQWUQkfWOes09Gb3P3oHeZ+/AFfusm4Y0Go3Gy9GGQKPRaLwcbzQEM6wWYAF6n70Dvc/egdP32ev6CDQajUbzd7yxRqDRaDSaImhDoNFoNF6OVxkCERkqIrtEJMmMkezxiEiKiGwRkXgROS+CnCcgIh+IyGER2VpkWZiIzBeRRPPbYeQ7d6WEfX5SRPab5zpeRIZbqdGZiEhjEVksIjtEZJuITDaXe+x5LmWfnX6evaaPQER8gQQMd9dpGHESRiultlsqzMWISAoQp5Ty2Ek3ItIPOAl8opRqby77L3BEKfWiafRrK6U8Jh52Cfv8JHBSKfWKldpcgRm4qoFSaqOIhAAbgJHAzXjoeS5ln6/ByefZm2oE3YAkpdRupdRZ4EtghMWaNE5AKbUMOFJs8QjgY/P3xxg3kMdQwj57LEqpg0qpjebvExixTSLx4PNcyj47HW8yBJFAapH/abjooNoMBcwTkQ0iMs5qMVVIPaXUQTBuKKCuxXqqikkistlsOvKYZpKiiEgU0BlYg5ec52L7DE4+z95kCBzFsPSGdrHeSqkuwDBgotmkoPFMpgHRQCxwEHjVUjUuQERqAN8C9yqljlutpypwsM9OP8/eZAjSgMZF/jcCDlikpcpQSh0wvw8D32M0kXkD6WYba2Fb62GL9bgcpVS6UipfKVUAvIuHnWsR8cd4IH6mlPrOXOzR59nRPrviPHuTIVgHxIhIMxEJAK4DZlusyaWISLDZyYSIBANDgK2lr+UxzAbGmr/HAj9aqKVKKHwgmlyJB51rERHgfWCHUuq1Ikkee55L2mdXnGevGTUEYA6zegPwBT5QSj1nrSLXIiLNMWoBYIQl/dwT91lEvgAGYLjnTQeeAH4AZgFNgH3AKKWUx3SulrDPAzCaCxSQAtxZ2H7u7ohIH2A5sAUoMBf/G6PN3CPPcyn7PBonn2evMgQajUajOR9vahrSaDQajQO0IdBoNBovRxsCjUaj8XK0IdBoNBovRxsCjUaj8XK0IdBoyoCIRBX19KnReBLaEGg0FiEiflZr0GhAGwKNpjz4isi7pm/4eSJSTURiRWS16QDs+0IHYCKyRETizN/hpjtwRORmEflaRH4C5lm3KxrNX2hDoNGUnRhgilKqHZAN/BP4BHhIKdURYwboE2XYTk9grFLqYlcJ1WjKgzYEGk3Z2aOUijd/b8DwAFlLKbXUXPYxUBbvrvM9xQ2CxjPQhkCjKTtnivzOB2qVkjePv+6voGJpp5yoSaOpNNoQaDQV5xhwVET6mv/HAIW1gxSgq/n76irWpdGUCz1qQaOpHGOB6SJSHdgN3GIufwWYJSJjgEVWidNoyoL2PqrRaDRejm4a0mg0Gi9HGwKNRqPxcrQh0Gg0Gi9HGwKNRqPxcrQh0Gg0Gi9HGwKNRqPxcrQh0Gg0Gi/n/wHrvAWUCNZBDAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html#sphx-glr-auto-examples-applications-plot-cyclical-feature-engineering-py\n",
    "\n",
    "#Trigonometric features\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def sin_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
    "\n",
    "def cos_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "hour_df = pd.DataFrame(\n",
    "    np.arange(26).reshape(-1, 1),\n",
    "    columns=[\"hour\"],\n",
    ")\n",
    "\n",
    "hour_df\n",
    "\n",
    "hour_df[\"hour_sin\"] = sin_transformer(24).fit_transform(hour_df)[\"hour\"]\n",
    "hour_df[\"hour_cos\"] = cos_transformer(24).fit_transform(hour_df)[\"hour\"]\n",
    "hour_df.plot(x=\"hour\")\n",
    "_ = plt.title(\"Trigonometric encoding for the 'hour' feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675848f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2022/06/time-series-forecasting-using-python/\n",
    "\n",
    "train_original=train.copy()\n",
    "test_original=test.copy()\n",
    "train['Datetime'] = pd.to_datetime(train.Datetime,format='%d-%m-%Y %H:%M')\n",
    "for i in (train, test, test_original, train_original):\n",
    "    i['year']=i.Datetime.dt.year \n",
    "    i['month']=i.Datetime.dt.month \n",
    "    i['day']=i.Datetime.dt.day    \n",
    "\n",
    "#extract the day of the week from Datetime\n",
    "#assign 1 if the day of the week is a weekend and 0 if the day of the week in not a weekend.\n",
    "\n",
    "def applyer(row):\n",
    "    if row.dayofweek == 5 or row.dayofweek == 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "temp2 = train['Datetime'].apply(applyer) \n",
    "train['weekend']=temp2\n",
    "\n",
    "#EDA\n",
    "train.groupby('year')['Count'].mean().plot.bar()\n",
    "train.groupby('month')['Count'].mean().plot.bar()\n",
    "temp=train.groupby(['year', 'month'])['Count'].mean() \n",
    "temp.plot(figsize=(15,5), title= 'Passenger Count(Monthwise)', fontsize=14)\n",
    "train.groupby('day')['Count'].mean().plot.bar()\n",
    "train.groupby('Hour')['Count'].mean().plot.bar()\n",
    "train.groupby('weekend')['Count'].mean().plot.bar()\n",
    "train.groupby('day of week')['Count'].mean().plot.bar()\n",
    "\n",
    "#As we have seen that there is a lot of noise in the hourly time series, we will aggregate the hourly time series into daily, weekly, and monthly time #series to reduce the noise and make it more stable and hence would be easier for a model to learn.\n",
    "\n",
    "train.Timestamp = pd.to_datetime(train.Datetime, format='%d-%m-%Y %H:%M') \n",
    "train.index = train.Timestamp \n",
    "hourly = train.resample('H').mean() \n",
    "daily = train.resample('D').mean() \n",
    "weekly = train.resample('W').mean() \n",
    "monthly = train.resample('M').mean()\n",
    "\n",
    "#Let’s look at the hourly, daily, weekly, and monthly time series.\n",
    "fig, axs = plt.subplots(4,1) \n",
    "hourly.Count.plot(figsize=(15,8), title= 'Hourly', fontsize=14, ax=axs[0])\n",
    "daily.Count.plot(figsize=(15,8), title= 'Daily', fontsize=14, ax=axs[1])\n",
    "weekly.Count.plot(figsize=(15,8), title= 'Weekly', fontsize=14, ax=axs[2]) \n",
    "monthly.Count.plot(figsize=(15,8), title= 'Monthly', fontsize=14, ax=axs[3]) \n",
    "plt.show()\n",
    "\n",
    "#it would be difficult to convert the monthly and weekly predictions to hourly predictions, as first we have to convert the monthly predictions to #weekly, weekly to daily, and daily to hourly predictions, which will become a very expanded process. So, we will work on the daily time series.\n",
    "test.Timestamp = pd.to_datetime(test.Datetime,format='%d-%m-%Y %H:%M') \n",
    "test.index = test.Timestamp \n",
    "test = test.resample('D').mean() \n",
    "train.Timestamp = pd.to_datetime(train.Datetime,format='%d-%m-%Y %H:%M')\n",
    " train.index = train.Timestamp  \n",
    "train = train.resample('D').mean()\n",
    "\n",
    "#Modeling Techniques and Evaluation\n",
    "#1) Splitting the data into the training and validation part\n",
    "Train=train.ix['2012-08-25':'2014-06-24'] valid=train.ix['2014-06-25':'2014-09-25']\n",
    "\n",
    "Train.Count.plot(figsize=(15,8), title= 'Daily Ridership', fontsize=14, label='train') \n",
    "valid.Count.plot(figsize=(15,8), title= 'Daily Ridership', fontsize=14, label='valid') \n",
    "plt.xlabel(\"Datetime\") plt.ylabel(\"Passenger count\") \n",
    "plt.legend(loc='best') plt.show()\n",
    "\n",
    "#models for Time Series Forecasting.\n",
    "\n",
    "# Naive Approach - next expected point is equal to the last observed point\n",
    "dd= np.asarray(Train.Count) \n",
    "y_hat = valid.copy() \n",
    "y_hat['naive'] = dd[len(dd)-1] \n",
    "plt.figure(figsize=(12,8)) \n",
    "plt.plot(Train.index, Train['Count'], label='Train') \n",
    "plt.plot(valid.index,valid['Count'], label='Valid') \n",
    "plt.plot(y_hat.index,y_hat['naive'], label='Naive Forecast') \n",
    "plt.legend(loc='best') \n",
    "plt.title(\"Naive Forecast\") \n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from math import sqrt \n",
    "rms = sqrt(mean_squared_error(valid.Count, y_hat.naive)) \n",
    "print(rms)\n",
    "\n",
    "# Moving Average - take the average of the passenger counts for the last few time periods only\n",
    "#rolling mean for the last 10, 20, and 50 days and visualize the results.\n",
    "\n",
    "y_hat_avg = valid.copy() \n",
    "y_hat_avg['moving_avg_forecast'] = Train['Count'].rolling(10).mean().iloc[-1] \n",
    "#y_hat_avg['moving_avg_forecast'] = Train['Count'].rolling(20).mean().iloc[-1] \n",
    "#y_hat_avg['moving_avg_forecast'] = Train['Count'].rolling(50).mean().iloc[-1] \n",
    "plt.figure(figsize=(15,5)) \n",
    "plt.plot(Train['Count'], label='Train') \n",
    "plt.plot(valid['Count'], label='Valid') \n",
    "plt.plot(y_hat_avg['moving_avg_forecast'], \n",
    "                label='Moving Average Forecast using 10 observations')\n",
    "plt.legend(loc='best') \n",
    "plt.show()\n",
    "\n",
    "rms = sqrt(mean_squared_error(valid.Count, y_hat_avg.moving_avg_forecast)) \n",
    "print(rms)\n",
    "\n",
    "# Simple Exponential Smoothing - we assign larger weights to more recent observations than to observations from the distant past.\n",
    "#The weights decrease exponentially as observations come from further in the past, the smallest weights are associated with the oldest observations.\n",
    "from statsmodels.tsa.api \n",
    "import ExponentialSmoothing, SimpleExpSmoothing, Holt \n",
    "y_hat_avg = valid.copy() \n",
    "fit2 = SimpleExpSmoothing(np.asarray(Train['Count'])).fit(smoothing_level=0.6,\n",
    "        optimized=False) y_hat_avg['SES'] = fit2.forecast(len(valid))\n",
    "plt.figure(figsize=(16,8))  \n",
    "plt.plot(Train['Count'], label='Train') \n",
    "plt.plot(valid['Count'], label='Valid') \n",
    "plt.plot(y_hat_avg['SES'], label='SES') \n",
    "plt.legend(loc='best') \n",
    "plt.show()\n",
    "\n",
    "rms = sqrt(mean_squared_error(valid.Count, y_hat_avg.SES)) \n",
    "print(rms)\n",
    "\n",
    "# Holt’s Linear Trend Model - It is an extension of simple exponential smoothing to allow forecasting of data with a trend.\n",
    "import statsmodels.api as sm \n",
    "sm.tsa.seasonal_decompose(Train.Count).plot() \n",
    "result = sm.tsa.stattools.adfuller(train.Count) \n",
    "plt.show()\n",
    "\n",
    "y_hat_avg = valid.copy() \n",
    "fit1 = Holt(np.asarray(Train['Count'])).fit(smoothing_level = 0.3,\n",
    "        smoothing_slope = 0.1) y_hat_avg['Holt_linear'] = fit1.forecast(len(valid)) \n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(Train['Count'], label='Train')\n",
    "plt.plot(valid['Count'], label='Valid') \n",
    "plt.plot(y_hat_avg['Holt_linear'], label='Holt_linear') \n",
    "plt.legend(loc='best') \n",
    "plt.show()\n",
    "\n",
    "rms = sqrt(mean_squared_error(valid.Count, y_hat_avg.Holt_linear)) \n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce07d1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "730\n",
      "           Births\n",
      "count  365.000000\n",
      "mean    41.980822\n",
      "std      7.348257\n",
      "min     23.000000\n",
      "25%     37.000000\n",
      "50%     42.000000\n",
      "75%     46.000000\n",
      "max     73.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 730 into shape (365,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25732/1056537914.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m#convert data into matrix of row-col vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;31m# feature scaling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 730 into shape (365,1)"
     ]
    }
   ],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2022/05/a-comprehensive-guide-to-time-series-analysis-and-forecasting/ 5*\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import statmodels\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data_path = 'D:\\\\timeseries\\\\testing_data\\\\'\n",
    "data = pd.read_csv(data_path+'daily-total-female-births-in-cal.csv', parse_dates = True, header = 0, squeeze=True)\n",
    "\n",
    "\n",
    "print(data.size) #output-365\n",
    "print(data.describe())\n",
    "\n",
    "data.head()\n",
    "data.hist()\n",
    "plt.show()\n",
    "\n",
    "# plt.plot(data)\n",
    "# plt.show()\n",
    "\n",
    "#Scale and Normalize Time Series Data for Further Modelling\n",
    "#convert data into matrix of row-col vectors\n",
    "values = data.values\n",
    "values = values.reshape((len(values), 1))\n",
    "# feature scaling\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#fit the scaler with the train data to get min-max values\n",
    "scaler = scalar.fit(values)\n",
    "print('Min: %f, Max: %f' % (scaler.data_min_, scaler.data_max_))\n",
    "# normalize the data and sanity check\n",
    "normalized = scaler.transform(values)\n",
    "for i in range(5):\n",
    "    print(normalized[i])\n",
    "# inverse transform to obtain original values\n",
    "original_matrix= scaler.inverse_transform(normalized)\n",
    "for i in range(5):\n",
    "    print(original_matrix[i])\n",
    "\n",
    "#Extracting Useful Features from Time-Series Data (Feature Engineering)\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "df = DataFrame(data.values)\n",
    "tshifts = df.shift(1)\n",
    "rwin = tshifts.rolling(window=2)\n",
    "moving_avg = rwin.mean()\n",
    "joined_df = concat([moving_avg, df], axis=1)\n",
    "joined_df.columns = ['mean(t-2,t-1)', 't+1']\n",
    "print(joined_df.head(5))\n",
    "\n",
    "#Expanding Window Statistics\n",
    "window = tshifts.expanding()\n",
    "joined_df2 = concat([rwin.mean(),df.shift(-1)], axis=1)\n",
    "joined_df2.columns = ['mean', 't+1']\n",
    "print(joined_df2.head(5))\n",
    "\n",
    "#Time-series Data Stationary Checks\n",
    "X = data.values\n",
    "seq = round(len(X) / 2)\n",
    "x1, x2 = X[0:seq], X[seq:]\n",
    "meanx1, meanx2 = x1.mean(), x2.mean()\n",
    "varx1, varx2 = x1.var(), x2.var()\n",
    "print('meanx1=%f, meanx2=%f' % (meanx1, meanx2))\n",
    "print('variancex1=%f, variancex2=%f' % (varx1, varx2))\n",
    "\n",
    "#ARMA, ARIMA, and SARIMAX Models for Time-Series Forecasting\n",
    "\n",
    "#1 AR model\n",
    "AR_model = ARIMA(indexedDataset_logScale, order=(2,1,0))\n",
    "AR_results = AR_model.fit(disp=-1)\n",
    "plt.plot(datasetLogDiffShifting)\n",
    "plt.plot(AR_results.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'%sum((AR_results.fittedvalues - datasetLogDiffShifting['#Passengers'])**2))\n",
    "\n",
    "#2MA Model\n",
    "MA_model = ARIMA(indexedDataset_logScale, order=(0,1,2))\n",
    "MA_results = MA_model.fit(disp=-1)\n",
    "plt.plot(datasetLogDiffShifting)\n",
    "plt.plot(MA_results.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'%sum((MA_results.fittedvalues - datasetLogDiffShifting['#Passengers'])**2))\n",
    "\n",
    "#3ARIMA\n",
    "ARIMA_model = ARIMA(indexedDataset_logScale, order=(2,1,2))\n",
    "ARIMA_results = ARIMA_model.fit(disp=-1)\n",
    "plt.plot(datasetLogDiffShifting)\n",
    "plt.plot(ARIMA_results.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'%sum((ARIMA_results.fittedvalues - datasetLogDiffShifting['#Passengers'])**2))\n",
    "\n",
    "#4 SARIMAX\n",
    "'''\n",
    "1. Seasonal Autoregressive Component\n",
    "2. Seasonal Moving Average Component\n",
    "3. Seasonal Integrity Order Component\n",
    "4. Seasonal Periodicity\n",
    "'''\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "SARIMAX_model=SARIMAX(train['#Passengers'],order=(1,1,1),seasonal_order=(1,0,0,12))\n",
    "SARIMAX_results=SARIMAX_model.fit()\n",
    "preds=SARIMAX_results.predict(start,end,typ='levels').rename('SARIMAX Predictions')\n",
    "test['#Passengers'].plot(legend=True,figsize=(8,5))\n",
    "preds.plot(legend=True)\n",
    "\n",
    "#5 NN Models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing\n",
    "from sklearn.metrics import r2_score\n",
    "from keras.layers import Dense, Dropout, SimpleRNN, LSTM\n",
    "from keras.models import Sequential\n",
    "\n",
    "minmax_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "data['Passengers'] = minmax_scaler.fit_transform(data['Passengers'].values.reshape(-1,1))\n",
    "data.head()\n",
    "\n",
    "#Train, test splits (80–20 ratio) — \n",
    "split = int(len(data[‘Passengers’])*0.8)\n",
    "x_train,y_train,x_test,y_test = np.array(x[:split]),np.array(y[:split]),\n",
    "np.array(x[split:]), np.array(y[split:])\n",
    "#reshaping data to original shape\n",
    "x_train = np.reshape(x_train, (split, 20, 1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 20, 1))\n",
    "\n",
    "#5.1 RNN Model — \n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(40, activation=\"tanh\", return_sequences=True, input_shape=(x_train.shape[1],1)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(SimpleRNN(50, return_sequences=True, activation=\"tanh\"))\n",
    "model.add(Dropout(0.1)) #remove overfitting\n",
    "model.add(SimpleRNN(10, activation=\"tanh\"))\n",
    "model.add(Dense(1))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"MSE\")\n",
    "model.fit(x_train, y_train, epochs=15, batch_size=50)\n",
    "preds = model.predict(x_test)\n",
    "\n",
    "# 5.2 LSTM Model — \n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation=\"ReLU\", return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(80, activation=\"ReLU\", return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50, activation=\"ReLU\", return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(30, activation=\"ReLU\"))\n",
    "model.add(Dense(1))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"MSE\")\n",
    "model.fit(x_train, y_train, epochs=15, batch_size=50)\n",
    "preds = model.predict(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee41e0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#smoothing Technique\n",
    "# https://medium.com/@srv96/smoothing-techniques-for-time-series-data-91cccfd008a2#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6Ijg2MTY0OWU0NTAzMTUzODNmNmI5ZDUxMGI3Y2Q0ZTkyMjZjM2NkODgiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJuYmYiOjE2NTEwNTIzODcsImF1ZCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsInN1YiI6IjExMzg1MTA1MzE4MzkyMTM3MTMzMyIsImVtYWlsIjoiYWpheXZlcm1hMjNAZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImF6cCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsIm5hbWUiOiJBamF5IFZlcm1hIiwicGljdHVyZSI6Imh0dHBzOi8vbGgzLmdvb2dsZXVzZXJjb250ZW50LmNvbS9hLS9BT2gxNEdoWDVTS1poYTNIc2ppMkxGRXotcXgwQVh5amw5Wm9kcGowUjNCSkxBPXM5Ni1jIiwiZ2l2ZW5fbmFtZSI6IkFqYXkiLCJmYW1pbHlfbmFtZSI6IlZlcm1hIiwiaWF0IjoxNjUxMDUyNjg3LCJleHAiOjE2NTEwNTYyODcsImp0aSI6IjkwZTE2YzgyMTg1YzJjMDUxM2RlNzQ1ZjlmOGM0MmViNmViODNkNTUifQ.SVWUtkftoOT4qlHenzcf2oWG1N2Os7nhDCcL5xgBKw9jDTOnN6jRWVzVY0gbffbGR1n16OIFVUQ9Ra4Dg-H3Ef72V9n8_QpvtQ4HvrGFYmFNGd5xiAusoHaoqISNhLtX5f2SRFaXMFqgqTfXChXm4_cWLDVBy2MbjAi7kI1sWGnuz7zGgK-VIlcyoUlhh5QdX3AG7rRHJk388Xorwd5IPfyGCrq1tTb1cz3ybwU32xPr1K2y7beUhEcbD--IHISH5VItSzRnygVxMMEGyW47wQma5AeeEiUONk3M-V77iCySQlqV0aO5_a4G68rTKvJcbGS5y788xFf9azRFbaWIEA\n",
    "def moving_avarage_smoothing(X,k):\n",
    "\tS = np.zeros(X.shape[0])\n",
    "\tfor t in range(X.shape[0]):\n",
    "\t\tif t < k:\n",
    "\t\t\tS[t] = np.mean(X[:t+1])\n",
    "\t\telse:\n",
    "\t\t\tS[t] = np.sum(X[t-k:t])/k\n",
    "\treturn S\n",
    "\n",
    "def double_exponential_smoothing(X,α,β):\n",
    "\tS,A,B = (np.zeros( X.shape[0] ) for i in range(3))\n",
    "\tS[0] = X[0]\n",
    "\tB[0] = X[1] - X[0]\n",
    "\tfor t in range(1,X.shape[0]):\n",
    "\t\tA[t] = α * X[t] + (1- α) * S[t-1]\n",
    "\t\tB[t] = β * (A[t] - A[t-1]) + (1 - β) * B[t-1]\n",
    "\t\tS[t] = A[t] + B[t]\n",
    "\treturn S\n",
    "\n",
    "def triple_exponential_smoothing(X,L,α,β,γ,ϕ):\n",
    "\n",
    "\tdef sig_ϕ(ϕ,m):\n",
    "\t\treturn np.sum(np.array([np.power(ϕ,i) for i in range(m+1)]))\n",
    "\n",
    "\tC, S, B, F = (np.zeros( X.shape[0] ) for i in range(4))\n",
    "\tS[0], F[0] = X[0], X[0]\n",
    "\tB[0] = np.mean( X[L:2*L] - X[:L] ) / L\n",
    "\tm = 12\n",
    "\tsig_ϕ = sig_ϕ(ϕ,m)\n",
    "\tfor t in range(1, X.shape[0]):\n",
    "\t\tS[t] = α * (X[t] - C[t % L]) + (1 - α) * (S[t-1] + ϕ * B[t-1])\n",
    "\t\tB[t] = β * (S[t] - S[t-1]) + (1-β) * ϕ * B[t-1]\n",
    "\t\tC[t % L] = γ * (X[t] - S[t]) + (1 - γ) * C[t % L]\n",
    "\t\tF[t] = S[t] + sig_ϕ * B[t] + C[t % L]\n",
    "\treturn S\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abcbd428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isstationary:True, isinvertible:True, arroots:[1.5-1.32287566j 1.5+1.32287566j]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TS_0</th>\n",
       "      <th>TS_1</th>\n",
       "      <th>TS_2</th>\n",
       "      <th>TS_3</th>\n",
       "      <th>TS_4</th>\n",
       "      <th>TS_5</th>\n",
       "      <th>TS_6</th>\n",
       "      <th>TS_7</th>\n",
       "      <th>TS_8</th>\n",
       "      <th>TS_9</th>\n",
       "      <th>TS_10</th>\n",
       "      <th>TS_11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.394072</td>\n",
       "      <td>0.614350</td>\n",
       "      <td>0.341411</td>\n",
       "      <td>0.225611</td>\n",
       "      <td>0.236063</td>\n",
       "      <td>-0.708100</td>\n",
       "      <td>-0.300582</td>\n",
       "      <td>0.667089</td>\n",
       "      <td>0.304429</td>\n",
       "      <td>0.720835</td>\n",
       "      <td>0.796007</td>\n",
       "      <td>1.734534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.076742</td>\n",
       "      <td>0.427964</td>\n",
       "      <td>-0.316885</td>\n",
       "      <td>0.455849</td>\n",
       "      <td>-0.848778</td>\n",
       "      <td>-0.208401</td>\n",
       "      <td>-0.318657</td>\n",
       "      <td>0.225593</td>\n",
       "      <td>-1.234162</td>\n",
       "      <td>1.597803</td>\n",
       "      <td>-0.576539</td>\n",
       "      <td>1.972715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.192466</td>\n",
       "      <td>-0.271959</td>\n",
       "      <td>0.397511</td>\n",
       "      <td>-0.306026</td>\n",
       "      <td>0.031103</td>\n",
       "      <td>0.503261</td>\n",
       "      <td>1.589360</td>\n",
       "      <td>-0.678936</td>\n",
       "      <td>-1.544253</td>\n",
       "      <td>1.251726</td>\n",
       "      <td>-1.187012</td>\n",
       "      <td>2.497354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.871188</td>\n",
       "      <td>0.738759</td>\n",
       "      <td>0.048153</td>\n",
       "      <td>0.256468</td>\n",
       "      <td>-0.687391</td>\n",
       "      <td>0.468587</td>\n",
       "      <td>1.249167</td>\n",
       "      <td>0.829319</td>\n",
       "      <td>1.570098</td>\n",
       "      <td>-0.040154</td>\n",
       "      <td>-1.664391</td>\n",
       "      <td>2.070954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.420852</td>\n",
       "      <td>-0.508070</td>\n",
       "      <td>0.229769</td>\n",
       "      <td>0.161890</td>\n",
       "      <td>-0.081185</td>\n",
       "      <td>-1.227538</td>\n",
       "      <td>-0.094799</td>\n",
       "      <td>0.751147</td>\n",
       "      <td>-1.422480</td>\n",
       "      <td>-0.293196</td>\n",
       "      <td>-2.033684</td>\n",
       "      <td>0.107879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TS_0      TS_1      TS_2      TS_3      TS_4      TS_5      TS_6  \\\n",
       "0  1.394072  0.614350  0.341411  0.225611  0.236063 -0.708100 -0.300582   \n",
       "1 -1.076742  0.427964 -0.316885  0.455849 -0.848778 -0.208401 -0.318657   \n",
       "2 -0.192466 -0.271959  0.397511 -0.306026  0.031103  0.503261  1.589360   \n",
       "3 -0.871188  0.738759  0.048153  0.256468 -0.687391  0.468587  1.249167   \n",
       "4  0.420852 -0.508070  0.229769  0.161890 -0.081185 -1.227538 -0.094799   \n",
       "\n",
       "       TS_7      TS_8      TS_9     TS_10     TS_11  \n",
       "0  0.667089  0.304429  0.720835  0.796007  1.734534  \n",
       "1  0.225593 -1.234162  1.597803 -0.576539  1.972715  \n",
       "2 -0.678936 -1.544253  1.251726 -1.187012  2.497354  \n",
       "3  0.829319  1.570098 -0.040154 -1.664391  2.070954  \n",
       "4  0.751147 -1.422480 -0.293196 -2.033684  0.107879  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_process.ArmaProcess.html\n",
    "# TS Synthetic Data\n",
    "\n",
    "import pandas as pd\n",
    "'''\n",
    "arroots: Roots of autoregressive lag-polynomial\n",
    "isinvertible: Arma process is invertible if MA roots are outside unit circle.\n",
    "isstationary: Arma process is stationary if AR roots are outside unit circle.\n",
    "maroots: Roots of moving average lag-polynomial\n",
    "\n",
    "'''\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "np.random.seed(12345)\n",
    "arparams = np.array([.75, -.25])\n",
    "maparams = np.array([.65, .35])\n",
    "ar = np.r_[1, -arparams] # add zero-lag and negate\n",
    "ma = np.r_[1, maparams] # add zero-lag\n",
    "arma_process = sm.tsa.ArmaProcess(ar, ma)\n",
    "print(f'isstationary:{arma_process.isstationary}, isinvertible:{arma_process.isinvertible}, arroots:{arma_process.arroots}')\n",
    "y = arma_process.generate_sample(250)\n",
    "#model = sm.tsa.ARIMA(y, (2, 0, 2), trend='n').fit(disp=0)\n",
    "model = sm.tsa.ARIMA(y, (2, 0, 2)).fit(disp=0)\n",
    "model.params\n",
    "\n",
    "\n",
    "#https://medium.com/wwblog/time-series-clustering-based-on-autocorrelation-using-python-94d5e3475179\n",
    "\n",
    "processes = [\n",
    "    arma_process.from_coeffs([], []),       # White noise\n",
    "    arma_process.from_coeffs([-0.5], []),   # AR(1) \n",
    "    arma_process.from_coeffs([], [0.9]),    # MA(1)\n",
    "    arma_process.from_coeffs([0.5], [0.5])  # ARMA(1, 1)\n",
    "]\n",
    "\n",
    "# Number of time series per process\n",
    "series_per_process = 3\n",
    "# Length of an individual time series\n",
    "T = 1_000\n",
    "n_processes = len(processes)\n",
    "n_series = n_processes * series_per_process\n",
    "# Generate the time series.\n",
    "df = pd.DataFrame()\n",
    "for i in range(n_series):\n",
    "    df[f\"TS_{i}\"] = processes[i // series_per_process] \\\n",
    "        .generate_sample(T)\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3aacea9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression took 3.2 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2022-04-18 12:00:01-05:30    0.038616\n",
       "2022-04-18 12:00:03-05:30    0.547338\n",
       "2022-04-18 12:00:04-05:30    0.449986\n",
       "2022-04-18 12:00:08-05:30    0.587789\n",
       "2022-04-18 12:00:10-05:30    1.028504\n",
       "                               ...   \n",
       "2022-04-19 15:46:31-05:30   -0.740375\n",
       "2022-04-19 15:46:34-05:30   -0.626330\n",
       "2022-04-19 15:46:36-05:30   -0.422997\n",
       "2022-04-19 15:46:37-05:30   -0.205155\n",
       "2022-04-19 15:46:39-05:30   -0.085990\n",
       "Length: 50001, dtype: float32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://medium.com/@edwinsutrisno/midimax-data-compression-for-large-time-series-data-daf744c89310\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Midimax Data Compression for Time-Series\n",
    "Author: Edwin Sutrisno\n",
    "Changes:\n",
    "    April 16, 2022: First version (Edwin Sutrisno).\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def compress_series(inputser: pd.Series, compfactor=2):\n",
    "    \"\"\"\n",
    "    Split into segments and pick 3 points from each segment, the minimum,\n",
    "    median, and maximum. Segment length = int(compfactor x 3). So, to achieve a\n",
    "    compression factor of 2, a segment length of 6 is needed.\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputser : pd.Series\n",
    "        Input data to be compressed.\n",
    "    compfactor : float\n",
    "        Compression factor. The default is 2.\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Compressed output series.\n",
    "    \"\"\"\n",
    "    # If comp factor is too low, return original data\n",
    "    if (compfactor < 1.4):\n",
    "        return inputser\n",
    "\n",
    "    win_size = int(3 * compfactor)  # window size\n",
    "\n",
    "    # Create a column ofsegment numbers\n",
    "    ser = inputser.rename('value')\n",
    "    ser = ser.round(3)\n",
    "    wdf = ser.to_frame()\n",
    "    del ser\n",
    "    start_idxs = wdf.index[range(0, len(wdf), win_size)]\n",
    "    wdf['win_start'] = 0\n",
    "    wdf.loc[start_idxs, 'win_start'] = 1\n",
    "    wdf['win_num'] = wdf['win_start'].cumsum()\n",
    "    wdf.drop(columns='win_start', inplace=True)\n",
    "    del win_size, start_idxs\n",
    "\n",
    "    # For each window, get the indices of min, median, and max\n",
    "    def get_midimax_idxs(gdf):\n",
    "        if len(gdf) == 1:\n",
    "            return [gdf.index[0]]\n",
    "        elif gdf['value'].iloc[0] == gdf['value'].iloc[-1]:\n",
    "            return [gdf.index[0]]\n",
    "        elif len(gdf) == 2:\n",
    "            return [gdf.index[0], gdf.index[1]]\n",
    "        else:\n",
    "            return [gdf.index[0], gdf.index[len(gdf) // 2], gdf.index[-1]]\n",
    "\n",
    "    wdf = wdf.dropna()\n",
    "    wdf_sorted = wdf.sort_values(['win_num', 'value'])\n",
    "    midimax_idxs = wdf_sorted.groupby('win_num').apply(get_midimax_idxs)\n",
    "\n",
    "    # Convert into a list\n",
    "    midimax_idxs = [idx for sublist in midimax_idxs for idx in sublist]\n",
    "    midimax_idxs.sort()\n",
    "    return inputser.loc[midimax_idxs]\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "n = 100000 # points\n",
    "timesteps = pd.to_timedelta(np.arange(n), unit='s')\n",
    "#timestamps = pd.to_datetime('2022–04–18 08:00:00') + timesteps\n",
    "#pd.to_datetime(['2018-10-26 12:00 -0530', '2018-10-26 12:00 -0500'])\n",
    "timestamps = pd.to_datetime('2022-04-18 12:00 -0530') + timesteps\n",
    "\n",
    "sine_waves = np.sin(2 * np.pi * 0.02 * np.arange(n))\n",
    "noise = np.random.normal(0, 0.1, n)\n",
    "signal = sine_waves + noise\n",
    "ts_data = pd.Series(signal, index=timestamps).astype('float32')\n",
    "\n",
    "ts_data\n",
    "\n",
    "# Run compression\n",
    "timer_start = time.time()\n",
    "ts_data_compressed = compress_series(ts_data, 2)\n",
    "timer_sec = round(time.time() - timer_start, 2)\n",
    "print('Compression took', timer_sec, 'seconds.')\n",
    "\n",
    "ts_data_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0dd4f19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajverma\\AppData\\Local\\Temp/ipykernel_13532/2184144846.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_temp['adapted_y'] = df_temp['y'].shift(-1)\n",
      "C:\\Users\\ajverma\\AppData\\Local\\Temp/ipykernel_13532/2184144846.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_temp['x2_avg_2periods'] = df_temp['x1'].rolling(2, min_periods=1).mean()\n",
      "C:\\Users\\ajverma\\AppData\\Local\\Temp/ipykernel_13532/2184144846.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_temp['x2_max_2periods'] = df_temp['x1'].rolling(2, min_periods=1).max()\n",
      "C:\\Users\\ajverma\\AppData\\Local\\Temp/ipykernel_13532/2184144846.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_temp['x2_green_sum_3periods']  = df_temp['x2_green'].rolling(2, min_periods=1).sum()\n",
      "C:\\Users\\ajverma\\AppData\\Local\\Temp/ipykernel_13532/2184144846.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_temp['x2_blue_sum_3periods']   = df_temp['x2_blue'].rolling(2, min_periods=1).sum()\n",
      "C:\\Users\\ajverma\\AppData\\Local\\Temp/ipykernel_13532/2184144846.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_temp['x2_yellow_sum_3periods'] = df_temp['x2_yellow'].rolling(2, min_periods=1).sum()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>x1</th>\n",
       "      <th>x3</th>\n",
       "      <th>date</th>\n",
       "      <th>y</th>\n",
       "      <th>x2_blue</th>\n",
       "      <th>x2_green</th>\n",
       "      <th>x2_yellow</th>\n",
       "      <th>adapted_y</th>\n",
       "      <th>x2_avg_2periods</th>\n",
       "      <th>x2_max_2periods</th>\n",
       "      <th>x2_green_sum_3periods</th>\n",
       "      <th>x2_blue_sum_3periods</th>\n",
       "      <th>x2_yellow_sum_3periods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cust_id  x1  x3       date  y  x2_blue  x2_green  x2_yellow  adapted_y  \\\n",
       "0        1   1  10 2020-03-01  0        0         1          0        1.0   \n",
       "1        1   2  15 2020-03-02  1        1         0          0        0.0   \n",
       "2        1   3  12 2020-03-03  0        1         0          0        NaN   \n",
       "3        2   4  19 2020-03-01  0        0         1          0        1.0   \n",
       "4        2   5  21 2020-03-02  1        1         0          0        0.0   \n",
       "5        2   6  13 2020-03-03  0        0         0          1        NaN   \n",
       "\n",
       "   x2_avg_2periods  x2_max_2periods  x2_green_sum_3periods  \\\n",
       "0              1.0              1.0                    1.0   \n",
       "1              1.5              2.0                    1.0   \n",
       "2              2.5              3.0                    0.0   \n",
       "3              4.0              4.0                    1.0   \n",
       "4              4.5              5.0                    1.0   \n",
       "5              5.5              6.0                    0.0   \n",
       "\n",
       "   x2_blue_sum_3periods  x2_yellow_sum_3periods  \n",
       "0                   0.0                     0.0  \n",
       "1                   1.0                     0.0  \n",
       "2                   2.0                     0.0  \n",
       "3                   0.0                     0.0  \n",
       "4                   1.0                     0.0  \n",
       "5                   1.0                     1.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://medium.com/@jedrzejalchimowicz/tackle-time-series-forecasting-with-supervised-machine-learning-a-step-by-step-guide-in-python-a5510063f8c9\n",
    "\n",
    "import pandas as pd\n",
    "# Create DataFrame\n",
    "data = {'cust_id': [1]*3 + [2]*3,\n",
    "        'x1': [1,2,3,4,5,6],\n",
    "        'x2': ['green', 'blue', 'blue', 'green', 'blue', 'yellow'],\n",
    "        'x3': [10,15,12,19,21,13],\n",
    "        'date': pd.date_range('3/1/2020', '3/3/2020').tolist()*2,\n",
    "        'y': [0,1,0,0,1,0]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#convert catg var to numeric\n",
    "# Create a table with encoded variable\n",
    "one_hot_x2 = pd.get_dummies(df['x2'], prefix='x2')\n",
    "\n",
    "# Join the newly created table with the original one\n",
    "df = pd.merge(df, one_hot_x2, left_index=True, right_index=True).drop(columns=['x2'])\n",
    "\n",
    "# Declare output dataframe\n",
    "df_new = pd.DataFrame()\n",
    "\n",
    "for customer in df['cust_id'].unique():\n",
    "  \n",
    "    # Get the part of df refering to one customer\n",
    "    df_temp = df[df['cust_id'] == customer]\n",
    "    \n",
    "    # Shift the dependent variable one period up\n",
    "    df_temp['adapted_y'] = df_temp['y'].shift(-1)\n",
    "    \n",
    "    # Create new variables from num variable\n",
    "    df_temp['x2_avg_2periods'] = df_temp['x1'].rolling(2, min_periods=1).mean()\n",
    "    df_temp['x2_max_2periods'] = df_temp['x1'].rolling(2, min_periods=1).max()\n",
    "    \n",
    "    # Create new variables from cat variable\n",
    "    df_temp['x2_green_sum_3periods']  = df_temp['x2_green'].rolling(2, min_periods=1).sum()\n",
    "    df_temp['x2_blue_sum_3periods']   = df_temp['x2_blue'].rolling(2, min_periods=1).sum()\n",
    "    df_temp['x2_yellow_sum_3periods'] = df_temp['x2_yellow'].rolling(2, min_periods=1).sum()\n",
    "    \n",
    "    # Add the customer to the output table\n",
    "    df_new = df_new.append(df_temp)\n",
    "    \n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05925f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>date</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>green</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>blue</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>blue</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>green</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>blue</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>green</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cust_id  x1  x2     x3       date  y\n",
       "0        1   1  10  green 2020-03-01  0\n",
       "1        1   2  15   blue 2020-03-02  1\n",
       "2        1   3  12   blue 2020-03-03  0\n",
       "3        2   4  19  green 2020-03-01  0\n",
       "4        2   5  21   blue 2020-03-02  1\n",
       "5        2   6  13  green 2020-03-03  0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2c034f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.03149176513774439"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://towardsdatascience.com/an-introduction-to-exceedance-probability-forecasting-4c96c0e7772c\n",
    "#https://towardsdatascience.com/forecasting-with-trees-hybrid-classifiers-for-time-series-b2509abf15f8\n",
    "\n",
    "#Exceedance probability forecasting - Binary Classification\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# reading the time series (pd.Series format)\n",
    "#tseries = pd.read_csv('path_to_data.csv') \n",
    "# you can simulate some data with:\n",
    "tseries = pd.Series(np.random.random(100))\n",
    "\n",
    "# using 3 lags -- the no. of observation we look back to model the next value of the series\n",
    "N_LAGS = 3\n",
    "# Constructing a set of observations based on past recent values (lags)\n",
    "tseries_df = pd.concat([tseries.shift(i) for i in range(N_LAGS, -1, -1)], axis=1)\n",
    "\n",
    "# Assigning column names and dropping na's\n",
    "tseries_df.columns = ['t-' + str(i) for i in list(reversed(range(N_LAGS)))] + ['t+1']\n",
    "tseries_df = tseries_df.dropna().reset_index(drop=True)\n",
    "\n",
    "# the target variable is the column 't+1' representing the next value of the series\n",
    "y = tseries_df['t+1']\n",
    "# Removing the target to create the predictor variables\n",
    "X = tseries_df.drop('t+1', axis=1)\n",
    "\n",
    "# basic train/test split -- you may want a more robust cross-validation approach\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=False)\n",
    "\n",
    "# setting the threshold to the 95th percentile\n",
    "thr = np.quantile(y_train, .95)\n",
    "\n",
    "# transforming the numeric target to a binary one \n",
    "y_train_binary = (y_train >= thr).astype(int)\n",
    "y_test_binary = (y_test >= thr).astype(int)\n",
    "\n",
    "# training a probabilistic classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train_binary)\n",
    "\n",
    "# predicting exceedance probability\n",
    "exceedance_prob_clf = classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#The alternative to a classification approach is to use a forecasting ensemble.\n",
    "# Using the RF as ensemble of trees\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Training the model\n",
    "forecasting_ensemble = RandomForestRegressor()\n",
    "# Here we use the numerical target\n",
    "## which represents the next value of the time series\n",
    "forecasting_ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Retrieving the predictions from the trees.\n",
    "per_tree_pred = [tree.predict(X_test) for tree in forecasting_ensemble.estimators_]\n",
    "per_tree_df = pd.DataFrame(per_tree_pred).T\n",
    "\n",
    "# Computing the ratio of trees which predict a value above the threshold\n",
    "exceedance_prob_fe = per_tree_df.apply(lambda x: np.mean(x > thr), axis=1).values\n",
    "\n",
    "from sklearn.metrics import brier_score_loss, roc_auc_score\n",
    "\n",
    "roc_auc_score(y_true=y_test_binary, y_score=exceedance_prob_fe)\n",
    "roc_auc_score(y_true=y_test_binary, y_score=exceedance_prob_clf)\n",
    "\n",
    "brier_score_loss(y_true=y_test_binary, y_prob=exceedance_prob_fe)\n",
    "brier_score_loss(y_true=y_test_binary, y_prob=exceedance_prob_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2583de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/smeitoma/train-demo\n",
    "\n",
    "'''Historical volatility\n",
    "Next, we calculate the historical volatility. The historical volatility calculated here is the standard deviation of the\n",
    "logarithmic price change over the last 10, 21, and 63 business days. Historical volatility is a risk indicator\n",
    "and is used to determine how violently prices have fluctuated. In general, stocks with large historical volatility are\n",
    "considered relatively riskier to hold as assets than stocks with small historical volatility.'''\n",
    "\n",
    "periods = [10, 21, 63]\n",
    "vol_names = []\n",
    "for period in periods:\n",
    "    vol_names.append(f\"volatility_{period}\")\n",
    "    price.loc[:, f\"volatility_{period}\"] = np.log(price[\"AdjustedClose\"]).diff().rolling(period).std()\n",
    "    \n",
    "\n",
    "# calculate 2 week return using AdjustedClose\n",
    "feats[\"return_2week\"] = feats[close_col].pct_change(10)\n",
    "# calculate last 1 month return using AdjustedClose\n",
    "feats[\"return_1month\"] = feats[close_col].pct_change(21)\n",
    "# calculate last 3 months return using AdjustedClose\n",
    "feats[\"return_3month\"] = feats[close_col].pct_change(63)\n",
    "\n",
    "# calculate 2 week historical volatility using AdjustedClose\n",
    "feats[\"volatility_2week\"] = (np.log(feats[close_col]).diff().rolling(10).std())\n",
    "# calculate last 1 month historical volatility using AdjustedClose\n",
    "feats[\"volatility_1month\"] = (np.log(feats[close_col]).diff().rolling(21).std())\n",
    "# calculate last 3 months historical volatility using AdjustedClose\n",
    "feats[\"volatility_3month\"] = (np.log(feats[close_col]).diff().rolling(63).std())\n",
    "\n",
    "\n",
    "# initialize model\n",
    "pred_model = LGBMRegressor(**lgbm_params)\n",
    "# train\n",
    "pred_model.fit(train_X[feat_cols].values, train_y)\n",
    "# prepare result data\n",
    "result = test_X[[\"SecuritiesCode\"]].copy()\n",
    "# predict\n",
    "result.loc[:, \"predict\"] = pred_model.predict(test_X[feat_cols])\n",
    "# actual result\n",
    "result.loc[:, \"Target\"] = test_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaeaba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47227397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/neomatrix369/everything-you-can-do-with-a-time-series-cryptos/notebook\n",
    "\n",
    "#Forward filling missing data\n",
    "data = data.iloc[1:]\n",
    "data = data.fillna(method='ffill')\n",
    "\n",
    "#Back filling missing data\n",
    "data = data.fillna(method='bfill')\n",
    "\n",
    "# downsample from hourly to 3 day frequency aggregated using mean\n",
    "btc_resampled = btc.resample('3D').mean()\n",
    "\n",
    "#upsample from 3 day frequency to daily frequency\n",
    "btc_resampled = btc.resample('D').pad()\n",
    "\n",
    "#Percent change plot\n",
    "btc['Change'] = btc.High.div(btc.High.shift())\n",
    "btc['Change'].plot(figsize=(20,8))\n",
    "_ = plt.title('Change in High price of BTC over time')\n",
    "\n",
    "btc['Volume_Change'] = btc.Volume.div(btc.Volume.shift())\n",
    "btc['Volume_Change'].plot(figsize=(20,8))\n",
    "_ = plt.title('Change in BTC Volume over time')\n",
    "\n",
    "#Daily returns\n",
    "btc['Return'] = btc.Change.sub(1).mul(100)\n",
    "btc['Return'].plot(figsize=(20,8))\n",
    "_ = plt.title('Return based on High price change of BTC over time')\n",
    "\n",
    "btc.High.pct_change().mul(100).plot(figsize=(20,6)) # Another way to calculate returns\n",
    "_ = plt.title('Percentage change of High price of BTC over time')\n",
    "\n",
    "btc.Volume.pct_change().mul(100).plot(figsize=(20,6)) # Another way to calculate returns\n",
    "_ = plt.title('Percentage change of BTC Volume over time')\n",
    "\n",
    "#Absolute change in successive rows\n",
    "btc.High.diff().plot(figsize=(20,6))\n",
    "_ = plt.title('Difference in High price change of BTC over time')\n",
    "\n",
    "#Comparing two or more time series\n",
    "#We will compare 2 time series by normalizing them. This is achieved by dividing each time series element of all #time series by the first element. This way both series start at the same point and can be easily compared.\n",
    "\n",
    "# Plotting before normalization\n",
    "btc.High.plot()\n",
    "eth.High.plot()\n",
    "plt.legend(['BTC','ETH'])\n",
    "_ = plt.title('Comparing BTC and ETH over time')\n",
    "plt.show()\n",
    "\n",
    "# Normalizing and comparison\n",
    "# Both coins start from 100\n",
    "normalized_btc = btc.High.div(btc.High.iloc[0]).mul(100)\n",
    "normalized_eth = eth.High.div(eth.High.iloc[0]).mul(100)\n",
    "normalized_btc.plot()\n",
    "normalized_eth.plot()\n",
    "_ = plt.title('Comparing BTC and ETH over time (normalized)')\n",
    "plt.legend(['BTC','ETH'])\n",
    "plt.show()\n",
    "\n",
    "#Window functions\n",
    "#Window functions are used to identify sub periods, calculates sub-metrics of sub-periods.\n",
    "#Rolling - Same size and sliding\n",
    "#Expanding - Contains all prior values\n",
    "\n",
    "# Rolling window functions\n",
    "rolling_btc = btc.High.rolling('90D').mean()\n",
    "btc.High.plot()\n",
    "rolling_btc.plot()\n",
    "_ = plt.title('Comparing High price and rolling High price of BTC over time')\n",
    "plt.legend(['High','Rolling Mean'])\n",
    "# Plotting a rolling mean of 90 day window with original High attribute of BTC\n",
    "plt.show()\n",
    "\n",
    "# Expanding window functions\n",
    "btc_mean = btc.High.expanding().mean()\n",
    "btc_std = btc.High.expanding().std()\n",
    "btc.High.plot()\n",
    "btc_mean.plot()\n",
    "btc_std.plot()\n",
    "_ = plt.title('Comparing High price, Expanding Mean of High Price and Expanding Std. Dev. of High Price of BTC over time')\n",
    "plt.legend(['High','Expanding Mean','Expanding Standard Deviation'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Open-high-low-close Charts (or OHLC Charts)\n",
    "trace = go.Ohlc(x=btc['06-2021'].index,\n",
    "                open=btc['06-2021'].Open,\n",
    "                high=btc['06-2021'].High,\n",
    "                low=btc['06-2021'].Low,\n",
    "                close=btc['06-2021'].Close)\n",
    "data = [trace]\n",
    "print('OHLC price chart of BTC for June 2021')\n",
    "iplot(data, filename='simple_ohlc')\n",
    "\n",
    "\n",
    "# Candlestick chart of march 2008\n",
    "trace = go.Candlestick(x=btc['06-2021'].index,\n",
    "                open=btc['06-2021'].Open,\n",
    "                high=btc['06-2021'].High,\n",
    "                low=btc['06-2021'].Low,\n",
    "                close=btc['06-2021'].Close)\n",
    "data = [trace]\n",
    "print('OHLC price chart (Candlestick) of BTC for June 2021')\n",
    "iplot(data, filename='simple_candlestick')\n",
    "\n",
    "# Autocorrelation of Closing price of BTC\n",
    "#As all lags are either close to 1 or at least greater than the confidence interval, they are statistically significant.\n",
    "plot_acf(btc[\"Close\"],lags=25,title=\"Autocorrelation chart: BTC (Close price)\")\n",
    "plt.show()\n",
    "\n",
    "# Partial Autocorrelation of Closing price of BTC\n",
    "plot_pacf(btc[\"Close\"],lags=25, title=\"Partial autocorrelation chart: BTC (Close price)\")\n",
    "plt.show()\n",
    "\n",
    "#Time series decomposition and Random walks - Trends, seasonality and noise\n",
    "\n",
    "# Let's take BTC's High for this\n",
    "btc[\"High\"].plot(figsize=(16,8))\n",
    "_ = plt.title('Comparing High price of BTC over time')\n",
    "\n",
    "# Now, for decomposition...\n",
    "print('Decomposing closing price of BTC over time')\n",
    "decomposed_btc_close = sm.tsa.seasonal_decompose(btc[\"Close\"],freq=360) # The frequncy is annual\n",
    "figure = decomposed_btc_close.plot()\n",
    "plt.show()\n",
    "\n",
    "# Plotting white noise\n",
    "rcParams['figure.figsize'] = 16, 6\n",
    "white_noise = np.random.normal(loc=0, scale=1, size=1000)\n",
    "_ = plt.title('Generate Whitenoise')\n",
    "# loc is mean, scale is variance\n",
    "plt.plot(white_noise)\n",
    "\n",
    "# Plotting autocorrelation of white noise\n",
    "plot_acf(white_noise,lags=20)\n",
    "plt.show()\n",
    "\n",
    "# Augmented Dickey-Fuller test on volume of BTC and ETH coins \n",
    "adf = adfuller(btc[\"Volume\"])\n",
    "print(\"p-value of BTC: {}\".format(float(adf[1])))\n",
    "adf = adfuller(eth[\"Volume\"])\n",
    "print(\"p-value of ETH: {}\".format(float(adf[1])))\n",
    "\n",
    "seed(42)\n",
    "rcParams['figure.figsize'] = 16, 6\n",
    "random_walk = normal(loc=0, scale=0.01, size=1000)\n",
    "_ = plt.title('Plot random walk')\n",
    "plt.plot(random_walk)\n",
    "plt.show()\n",
    "\n",
    "#Stationarity\n",
    "# The original non-stationary plot\n",
    "decomposed_btc_close.trend.plot()\n",
    "_ = plt.title('Decomposed BTC closing price over time')\n",
    "\n",
    "# The original non-stationary plot\n",
    "decomposed_btc_close.trend.diff().plot()\n",
    "_ = plt.title('Decomposed BTC closing price difference over time')\n",
    "\n",
    "# AR(1) MA(1) model:AR parameter = +0.9\n",
    "rcParams['figure.figsize'] = 16, 12\n",
    "plt.subplot(4,1,1)\n",
    "ar1 = np.array([1, -0.9]) # We choose -0.9 as AR parameter is +0.9\n",
    "ma1 = np.array([1])\n",
    "AR1 = ArmaProcess(ar1, ma1)\n",
    "sim1 = AR1.generate_sample(nsample=1000)\n",
    "plt.title('AR(1) model: AR parameter = +0.9')\n",
    "plt.plot(sim1)\n",
    "# We will take care of MA model later\n",
    "# AR(1) MA(1) AR parameter = -0.9\n",
    "plt.subplot(4,1,2)\n",
    "ar2 = np.array([1, 0.9]) # We choose +0.9 as AR parameter is -0.9\n",
    "ma2 = np.array([1])\n",
    "AR2 = ArmaProcess(ar2, ma2)\n",
    "sim2 = AR2.generate_sample(nsample=1000)\n",
    "plt.title('AR(1) model: AR parameter = -0.9')\n",
    "plt.plot(sim2)\n",
    "# AR(2) MA(1) AR parameter = 0.9\n",
    "plt.subplot(4,1,3)\n",
    "ar3 = np.array([2, -0.9]) # We choose -0.9 as AR parameter is +0.9\n",
    "ma3 = np.array([1])\n",
    "AR3 = ArmaProcess(ar3, ma3)\n",
    "sim3 = AR3.generate_sample(nsample=1000)\n",
    "plt.title('AR(2) model: AR parameter = +0.9')\n",
    "plt.plot(sim3)\n",
    "# AR(2) MA(1) AR parameter = -0.9\n",
    "plt.subplot(4,1,4)\n",
    "ar4 = np.array([2, 0.9]) # We choose +0.9 as AR parameter is -0.9\n",
    "ma4 = np.array([1])\n",
    "AR4 = ArmaProcess(ar4, ma4)\n",
    "sim4 = AR4.generate_sample(nsample=1000)\n",
    "plt.title('AR(2) model: AR parameter = -0.9')\n",
    "plt.plot(sim4)\n",
    "plt.show()\n",
    "\n",
    "model = ARMA(sim1, order=(1,0))\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "print(\"μ={} ,ϕ={}\".format(result.params[0],result.params[1]))\n",
    "\n",
    "# Predicting simulated AR(1) model \n",
    "result.plot_predict(start=900, end=1010)\n",
    "_ = plt.title('AR model: Actual versus predicted values (with confidence interval plotted) - simulated values')\n",
    "plt.show()\n",
    "\n",
    "# Predicting Close price of BTC\n",
    "close_price = ARMA(btc[\"Close\"].diff().iloc[1:].values, order=(1,0))\n",
    "res = close_price.fit()\n",
    "res.plot_predict(start=1000, end=1100)\n",
    "_ = plt.title('ARMA model: Actual versus predicted values of Closing price of BTC')\n",
    "plt.show()\n",
    "\n",
    "#MA models\n",
    "rcParams['figure.figsize'] = 16, 6\n",
    "ar1 = np.array([1])\n",
    "ma1 = np.array([1, -0.5])\n",
    "MA1 = ArmaProcess(ar1, ma1)\n",
    "sim1 = MA1.generate_sample(nsample=1000)\n",
    "_ = plt.title('ARMA generated sample')\n",
    "plt.plot(sim1)\n",
    "\n",
    "model = ARMA(sim1, order=(0,1))\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "print(\"μ={} ,θ={}\".format(result.params[0],result.params[1]))\n",
    "\n",
    "\n",
    "#ARMA\n",
    "%%time\n",
    "# Forecasting and predicting Closing price of BTC \n",
    "model = ARMA(btc[\"Close\"].diff().iloc[1:].values, order=(3,2))\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "print(\"μ={}, ϕ={}, θ={}\".format(result.params[0],result.params[1],result.params[2]))\n",
    "result.plot_predict(start=1000, end=1100)\n",
    "_ = plt.title('ARMA model: predicting Close price BTC')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#ARIMA\n",
    "%%time\n",
    "# Predicting the close price of BTC \n",
    "rcParams['figure.figsize'] = 16, 6\n",
    "model = ARIMA(btc[\"Close\"].diff().iloc[1:].values, order=(2,1,0))\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "result.plot_predict(start=700, end=1000)\n",
    "_ = plt.title('ARIMA model: predicting Close price of BTC')\n",
    "plt.show()\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(btc[\"Close\"].diff().iloc[700:1001].values, result.predict(start=700,end=1000)))\n",
    "print(\"The root mean squared error is {}.\".format(rmse))\n",
    "\n",
    "#VAR model\n",
    "# %%time\n",
    "# # Predicting closing price of BTC and ETH\n",
    "## Encountering error: LinAlgError: Singular forecast error covariance matrix encountered at period 133\n",
    "# train_sample = pd.concat([btc[\"Close\"].diff().iloc[1:], eth[\"Close\"].diff().iloc[1:]],axis=1)\n",
    "# model = sm.tsa.VARMAX(train_sample,order=(2,1),trend='c', enforce_invertibility=False, enforce_stationarity=False) \n",
    "# result = model.fit(maxiter=1000,disp=False)\n",
    "# print(result.summary())\n",
    "# predicted_result = result.predict(start=0, end=1000)\n",
    "# result.plot_diagnostics()\n",
    "# # calculating error\n",
    "# rmse = math.sqrt(mean_squared_error(train_sample.iloc[1:1002].values, predicted_result.values))\n",
    "# print(\"The root mean squared error is {}.\".format(rmse))\n",
    "\n",
    "#SARIMA\n",
    "%%time\n",
    "# Predicting closing price of BTC'\n",
    "train_sample = btc[\"Close\"].diff().iloc[1:].values\n",
    "\n",
    "model = sm.tsa.SARIMAX(train_sample,order=(4,0,4),trend='c', enforce_invertibility=False, enforce_stationarity=False)\n",
    "result = model.fit(maxiter=1000, disp=False)\n",
    "print(result.summary())\n",
    "predicted_result = result.predict(start=0, end=500)\n",
    "result.plot_diagnostics()\n",
    "# calculating error\n",
    "rmse = math.sqrt(mean_squared_error(train_sample[1:502], predicted_result))\n",
    "print(\"The root mean squared error is {}.\".format(rmse))\n",
    "\n",
    "# Unobserved Components\n",
    "%%time\n",
    "# Predicting closing price of BTC'\n",
    "train_sample = btc[\"Close\"].diff().iloc[1:].values\n",
    "model = sm.tsa.UnobservedComponents(train_sample,'local level')\n",
    "result = model.fit(maxiter=1000,disp=False)\n",
    "print(result.summary())\n",
    "predicted_result = result.predict(start=0, end=500)\n",
    "result.plot_diagnostics()\n",
    "# calculating error\n",
    "rmse = math.sqrt(mean_squared_error(train_sample[1:502], predicted_result))\n",
    "print(\"The root mean squared error is {}.\".format(rmse))\n",
    "\n",
    "%%time\n",
    "##### Currently throws an error, hence disabled: LinAlgError: Singular forecast error covariance matrix encountered at period 1 or period 0\n",
    "# Predicting closing price of BTC and ETH\n",
    "# train_sample = pd.concat([btc[\"Close\"].diff().iloc[1:], eth[\"Close\"].diff().iloc[1:]],axis=1)\n",
    "# train_sample = train_sample.fillna(0.0) # Ideally we should not need to do this\n",
    "# model = sm.tsa.DynamicFactor(train_sample, k_factors=1, factor_order=2)\n",
    "# result = model.fit(maxiter=1000, disp=False)\n",
    "# print(result.summary())\n",
    "# predicted_result = result.predict(start=0, end=1000)\n",
    "# result.plot_diagnostics()\n",
    "# # calculating error\n",
    "# rmse = math.sqrt(mean_squared_error(train_sample.iloc[1:1002].values, predicted_result.values))\n",
    "# print(\"The root mean squared error is {}.\".format(rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
