{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a51a8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formatted_summary using freq: We wear different clothes and do different things in different weather conditions.\n",
      "Weather stations around the world measure different parts of weather.\n",
      "Climate tells us what kinds of weather usually happen in an area at different times of the year.\n",
      "Weather includes wind, lightning, storms, hurricanes, tornadoes (also known as twisters), rain, hail, snow, and lots more.\n",
      "Ways to measure weather are wind speed, wind direction, temperature and humidity.\n",
      "\n",
      "\n",
      " formatted_summary using tdidf: Energy from the Sun affects the weather too.\n",
      "Changes in weather can affect our mood and life.\n",
      "We wear different clothes and do different things in different weather conditions.\n",
      "Weather stations around the world measure different parts of weather.\n",
      "People try to use these measurements to make weather forecasts for the future.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ajverma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ajverma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2023/03/exploring-the-extractive-method-of-text-summarization/\n",
    "\n",
    "#Using a Sample Text From Wikipedia to Generate Summary\n",
    "text = '''\n",
    "Weather is the day-to-day or hour-to-hour change in the atmosphere. \n",
    "Weather includes wind, lightning, storms, hurricanes, tornadoes (also known as twisters), rain, hail, snow, and lots more. \n",
    "Energy from the Sun affects the weather too. \n",
    "Climate tells us what kinds of weather usually happen in an area at different times of the year. \n",
    "Changes in weather can affect our mood and life. We wear different clothes and do different things in different weather conditions. \n",
    "We choose different foods in different seasons.\n",
    "Weather stations around the world measure different parts of weather. \n",
    "Ways to measure weather are wind speed, wind direction, temperature and humidity. \n",
    "People try to use these measurements to make weather forecasts for the future. \n",
    "These people are scientists that are called meteorologists. \n",
    "They use computers to build large mathematical models to follow weather trends.'''\n",
    "\n",
    "\n",
    "#1 Frequency-based Approach\n",
    "# import the required libraries\n",
    "import nltk\n",
    "nltk.download('punkt') # punkt tokenizer for sentence tokenization\n",
    "nltk.download('stopwords') # list of stop words, such as 'a', 'an', 'the', 'in', etc, which would be dropped\n",
    "from collections import Counter # Imports the Counter class from the collections module, used for counting the frequency of words in a text.\n",
    "from nltk.corpus import stopwords # Imports the stop words list from the NLTK corpus\n",
    "# corpus is a large collection of text or speech data used for statistical analysis\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize # Imports the sentence tokenizer and word tokenizer from the NLTK tokenizer module. \n",
    "# Sentence tokenizer is for splitting text into sentences\n",
    "# word tokenizer is for splitting sentences into words\n",
    "\n",
    "# this function would take 2 inputs, one being the text, and the other being the summary which would contain the number of lines\n",
    "def generate_summary_using_freq(text, n):\n",
    "    # Tokenize the text into individual sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Tokenize each sentence into individual words and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # the following line would tokenize each sentence from sentences into individual words using the word_tokenize function of nltk.tokenize module\n",
    "    # Then removes any stop words and non-alphanumeric characters from the resulting list of words and converts them all to lowercase\n",
    "    words = [word.lower() for word in word_tokenize(text) if word.lower() not in stop_words and word.isalnum()]\n",
    "\n",
    "    # Compute the frequency of each word\n",
    "    word_freq = Counter(words)\n",
    "\n",
    "    # Compute the score for each sentence based on the frequency of its words\n",
    "    # After this block of code is executed, sentence_scores will contain the scores of each sentence in the given text, \n",
    "    # where each score is a sum of the frequency counts of its constituent words\n",
    "\n",
    "    # empty dictionary to store the scores for each sentence\n",
    "    sentence_scores = {}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_words = [word.lower() for word in word_tokenize(sentence) if word.lower() not in stop_words and word.isalnum()]\n",
    "        sentence_score = sum([word_freq[word] for word in sentence_words])\n",
    "        \n",
    "        if len(sentence_words) < 20:\n",
    "            sentence_scores[sentence] = sentence_score\n",
    "\n",
    "    # checks if the length of the sentence_words list is less than 20 (parameter can be adjusted based on the desired length of summary sentences)\n",
    "    # If condition -> true, score of the current sentence is added to the sentence_scores dictionary with the sentence itself as the key\n",
    "    # This is to filter out very short sentences that may not provide meaningful information for summary generation\n",
    "\n",
    "    # Select the top n sentences with the highest scores\n",
    "    summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:n]\n",
    "    summary = ' '.join(summary_sentences)\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "summary = generate_summary_using_freq(text, 5)\n",
    "summary_sentences = summary.split('. ')\n",
    "formatted_summary = '.\\n'.join(summary_sentences)\n",
    "\n",
    "print('formatted_summary using freq:', formatted_summary)\n",
    "\n",
    "#2 TF-IDF Approach\n",
    "# importing the required libraries\n",
    "\n",
    "# importing TfidfVectorizer class to convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# importing cosine_similarity function to compute the cosine similarity between two vectors.\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# importing nlargest to return the n largest elements from an iterable in descending order.\n",
    "from heapq import nlargest\n",
    "\n",
    "def generate_summary_using_TFIDF(text, n):\n",
    "    # Tokenize the text into individual sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Create the TF-IDF matrix\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    # Compute the cosine similarity between each sentence and the document\n",
    "    sentence_scores = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])[0]\n",
    "\n",
    "    # Select the top n sentences with the highest scores\n",
    "    summary_sentences = nlargest(n, range(len(sentence_scores)), key=sentence_scores.__getitem__)\n",
    "\n",
    "    summary_tfidf = ' '.join([sentences[i] for i in sorted(summary_sentences)])\n",
    "    return summary_tfidf\n",
    "\n",
    "summary = generate_summary_using_TFIDF(text, 5)\n",
    "summary_sentences = summary.split('. ')\n",
    "formatted_summary = '.\\n'.join(summary_sentences)\n",
    "\n",
    "print('\\n\\n formatted_summary using tdidf:',formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62827af8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
