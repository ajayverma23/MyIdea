{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eacbef46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'text_cleaning_list' (list)\n",
      "Stored 'text_cleaning_dict' (dict)\n",
      "Stored 'text_extract_feat_list' (list)\n",
      "Stored 'text_extract_feat_dict' (dict)\n"
     ]
    }
   ],
   "source": [
    "#replaced text_cleaning() and extract_text_features()\n",
    "\n",
    "def RemoveTextPattern(input_txt, pattern):\n",
    "    import re\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt\n",
    "\n",
    "# Noise Removal\n",
    "def scrub_words(text):\n",
    "    \"\"\"Basic cleaning of texts.\"\"\"\n",
    "    \n",
    "    import re\n",
    "    # remove html markup\n",
    "    text=re.sub(\"(<.*?>)\",\"\",text)\n",
    "    \n",
    "    #remove non-ascii and digits\n",
    "    text=re.sub(\"(\\\\W|\\\\d)\",\" \",text)\n",
    "    \n",
    "    #remove whitespace\n",
    "    text=text.strip()\n",
    "    return text\n",
    "\n",
    "def RemoveCustomizeWords(text):\n",
    "    #from nltk.corpus import stopwords\n",
    "    import re\n",
    "    # lower text \n",
    "    text = text.lower()\n",
    "    #removing stop words\n",
    "    #text = ' '.join([e_words for e_words in text.split(' ') if e_words not in stopwords.words('english')])\n",
    "    \n",
    "    #removing square brackets\n",
    "    text=re.sub('[.*?]', '', text)\n",
    "    text=re.sub('+', '', text)\n",
    "    #removing hyperlink\n",
    "    text= re.sub('https?://S+|www.S+', '', text)\n",
    "    #removing puncuation\n",
    "    text=re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('n' , '', text)\n",
    "    #remove words containing numbers\n",
    "    text=re.sub('w*dw*' , '', text)\n",
    "    #tokenizer\n",
    "    text = nltk.word_tokenize(text)\n",
    "    #lemmatizer\n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "# Function for expanding contractions\n",
    "def expand_contractions(text):\n",
    "    #contractions_dict outside create problem while importing\n",
    "    contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\"can't\": \"can not\",\"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\"i'll\": \"i will\",\"i'll've\": \"i will have\",\"i'm\": \"i am\",\"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\n",
    "    \"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\n",
    "    \"there'd\": \"there would\",\"there'd've\": \"there would have\",\n",
    "    \"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\n",
    "    \"what've\": \"what have\",\"when've\": \"when have\",\"where'd\": \"where did\",\n",
    "    \"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
    "    \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\"you'd've\": \"you would have\",\"you'll\": \"you will\",\"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\"you've\": \"you have\"}\n",
    "\n",
    "    \n",
    "    \n",
    "'''\n",
    "TBA:\n",
    "nltk.download('vader_lexicon')\n",
    "sentiments = SentimentIntensityAnalyzer()\n",
    "data[\"Positive\"] = [sentiments.polarity_scores(i)[\"pos\"] for i in data[\"content\"]]\n",
    "data[\"Negative\"] = [sentiments.polarity_scores(i)[\"neg\"] for i in data[\"content\"]]\n",
    "data[\"Neutral\"] = [sentiments.polarity_scores(i)[\"neu\"] for i in data[\"content\"]]\n",
    "data = data[[\"content\", \"Positive\", \"Negative\", \"Neutral\"]]\n",
    "\n",
    "positive =' '.join([i for i in data['content'][data['Positive'] > data[\"Negative\"]]])\n",
    "negative =' '.join([i for i in data['content'][data['Negative'] > data[\"Positive\"]]])\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def text_clean_feat_extract(data, text_col=None, cleaning_list=[], cleaning_dict={},\n",
    "        extract_feat_list=[], extract_feat_dict={}, series_out=True, drop_text_col=True,\n",
    "        encoding_type=['pert_dist'], add_num_feat=True, drop_single_value_cols=False):\n",
    "    \n",
    "    '''\n",
    "    v1.5 : Apr27,2023 bug fixed of argumnet text_col not needed @ single feat df like series/ TBC\n",
    "    v1.4 : Mar02,2023 bug fixed, drop_single_value_cols=True\n",
    "    v1.3 : Feb08,2023 added add_num_feat option/drop single value cols/cat to num\n",
    "    v1.2 : Dec 05,2022 added drop_text_col/encoding_type\n",
    "    v1.1 : Nov 29,2022 updated for cleaning_list/cleaning_dict and extract_feat_list/extract_feat_dict\n",
    "    v1.0 : Nov22,2022 initial ver\n",
    "     \n",
    "    assumption:\n",
    "    https://pypi.org/project/nlp-text-cleaner/\n",
    "    !pip install nlp-text-cleaner\n",
    "    \n",
    "    list: used for setting the flag\n",
    "    dict: used for to collect external values\n",
    "    \n",
    "    cleaning_list=['LowerCase', 'RemovePunct',  \n",
    "              'RemoveUnicode', 'RemoveLeadTrailWhiteSpaces', 'RemoveDupWhiteSpaces',\n",
    "              'CorrectGrammar', 'RemoveStopwords', 'ApplyStem', 'ApplyLamma',\n",
    "              'RemoveHashTags', 'RemoveHyperLinks', 'CleanHtmlCode', 'ReplaceAbbr', 'NoiseRemoval']\n",
    "              \n",
    "    TBA >> 'RemoveCustWords', 'ReplaceExtAbbr'\n",
    "        \n",
    "    cleaning_dict = {'ExtRemoveStopWords':['a'], 'RemoveCommonWords':10, 'RemoveRareWords':10,\n",
    "    'RemovePattern':\"@[w]*\", 'RemoveShortWords':2}\n",
    "    \n",
    "    extract_feat_list = ['DetectLang', 'SplitIntoSent', 'SplitIntoWords', 'GetPosTags',\n",
    "    'StopWordsCount', 'TokensFeat', 'SentiMent', 'POS', 'NamedEntity'\n",
    "    ]\n",
    "    #'CharCount', 'WordCount', 'UniqueWordCount', 'UrlCount', 'MeanWordength',\n",
    "    extract_feat_dict = { 'TokensCountList':['a'], 'TokensPresentList':['a']}\n",
    "    \n",
    "    Assumptions:\n",
    "    only cleaning: series_out=True/False // drop_text_col=True/False\n",
    "    only feat extraction: series_out=False // if pass directly for model fit, drop_text_col=True\n",
    "    Both cleaning /feat extraction: try 1 by 1 // drop_text_col=False\n",
    "    add_num_feat: only feat extraction time it will merge num feat \n",
    "    \n",
    "    Error:\n",
    "    drop_single_value_cols - ValueError: X has 10 features, but MinMaxScaler is expecting 11 features as input.\n",
    "    '''\n",
    "    print('entry text_clean_feat_extract')\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from collections import Counter\n",
    "    import re\n",
    "    from nlp_text_cleaner import nlp_text_cleaner as cleaner\n",
    "    \n",
    "    import string\n",
    "    import langdetect\n",
    "    from textblob import TextBlob\n",
    "    \n",
    "    import nltk\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk import tokenize,ngrams\n",
    "    from nltk.corpus import stopwords\n",
    "    nltk.download('vader_lexicon') #download vader from nltk\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    sia = SentimentIntensityAnalyzer() #creating an object of sentiment intensity analyzer\n",
    "    \n",
    "    print('data-typ before chg:', type(data))\n",
    "    \n",
    "    print('##1 input format')\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        df = data.copy()\n",
    "        text_col = text_col\n",
    "    elif isinstance(data, pd.Series):\n",
    "        df = pd.DataFrame(data)\n",
    "        text_col = data.name\n",
    "    elif (isinstance(data, np.ndarray)) | (isinstance(data, list)):\n",
    "        df = pd.DataFrame(data)\n",
    "        text_col = 0\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    print('\\n data-typ after chg, should be dataframe:', type(df))\n",
    "    print(f'\\n input data>>\\n cleaning_list:{cleaning_list}\\n cleaning_dict:{cleaning_dict}\\\n",
    "    \\n extract_feat_list:{extract_feat_list} \\n extract_feat_dict:{extract_feat_dict}')\n",
    "    \n",
    "    print(f'\\n text_col:{text_col}, series_out:{series_out}, drop_text_col:{drop_text_col}\\\n",
    "    encoding_type:{encoding_type}, add_num_feat:{add_num_feat}, drop_single_value_cols:{drop_single_value_cols}')\n",
    "    \n",
    "    print('execution type chk')\n",
    "    if cleaning_list or cleaning_dict:\n",
    "        print('execution for cleaning')\n",
    "    elif extract_feat_list or extract_feat_dict:\n",
    "        print('execution for feature extraction')\n",
    "    elif cleaning_list and cleaning_dict and extract_feat_list and extract_feat_dict:\n",
    "        print('execution for cleaning/feature extraction')\n",
    "    else:\n",
    "        print('execution for nothing')\n",
    "    \n",
    "    print('b4 df-cols:', df.columns.tolist())\n",
    "    text_col = 'text' #bug Apr 27, commented dueto mismatch of text_col\n",
    "    df = df[[text_col]] #v1.3 only text feat\n",
    "    print('after df-cols:', df.columns.tolist())\n",
    "    \n",
    "    if extract_feat_list:\n",
    "        print('##2 extract feat w.r.t extract_feat_list flag')\n",
    "        #if encoding_type:\n",
    "        #2.1 internal inbuilt functions used for features\n",
    "        if 'DetectLang' in extract_feat_list:\n",
    "            df['lang'] = df[text_col].apply(lambda x: cleaner.detect_language(x))\n",
    "        if 'GetPosTags' in extract_feat_list:   \n",
    "            pass\n",
    "            #df['pos_tags'] = df[text_col].apply(lambda x: cleaner.get_pos_tags(x))# it's list of tuples\n",
    "            \n",
    "        #2.2 extternal functions/logic used for features\n",
    "        if 'StopWordsCount' in extract_feat_list:\n",
    "            print('2.2.1 executing StopWordsCount')\n",
    "            stop=set(stopwords.words('english'))\n",
    "            stop_list = list(stop)\n",
    "            #StopWordsCount = STOPWORDS_list + stop_list #external\n",
    "            StopWordsCount = stop_list #internal\n",
    "            df['StopWordsCount'] = df[text_col].apply(lambda x: len([w for w in str(x).lower().split() if w in StopWordsCount]))\n",
    "            \n",
    "        if 'TokensFeat' in extract_feat_list:\n",
    "            print('2.2.2 executing TokenFeat')\n",
    "            df['char_count'] = df[text_col].apply(len)\n",
    "            df['word_count'] = df[text_col].apply(lambda x: len(str(x).split()))\n",
    "            df['unique_word_count'] = df[text_col].apply(lambda x: len(set(str(x).split())))\n",
    "            df['url_count'] = df[text_col].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "            df['mean_word_length'] = df[text_col].apply(lambda x: round(np.mean([len(w) for w in str(x).split()]),2))\n",
    "            df['word_density'] = round(df['char_count'] / (df['word_count']+1),2)\n",
    "            df['punctuation_count'] = df[text_col].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation)))\n",
    "            df['title_word_count'] = df[text_col].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "            df['upper_case_word_count'] = df[text_col].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
    "            df['hashtag_count'] = df[text_col].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "            df['mention_count'] = df[text_col].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
    "            \n",
    "        if 'SentiMent' in extract_feat_list:\n",
    "            print('2.2.3 executing SentMent')\n",
    "            df[\"sentiment\"] = df[text_col].apply(lambda x: round(TextBlob(x).sentiment.polarity),2)\n",
    "            df['scores']=df[text_col].apply(lambda x: sia.polarity_scores(str(x))) #It's dict, can't be encode\n",
    "            df['compound'] = df['scores'].apply(lambda score_dict:round(score_dict['compound']),2)\n",
    "            df['pos'] = df['scores'].apply(lambda pos_dict:round(pos_dict['pos']),2)\n",
    "            df['neg'] = df['scores'].apply(lambda neg_dict:round(neg_dict['neg']),2)\n",
    "            #create a new column named type, which indicates whether the review is pos, neg, or neutral.\n",
    "            df['sentiment_type']=''\n",
    "            df.loc[df.compound>0,'sentiment_type']='POS'\n",
    "            df.loc[df.compound==0,'sentiment_type']='NEUTRAL'\n",
    "            df.loc[df.compound<0,'sentiment_type']='NEG'\n",
    "            df.drop('scores', axis=1, inplace=True)# it's dict score of pos/neg/neutral\n",
    "            df['sentiment_type'] = df['sentiment_type'].map({'POS':2, 'NEG':1, 'NEUTRAL':0})\n",
    "            \n",
    "        if 'POS' in extract_feat_list:\n",
    "            print('2.2.4 executing POS')\n",
    "            pos_family = {\n",
    "            'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "            'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "            'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "            'adj' :  ['JJ','JJR','JJS'],\n",
    "            'adv' : ['RB','RBR','RBS','WRB']\n",
    "            }\n",
    "            \n",
    "            # function to check and get the part of speech tag count of a words in a given sentence\n",
    "            def check_pos_tag(x, flag):\n",
    "                cnt = 0\n",
    "                try:\n",
    "                    wiki = textblob.TextBlob(x)\n",
    "                    for tup in wiki.tags:\n",
    "                        ppo = list(tup)[1]\n",
    "                        if ppo in pos_family[flag]:\n",
    "                            cnt += 1\n",
    "                except:\n",
    "                    pass\n",
    "                return cnt\n",
    "            df['noun_count'] = df[text_col].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "            df['verb_count'] = df[text_col].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "            df['adj_count'] = df[text_col].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "            df['adv_count'] = df[text_col].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "            df['pron_count'] = df[text_col].apply(lambda x: check_pos_tag(x, 'porn'))\n",
    "            \n",
    "            #not working / TBT\n",
    "#             df.assign(\n",
    "#             noun_count = df[text_col].apply(lambda x: check_pos_tag(x, 'noun')),\n",
    "#             verb_count = df[text_col].apply(lambda x: check_pos_tag(x, 'verb')),\n",
    "#             adj_count = df[text_col].apply(lambda x: check_pos_tag(x, 'adj')),\n",
    "#             adv_count = df[text_col].apply(lambda x: check_pos_tag(x, 'adv')),\n",
    "#             pron_count = df[text_col].apply(lambda x: check_pos_tag(x, 'pron'))\n",
    "#                  )\n",
    "            \n",
    "        if 'NamedEntity' in extract_feat_list:\n",
    "            pass\n",
    "            #Named-Entity Recognition - TBT\n",
    "        #     import spacy\n",
    "        #     ner = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "        #     df[\"tags\"] = df[text_col].apply(lambda x: [(tag.text, tag.label_) for tag in ner(x).ents]) ## tag text and exctract tags into a list\n",
    "        #     df[\"tags\"] = df[\"tags\"].apply(lambda x: utils_lst_count(x)) ## count tags\n",
    "\n",
    "        #     ## extract features\n",
    "        #     tags_set = []\n",
    "        #     for lst in df[\"tags\"].tolist():\n",
    "        #          for dic in lst:\n",
    "        #                 for k in dic.keys():\n",
    "        #                     tags_set.append(k[1])\n",
    "        #     tags_set = list(set(tags_set))\n",
    "        #     for feature in tags_set:\n",
    "        #          df[\"tags_\"+feature] = df[\"tags\"].apply(lambda x: utils_ner_features(x, feature))\n",
    "\n",
    "   \n",
    "    if extract_feat_dict:\n",
    "        print('##3 extract feat w.r.t extract_feat_dict external values')\n",
    "        if 'TokensCountList' in extract_feat_dict:\n",
    "            print('#3.1 TokensCountList')\n",
    "            TokensCountList = extract_feat_dict['TokensCountList']\n",
    "            if (isinstance(TokensCountList, str)): # when it's str\n",
    "                TokensCountList = list(TokensCountList)\n",
    "            if TokensCountList:\n",
    "                df['TokensCount'] = df[text_col].apply(lambda x: len([w for w in str(x).lower().split() if w in TokensCountList]))\n",
    "\n",
    "        if 'TokensPresentList' in extract_feat_dict:\n",
    "            print('#3.2 TokensPresentList')\n",
    "            TokensPresentList = extract_feat_dict['TokensPresentList']\n",
    "            if (isinstance(TokensPresentList, str)): # when it's str\n",
    "                TokensPresentList = list(TokensPresentList)\n",
    "            if TokensPresentList:\n",
    "                df['TokensPresent'] = df[text_col].str.contains('|'.join(TokensPresentList), case=False)\n",
    "                df['TokensPresent'] = df['TokensPresent'].astype(int) # convert(True/False-->1/0)\n",
    "        \n",
    "    \n",
    "    if cleaning_list:\n",
    "        print('##4 clean text w.r.t cleaning_list flag')\n",
    "        print(f'df-type:{type(df)}, df-len:{len(df)}, df-shp:{df.shape},\\\n",
    "        df-cols:{df.columns.tolist()}, text_col:{text_col}')\n",
    "        \n",
    "        #3.1 internal inbuilt functions used for cleaning\n",
    "        if 'LowerCase' in cleaning_list:\n",
    "            df[text_col] = df[text_col].apply(lambda x: cleaner.lower_case_text(x))\n",
    "        if 'RemovePunct' in cleaning_list:\n",
    "            df[text_col] = df[text_col].apply(lambda x: cleaner.remove_punctuation(x))\n",
    "        if 'RemoveUnicode' in cleaning_list:\n",
    "            df[text_col] = df[text_col].apply(lambda x: cleaner.remove_unicode(x))\n",
    "        if 'RemoveLeadTrailWhiteSpaces' in cleaning_list:\n",
    "            df[text_col] = df[text_col].apply(lambda x: cleaner.remove_leading_trailing_whitespaces(x))\n",
    "        if 'RemoveDupWhiteSpaces' in cleaning_list:\n",
    "            df[text_col] = df[text_col].apply(lambda x: cleaner.remove_duplicate_whitespaces(x))\n",
    "        if 'CorrectGrammar' in cleaning_list:\n",
    "            df[text_col] = df[text_col].apply(lambda x: cleaner.correct_grammar(x))\n",
    "        if 'RemoveStopwords' in cleaning_list:\n",
    "            df[text_col] = df[text_col].apply(lambda x: cleaner.remove_stopwords(x))\n",
    "        if 'ApplyStem' in cleaning_list:\n",
    "            df[text_col] = df[text_col].apply(lambda x: cleaner.apply_stemming(x))\n",
    "        if 'ApplyLamma' in cleaning_list:\n",
    "            df[text_col] = df[text_col].apply(lambda x: cleaner.apply_lammatization(x))\n",
    "        if 'RemoveHashTags' in cleaning_list:\n",
    "            df[text_col] = df[text_col].apply(lambda x: cleaner.remove_hashtags(x))\n",
    "        if 'RemoveHyperLinks' in cleaning_list:\n",
    "            df[text_col] = df[text_col].apply(lambda x: cleaner.remove_hyperlinks(x))\n",
    "        if 'CleanHtmlCode' in cleaning_list:\n",
    "            df[text_col] = df[text_col].apply(lambda x: cleaner.clean_html_code(x))\n",
    "        if 'ReplaceAbbr' in cleaning_list:\n",
    "            df[text_col] = df[text_col].apply(lambda x: cleaner.replace_contraction(x))\n",
    "            \n",
    "        #3.2 external functions used for cleaning\n",
    "        if 'NoiseRemoval' in cleaning_list:\n",
    "            df[text_col]=df[text_col].apply(lambda x : \" \".join([scrub_words(word) for word in str(x).split()]))\n",
    "            \n",
    "        if 'RemoveCustWords' in cleaning_list:#not added in list\n",
    "            df[text_col] = df[text_col].apply(RemoveCustomizeWords)\n",
    "            df[text_col] = df[text_col].apply(lambda x :RemoveCustomizeWords(x))\n",
    "            \n",
    "        if 'ReplaceExtAbbr' in cleaning_list: #not added in list\n",
    "            df[text_col]=df[text_col].apply(lambda x:expand_contractions(x))\n",
    "\n",
    "    else:\n",
    "        # default method to clean single sentence/\n",
    "        df[text_col] = df[text_col].apply(lambda x: cleaner.clean_single_sentence(x))\n",
    "        # default method to get cleaned sentences from a paragraph\n",
    "        #df[text_col] = df[text_col].apply(lambda x: cleaner.clean_paragraph_to_sentences(x))\n",
    "        # default method to clean complete paragraph\n",
    "        #df[text_col] = df[text_col].apply(lambda x: cleaner.clean_paragraph(x))\n",
    "        \n",
    "    print('##5 clean text w.r.t cleaning_dict external values')\n",
    "    #print(f'df-type:{type(df)}, df-len:{len(df)}, df-data:{df}')\n",
    "    print(f'df-type:{type(df)}, df-shp:{df.shape}')\n",
    "    if cleaning_dict:\n",
    "        cnt = Counter()\n",
    "        for text in df[text_col].values:\n",
    "            for word in text.split():\n",
    "                cnt[word] += 1\n",
    "\n",
    "        if 'RemoveCommonWords' in cleaning_dict:\n",
    "            print('#5.1 RemoveCommonWords')\n",
    "            RemoveCommonWords = cleaning_dict['RemoveCommonWords']\n",
    "            if RemoveCommonWords:\n",
    "                FREQWORDS = set([w for (w, wc) in cnt.most_common(RemoveCommonWords)])\n",
    "                df[text_col]=df[text_col].apply(lambda x : \" \".join([word for word in str(x).split() if word not in FREQWORDS]))\n",
    "        if 'RemoveRareWords' in cleaning_dict:\n",
    "            print('#5.2 RemoveRareWords')\n",
    "            RemoveRareWords = cleaning_dict['RemoveRareWords']\n",
    "            if RemoveRareWords:\n",
    "                RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-RemoveRareWords-1:-1]])\n",
    "                df[text_col]=df[text_col].apply(lambda x : \" \".join([word for word in str(x).split() if word not in RAREWORDS]))\n",
    "\n",
    "        if 'ExtRemoveStopWords' in cleaning_dict:\n",
    "            print('#5.3 ExtRemoveStopWords')\n",
    "            #from nltk.corpus import stopwords\n",
    "            #stop=set(stopwords.words('english'))\n",
    "            #internal_STOPWORDS_list = list(stop)\n",
    "            ExtRemoveStopWords = cleaning_dict['ExtRemoveStopWords']\n",
    "\n",
    "            if (isinstance(ExtRemoveStopWords, str)): # when it's str\n",
    "                ExtRemoveStopWords = list(ExtRemoveStopWords)\n",
    "            #ExtRemoveStopWords = ExtRemoveStopWords + internal_STOPWORDS_list\n",
    "            if ExtRemoveStopWords:\n",
    "                df[text_col]=df[text_col].apply(lambda x : \" \".join([word for word in str(x).split() if word not in ExtRemoveStopWords]))\n",
    "\n",
    "        if 'RemovePattern' in cleaning_dict:\n",
    "            print('#5.4 RemovePattern')\n",
    "            RemovePattern = cleaning_dict['RemovePattern']\n",
    "            if RemovePattern:\n",
    "                df[text_col] = np.vectorize(RemoveTextPattern)(df[text_col], RemovePattern) # function RemoveTextPattern()\n",
    "\n",
    "        if 'RemoveShortWords' in cleaning_dict: #remove_short_words_length\n",
    "            print('#5.5 RemoveShortWords')\n",
    "            RemoveShortWords = cleaning_dict['RemoveShortWords']\n",
    "            if RemoveShortWords:\n",
    "                df[text_col] = df[text_col].apply(lambda x: ' '.join([w for w in x.split() if len(w)>RemoveShortWords]))\n",
    "\n",
    "    print('##6 ouput format')\n",
    "    #print(f'df-type:{type(df)}, df-len:{len(df)}, df-data:{df}')\n",
    "    print(f'df-type:{type(df)}, df-shp:{df.shape}')\n",
    "    #print('df-cols:', df.columns.tolist())\n",
    "    if series_out: # return series / only cleaning\n",
    "        print('if series_out:', series_out)\n",
    "        return df[text_col].squeeze()\n",
    "    else: #testing after clean / cleaning&feat_extract\n",
    "        print('else series_out:', series_out)\n",
    "        df['word_count'] = df[text_col].apply(lambda x: len(str(x).split()))\n",
    "        df['char_count'] = df[text_col].apply(len)\n",
    "        #print(f'1df-type:{type(df)}, df-len:{len(df)}, df-data:{df}')\n",
    "        print(f'1df-type:{type(df)}, df-len:{len(df)}, df-shp:{df.shape}')\n",
    "        \n",
    "        #change column position for better view Nov15,2022\n",
    "        first_column = df.pop('word_count')\n",
    "        df.insert(0, 'word_count', first_column)\n",
    "        second_column = df.pop('char_count')\n",
    "        df.insert(1, 'char_count', second_column)\n",
    "        #print(f'2df-type:{type(df)}, df-len:{len(df)}, df-data:{df}')\n",
    "        print(f'2df-type:{type(df)}, df-len:{len(df)}, df-shp:{df.shape}')\n",
    "        \n",
    "    #print('6.1 df-head-testing1:', df.head())#Mar02,2023\n",
    "    print('df-cols:', df.columns.tolist())\n",
    "    \n",
    "    if drop_text_col:\n",
    "        print('##7 drop_text_col:', drop_text_col)\n",
    "        df.drop(text_col, axis=1, inplace=True)\n",
    "        \n",
    "    #if add_num_feat and extract_feat_list:#v1.3\n",
    "    if add_num_feat:#Mar10,2023\n",
    "        print('##8 merging num features')\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            num_cols_list = list(data.select_dtypes('number').columns)\n",
    "            if num_cols_list:\n",
    "                df1 = data[num_cols_list]\n",
    "                df = pd.concat([df, df1], axis=1)\n",
    "                \n",
    "    print('df-cols:', df.columns.tolist())    \n",
    "    print(f'before drop single value cols>>df-shp:{df.shape}, df-cols:{df.columns.tolist()}')\n",
    "    if drop_single_value_cols: #v1.4\n",
    "        df = df[[c for c in list(df) if len(df[c].unique()) > 1]]\n",
    "    print(f'After drop single value cols>>df-shp:{df.shape}, df-cols:{df.columns.tolist()}')\n",
    "    #print('8 df-head-testing2:', df.head())#Mar02,2023\n",
    "    #print(df.info())\n",
    "    \n",
    "    \n",
    "    #convert catg/obj to num\n",
    "    if extract_feat_list: #v1.4\n",
    "        cat_cols_list = list(df.select_dtypes(['category', 'object']).columns)\n",
    "        print('cat_cols_list:', cat_cols_list)\n",
    "\n",
    "        for col in cat_cols_list:\n",
    "            print('###9 convert catg/obj to num')\n",
    "            #map index\n",
    "            #sorted_indices = df[col].value_counts().index\n",
    "            #df[col] =  df[col].map(dict(zip(sorted_indices, range(1, len(sorted_indices)+1))))\n",
    "\n",
    "            #map % distribution\n",
    "            sorted_pert = df[col].value_counts(normalize=True).to_dict()\n",
    "            df[col] =  df[col].map(sorted_pert)\n",
    "    \n",
    "    #print('9 df-head-testing3:', df.head())#Mar02,2023\n",
    "    print('df-cols:', df.columns.tolist())\n",
    "    print('exit text_clean_feat_extract')\n",
    "    return df\n",
    "\n",
    "text_cleaning_list=['LowerCase', 'RemovePunct',  \n",
    "          'RemoveUnicode', 'RemoveLeadTrailWhiteSpaces', 'RemoveDupWhiteSpaces',\n",
    "          'CorrectGrammar', 'RemoveStopwords', 'ApplyStem', 'ApplyLamma',\n",
    "          'RemoveHashTags', 'RemoveHyperLinks', 'CleanHtmlCode', 'ReplaceAbbr'] \n",
    "\n",
    "text_cleaning_dict = {'ExtRemoveStopWords':['a'], 'RemoveCommonWords':10, 'RemoveRareWords':10,\n",
    "'RemovePattern':\"@[w]*\", 'RemoveShortWords':2}\n",
    "\n",
    "# text_extract_feat_list = ['DetectLang', 'SplitIntoSent', 'SplitIntoWords',\n",
    "# 'StopWordsCount', 'TokensFeat', 'SentiMent', 'POS', 'NamedEntity']\n",
    "\n",
    "text_extract_feat_list = ['SplitIntoSent', 'SplitIntoWords',\n",
    "'StopWordsCount', 'TokensFeat', 'SentiMent', 'POS', 'NamedEntity']\n",
    "\n",
    "text_extract_feat_dict = {'TokensCountList':['a'], 'TokensPresentList':['a']}\n",
    "\n",
    "%store text_cleaning_list\n",
    "%store text_cleaning_dict\n",
    "%store text_extract_feat_list\n",
    "%store text_extract_feat_dict"
   ]
  },
  {
   "cell_type": "raw",
   "id": "afd1877c",
   "metadata": {},
   "source": [
    "#testing\n",
    "import pandas as pd\n",
    "\n",
    "data_path = 'D:\\\\dataset\\\\'\n",
    "\n",
    "df = pd.read_csv(data_path+'fake_df_nlp.csv', encoding=\"ISO-8859-1\", index_col = 0)\n",
    "#fake_df_nlp.to_excel(data_path+'fake_df_nlp.xlsx')\n",
    "\n",
    "num_cols_list = list(df.select_dtypes('number').columns)\n",
    "target_col = 'gender'\n",
    "text_col = 'feedback'\n",
    "df.head(1)\n",
    "\n",
    "num_cols_list.append(text_col)\n",
    "X = df[num_cols_list]\n",
    "y = df[target_col]\n",
    "\n",
    "cleaning_list = []\n",
    "#cleaning_list = text_cleaning_list\n",
    "#feat_list = []\n",
    "feat_list =['POS', 'DetectLang']\n",
    "encoding_type = True\n",
    "\n",
    "#test function\n",
    "df_clean_feat = text_clean_feat_extract(data=X, text_col=text_col, cleaning_list=cleaning_list, cleaning_dict={},\n",
    "        extract_feat_list=feat_list, extract_feat_dict={}, series_out=False, drop_text_col=True,\n",
    "                           encoding_type=encoding_type, add_num_feat=True)\n",
    "\n",
    "df_clean_feat.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9df3a403",
   "metadata": {},
   "source": [
    "# common feature\n",
    "\n",
    "#https://www.analyticsvidhya.com/blog/2022/09/complete-guide-to-analyzing-movie-reviews-using-nlp/\n",
    "\n",
    "import syllables\n",
    "import textstat\n",
    "\n",
    "#3. Create Feature: Syllables\n",
    "df['Total_Syllables'] = df['Review'].apply(lambda x : syllables.estimate(x))\n",
    "df['Average_Syllables'] = df['Total_Syllables']/df['Review_Words']\n",
    "\n",
    "#4. Create Feature: Flesch Reading Ease\n",
    "df['flesch_reading_ease'] = df['Review'].apply(lambda x : textstat.flesch_reading_ease(x) )\n",
    "# Example of a negative readability example\n",
    "a = df.sort_values(by='flesch_reading_ease').head().iloc[1]\n",
    "\n",
    "[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ef63dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_embedding(data, fillna=True, vector_size=300, ngram=2, min_count=2, window=5,\n",
    "                         text_col=None, drop_text_col=True, embedding_type='Word2Vec'):\n",
    "    '''\n",
    "    gensim_embedding_w2v replaced by gensim_embedding\n",
    "    \n",
    "    v1.3 : Mar18,2023-updated for FastText\n",
    "    v1.2 : Dec15,2022-updated for Doc2Vec\n",
    "    v1.1 : Nov11,2022-updated for num cols\n",
    "    v1.0 : Oct28,2022-initial version\n",
    "    \n",
    "    embedding_type: Doc2Vec/Word2Vec/FastText\n",
    "    '''\n",
    "    print('entry gensim_embedding')\n",
    "    from gensim.models import Word2Vec, Doc2Vec\n",
    "    from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "    from gensim.models.fasttext import FastText\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    \n",
    "    \n",
    "    print('data-typ:', type(data))\n",
    "   \n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        df = data.copy()\n",
    "        text_col = text_col\n",
    "        #text_col = 'text' #Nov09,2022\n",
    "    elif isinstance(data, pd.Series):  #convert i/p series to df for processing Oct20,2022\n",
    "        df = pd.DataFrame(data)\n",
    "        text_col = data.name\n",
    "    elif (isinstance(data, np.ndarray)) | (isinstance(data, list)):\n",
    "        df = pd.DataFrame(data)\n",
    "        text_col = 0\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    data1 = df[text_col]\n",
    "    # Training a word2vec model from the given data set\n",
    "    if embedding_type=='Word2Vec':\n",
    "        model = Word2Vec(data1, vector_size=vector_size, min_count=min_count, window=window, sg=ngram, workers=4, epochs=10)\n",
    "    elif embedding_type=='Doc2Vec':\n",
    "        tagged_data = [TaggedDocument(words = word_tokenize(_d.lower()), tags = [str(i)]) for i, _d in enumerate(data1)]\n",
    "        model = Doc2Vec(tagged_data, vector_size=vector_size, min_count=min_count, window=window, workers=4, epochs=10)\n",
    "    \n",
    "    elif embedding_type=='FastText':\n",
    "        down_sampling = 1e-2\n",
    "        model = FastText(data1, vector_size=vector_size, window=window, min_count=min_count,\n",
    "                      sample=down_sampling, workers = 4, sg=ngram, epochs=10)\n",
    "        \n",
    "    words_list = list(model.wv.index_to_key)\n",
    "    words_dict = model.wv.key_to_index\n",
    "    \n",
    "    def get_embedding(doc_tokens):\n",
    "        embeddings = []\n",
    "        if len(doc_tokens)<1:\n",
    "            return np.zeros(vector_size)\n",
    "        else:\n",
    "            for tok in doc_tokens:\n",
    "                if tok in model.wv.index_to_key:\n",
    "                    embeddings.append(model.wv.word_vec(tok))\n",
    "                else:\n",
    "                    embeddings.append(np.random.rand(vector_size))\n",
    "            # mean the vectors of individual words to get the vector of the document\n",
    "            return np.mean(embeddings, axis=0)\n",
    "        \n",
    "    df['vector']=df[text_col].apply(lambda x :get_embedding(x.split()))\n",
    "    \n",
    "    #expend vector into diff cols\n",
    "    df = (df.drop(columns=['vector'])\n",
    "         .join(df['vector'].apply(pd.Series).add_prefix('vec_'))\n",
    "         .fillna('') # optional\n",
    "      )\n",
    "    if drop_text_col:\n",
    "        df.drop(text_col, axis=1, inplace=True)\n",
    "    print('exit gensim_embedding')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee2a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_col(df, combine_all_cat_cols=True):\n",
    "    '''\n",
    "    ver 1.4 Feb07,2023 added combine_all_cat_cols\n",
    "    ver 1.3 Feb01,2023 added print for testing\n",
    "    ver 1.2 Dec09,2022 moved n_token from argument to hardcoded temp to work with function_transformer\n",
    "    ver 1.1 Oct29,2022 updated combined_text by ' ' instead of '_'\n",
    "    ver 1.0 Oct12,2022 initial version\n",
    "    \n",
    "    check for text col, if more than 1 text col then combine them\n",
    "    rename text col to 'text'\n",
    "    return text col name for NLP processing\n",
    "    '''\n",
    "    print('Entry extract_text_col')\n",
    "    from copy import deepcopy\n",
    "    \n",
    "    n_token=10 # TBA as arugment \n",
    "    \n",
    "    print('df-cols:', df.columns.tolist())\n",
    "    df1 = df.copy()\n",
    "    #df1 = deepcopy(df)\n",
    "    #df1['text1'] = df1['v2'] # temp for testing\n",
    "    text_col_list = [col for col in df1.columns if (df1[col].apply(lambda x: len(str(x).split())).max()) > n_token]\n",
    "    print('text_col_list start:', text_col_list, len(text_col_list))\n",
    "    \n",
    "    if len(text_col_list) == 1:\n",
    "        text_col = text_col_list[0]\n",
    "    elif len(text_col_list) > 1:\n",
    "        df1['combined_text'] = df1[text_col_list].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "        text_col = 'combined_text'\n",
    "        df1.drop(text_col_list, inplace=True, axis=1)\n",
    "    else:\n",
    "        text_col = None\n",
    "    print('text_col:', text_col)\n",
    "    \n",
    "    df1.rename(columns={text_col: 'text'},  inplace=True)\n",
    "    print('df1-cols:', df1.columns.tolist())\n",
    "    \n",
    "    if combine_all_cat_cols: #v1.4\n",
    "        cat_cols_list = list(df1.select_dtypes(['category', 'object']).columns)\n",
    "        if cat_cols_list:\n",
    "            df1['combined_text_catg'] = df1[df1.columns[0:]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)\n",
    "            df1.drop(cat_cols_list, axis=1, inplace=True)\n",
    "            df1.rename(columns={'combined_text_catg': 'text'},  inplace=True)\n",
    "            \n",
    "    print('df1-cols after combine_all_cat_cols:', df1.columns.tolist())\n",
    "    \n",
    "    print('Exit extract_text_col')\n",
    "    return df1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd08402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_BoW_feature_selection(X, y, analyzer, max_df, min_df, max_features, ngram_range, \n",
    "             matrix_type='sparse', BoW_type='CountVect', p_value_limit = 0.95,\n",
    "            feature_selection='chi2_test', text_col=None, refit=True):\n",
    "    \n",
    "    '''\n",
    "    v1.2 Nov23,2022 bug fixed\n",
    "    v1.1 Nov18,2022 updated for FunctionTransformer/ renamed function text_BoW_feature_selection_train()\n",
    "    v1.0: initial verX_valid, \n",
    "\n",
    "    BoW_type : Count_vect / TfIdf_vect\n",
    "    matrix_type : sparse/dense\n",
    "    refit:  if True then \n",
    "    '''\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "    from sklearn.feature_selection import chi2\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    print('entry text_BoW_feature_selection')\n",
    "    print(f'analyzer:{analyzer}, max_df:{max_df}, min_df:{min_df}, max_features:{max_features},\\\n",
    "    ngram_range:{ngram_range}, BoW_type:{BoW_type}, text_col:{text_col}, refit:{refit}')\n",
    "    \n",
    "    data = X.copy()\n",
    "    print('data-typ b4 chg:', type(data))\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        if len(data.columns)==1: # Nov09,2022\n",
    "            X = pd.DataFrame(data.values)\n",
    "            text_col = 0\n",
    "        else: #when data has num cols Nov09,2022\n",
    "            text_col = text_col\n",
    "            X = data.copy()\n",
    "    elif isinstance(data, pd.Series): #convert i/p series to df for processing Oct20,2022\n",
    "        X = pd.DataFrame(data)\n",
    "        text_col = data.name\n",
    "    elif (isinstance(data, np.ndarray)) | (isinstance(data, list)):\n",
    "        X = pd.DataFrame(data)\n",
    "        text_col = 0\n",
    "    else:\n",
    "        pass\n",
    "    print('X-typ after chg:', type(X))\n",
    "    \n",
    "    #1 BoW_type selection\n",
    "    if BoW_type == 'CountVect':\n",
    "        vect = CountVectorizer(analyzer=analyzer, max_df=max_df, min_df=min_df,\n",
    "                               max_features=max_features, ngram_range= ngram_range)\n",
    "    elif BoW_type == 'TfIdfVect':\n",
    "        vect = TfidfVectorizer(analyzer=analyzer, max_df=max_df, min_df=min_df,\n",
    "                               max_features=max_features, ngram_range= ngram_range)\n",
    "    \n",
    "    train_vect = vect.fit_transform(X[text_col])\n",
    "\n",
    "    dic_vocabulary = vect.vocabulary_ # dict of ngrams key with count\n",
    "    feat_list_all = vect.get_feature_names_out() # max_df features from ngrams\n",
    "    print('#1 BoW_type selection - feat_list_all:', len(feat_list_all))\n",
    "    \n",
    "    #2 imp feature selection w.r.r chi_test and p-value\n",
    "    if feature_selection:\n",
    "        dtf_features = pd.DataFrame()\n",
    "        for cat in np.unique(y):\n",
    "            chi, p = chi2(train_vect, y==cat)\n",
    "            dtf_features = dtf_features.append(pd.DataFrame({\"feature\":feat_list_all, \"score\":1-p, \"y\":cat}))\n",
    "            dtf_features = dtf_features.sort_values([\"y\", \"score\"], ascending=[True, False])\n",
    "            dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\n",
    "        feat_list_selected = dtf_features[\"feature\"].unique().tolist() # imp features from ngrams, selected w.r.t p-value of chi2 test\n",
    "        print('#2 feature_selection - feat_list_selected:', len(feat_list_selected))\n",
    "        \n",
    "    #3 refit the vectorizer on the corpus by giving this new set of words as input, added vocabulary\n",
    "    if refit:\n",
    "        if BoW_type == 'CountVect':\n",
    "            vect = CountVectorizer(analyzer=analyzer, max_df=max_df, min_df=min_df,\n",
    "                                   max_features=max_features, ngram_range= ngram_range, vocabulary=feat_list_selected)\n",
    "        elif BoW_type == 'TfIdfVect':\n",
    "            vect = TfidfVectorizer(analyzer=analyzer, max_df=max_df, min_df=min_df,\n",
    "                                   max_features=max_features, ngram_range= ngram_range, vocabulary=feat_list_selected)\n",
    "        train_vect = vect.fit_transform(X[text_col])\n",
    "        \n",
    "        # sparse to dense\n",
    "        train_vect = train_vect.todense() if matrix_type=='dense' else train_vect\n",
    "\n",
    "        dic_vocabulary = vect.vocabulary_ # dict of ngrams key with count\n",
    "        feat_list_final = vect.get_feature_names_out() # max_df features from ngrams\n",
    "        print(f'#3 refit yes - feat_list_all:{len(feat_list_all)}, feat_list_selected:{len(feat_list_selected)}, feat_list_final:{len(feat_list_final)}')\n",
    "        print('entry text_BoW_feature_selection')\n",
    "        return feat_list_final\n",
    "    else:\n",
    "        print(f'#3 refit no - feat_list_all:{len(feat_list_all)}, feat_list_selected:{len(feat_list_selected)}')\n",
    "        print('exit text_BoW_feature_selection')\n",
    "        return feat_list_selected\n",
    "    \n",
    "# from sklearn.preprocessing import FunctionTransformer # sklearn pipeline\n",
    "# BOW_feat_selection_ft = FunctionTransformer(text_BoW_feature_selection, kw_args= {'analyzer': analyzer,\n",
    "#     'max_df': max_df, 'min_df': min_df, 'max_features': max_features, 'ngram_range': ngram_range,\n",
    "#     'BoW_type': BoW_type, 'target': target, 'text_col': text_col, 'refit': refit})\n",
    "\n",
    "# from imblearn import FunctionSampler # imblearn pipeline\n",
    "# BOW_feat_selection_fs = FunctionSampler(func=text_BoW_feature_selection, validate=False, kw_args= {'analyzer': analyzer,\n",
    "#     'max_df': max_df, 'min_df': min_df, 'max_features': max_features, 'ngram_range': ngram_range,\n",
    "#     'BoW_type': BoW_type, 'target': target, 'text_col': text_col, 'refit': refit})\n",
    "\n",
    "# %store BOW_feat_selection_ft\n",
    "# %store BOW_feat_selection_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20cbc376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_BoW_feature_selection_train(X, y, analyzer, max_df, min_df, max_features, ngram_range, \n",
    "             matrix_type='sparse', BoW_type='Count_vect', p_value_limit = 0.95,\n",
    "            feature_selection='chi2_test'):\n",
    "    '''\n",
    "    v1.0: initial verX_valid, \n",
    "    \n",
    "    \n",
    "    BoW_type : Count_vect / TfIdf_vect\n",
    "    matrix_type : sparse/dense\n",
    "    '''\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "    from sklearn.feature_selection import chi2\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    if BoW_type == 'Count_vect':\n",
    "        vect = CountVectorizer(analyzer=analyzer, max_df=max_df, min_df=min_df,\n",
    "                               max_features=max_features, ngram_range= ngram_range)\n",
    "    elif BoW_type == 'TfIdf_vect':\n",
    "        vect = TfidfVectorizer(analyzer=analyzer, max_df=max_df, min_df=min_df,\n",
    "                               max_features=max_features, ngram_range= ngram_range)\n",
    "    \n",
    "    train_vect = vect.fit_transform(X)\n",
    "\n",
    "    dic_vocabulary = vect.vocabulary_ # dict of ngrams key with count\n",
    "    X_names = vect.get_feature_names_out() # max_df features from ngrams\n",
    "    \n",
    "    #imp feature selection w.r.r chi_test and p-value\n",
    "    if feature_selection:\n",
    "        dtf_features = pd.DataFrame()\n",
    "        for cat in np.unique(y):\n",
    "            chi, p = chi2(train_vect, y==cat)\n",
    "            dtf_features = dtf_features.append(pd.DataFrame({\"feature\":X_names, \"score\":1-p, \"y\":cat}))\n",
    "            dtf_features = dtf_features.sort_values([\"y\", \"score\"], ascending=[True, False])\n",
    "            dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\n",
    "        X_names = dtf_features[\"feature\"].unique().tolist() # imp features from ngrams, selected w.r.t p-value of chi2 test\n",
    "\n",
    "    # refit the vectorizer on the corpus by giving this new set of words as input, added vocabulary\n",
    "    if BoW_type == 'Count_vect':\n",
    "        vect = CountVectorizer(analyzer=analyzer, max_df=max_df, min_df=min_df,\n",
    "                               max_features=max_features, ngram_range= ngram_range, vocabulary=X_names)\n",
    "    elif BoW_type == 'TfIdf_vect':\n",
    "        vect = TfidfVectorizer(analyzer=analyzer, max_df=max_df, min_df=min_df,\n",
    "                               max_features=max_features, ngram_range= ngram_range, vocabulary=X_names)\n",
    "    \n",
    "    train_vect = vect.fit_transform(X)\n",
    "    # sparse to dense\n",
    "    train_vect = train_vect.todense() if matrix_type=='dense' else train_vect\n",
    "    \n",
    "    dic_vocabulary = vect.vocabulary_ # dict of ngrams key with count\n",
    "    X_names = vect.get_feature_names_out() # max_df features from ngrams\n",
    "    return vect, train_vect\n",
    "    \n",
    "    \n",
    "def text_BoW_feature_selection_test(vect, X_test, matrix_type='sparse'):\n",
    "    '''\n",
    "    v1.0 : inital ver\n",
    "    '''\n",
    "    test_vect = vect.fit_transform(X_test)\n",
    "    # sparse to dense\n",
    "    test_vect = test_vect.todense() if matrix_type=='dense' else test_vect\n",
    "    return test_vect\n",
    "\n",
    "def add_vect_text_features(vect, vect_matrix, text_feat_df):\n",
    "    '''\n",
    "    v1.0 : initial verison\n",
    "    1.1 : \n",
    "    \n",
    "    paramters:\n",
    "    vect: trained vector from count_vect or tfidf_vect\n",
    "    vect_matrix : count_vect or tfidf_vect output either dense or sparse\n",
    "    text_feat_df : text_features excluding text/target columns\n",
    "    \n",
    "    issues:\n",
    "    there could be text_col ='text' in vect_matrix so add some unique name to avoid error\n",
    "    \n",
    "    '''\n",
    "    import scipy\n",
    "    import pandas as pd\n",
    "    \n",
    "    # check vect_matrix sparse or dense, for df dense is needed\n",
    "    if scipy.sparse.issparse(vect_matrix): #sparse --> convert to dense\n",
    "        vect_df = pd.DataFrame(vect_matrix.todense(), columns=vect.get_feature_names())\n",
    "    else: #dense\n",
    "        vect_df = pd.DataFrame(vect_matrix, columns=vect.get_feature_names())\n",
    "    print('vect_df:', vect_df.shape)\n",
    "    \n",
    "    #merge vect with text_feat_df\n",
    "    vect_df.index = text_feat_df.index # if vect_df does not have original index\n",
    "    vect_df = pd.concat([vect_df, text_feat_df], axis=1)\n",
    "    print('vect_df-merged with feat_df:', vect_df.shape)\n",
    "    return vect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da53db05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "\n",
    "#Spelling Correction\n",
    "#from textblob import TextBlob\n",
    "#incorrect_text = 'any tezt with for checing'\n",
    "#textblob = TextBlob(incorrect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4da166e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing/cleaning TBC\n",
    "#https://medium.com/@rukshanjayasekara/text-classification-with-fasttext-5cac26ce7bc6\n",
    "\n",
    "# Handling the retweet(RT) tag\n",
    "def replace_retweet(tweet, default_replace=\"\"):\n",
    "    tweet = re.sub('RT\\s+', default_replace, tweet)\n",
    "    return tweet\n",
    "# Handling user tags(@)\n",
    "def replace_user(tweet, default_replace=\"user\"):\n",
    "    tweet = re.sub('\\B@\\w+', default_replace, tweet)\n",
    "    return tweet\n",
    "# Replace emojis with meaningful text\n",
    "def demojize(tweet):\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    return tweet\n",
    "def replace_url(tweet, default_replace=\"\"):\n",
    "    tweet = re.sub('(http|https):\\/\\/\\S+', default_replace, tweet)\n",
    "    return tweet\n",
    "# Remove hashtag symbol(#)\n",
    "def replace_hashtag(tweet, default_replace=\"\"):\n",
    "    tweet = re.sub('#+', default_replace, tweet)\n",
    "    return tweet\n",
    "def to_lowercase(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    return tweet\n",
    "def word_repetition(tweet):\n",
    "    tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
    "    return tweet\n",
    "def punct_repetition(tweet, default_replace=\"\"):\n",
    "    tweet = re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', default_replace, tweet)\n",
    "    return tweet\n",
    "# Replace contractions with their extended forms\n",
    "def fix_contractions(tweet):\n",
    "    tweet = contractions.fix(tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocessor(tweet, verbose=False):\n",
    "    if verbose: print(\"Initial tweet: {}\".format(tweet))\n",
    "## Twitter Features\n",
    "    tweet = replace_retweet(tweet) # replace retweet\n",
    "    tweet = replace_user(tweet, \"\") # replace user tag\n",
    "    tweet = replace_url(tweet) # replace url\n",
    "    tweet = replace_hashtag(tweet) # replace hashtag\n",
    "    if verbose: print(\"Post Twitter processing tweet: {}\".format(tweet))\n",
    "## Word Features\n",
    "    tweet = to_lowercase(tweet) # lower case\n",
    "    tweet = fix_contractions(tweet) # replace contractions\n",
    "    tweet = punct_repetition(tweet) # replace punctuation repetition\n",
    "    tweet = word_repetition(tweet) # replace word repetition\n",
    "    tweet = demojize(tweet) # replace emojis\n",
    "    if verbose: print(\"Post Word processing tweet: {}\".format(tweet))\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1d8f161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tab NN Models\n",
    "\n",
    "def create_corpus_ListsOfList(df, text_col='text'): #not used yet\n",
    "    '''\n",
    "    v1.0: initital draft\n",
    "    \n",
    "    returns: lists of list of tokens in text\n",
    "    \n",
    "    '''\n",
    "    from tqdm import tqdm\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    stop=set(stopwords.words('english'))\n",
    "    corpus=[]\n",
    "    for tweet in tqdm(df[text_col]):\n",
    "        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "def build_vocab_count_dict(X):\n",
    "    '''\n",
    "    v1.0: initital draft\n",
    "    \n",
    "    X = df['text']\n",
    "    returns: token's count dict\n",
    "    \n",
    "    '''\n",
    "    tweets = X.apply(lambda s: s.split()).values      \n",
    "    vocab_count_dict = {}\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            try:\n",
    "                vocab_count_dict[word] += 1\n",
    "            except KeyError:\n",
    "                vocab_count_dict[word] = 1                \n",
    "    return vocab_count_dict\n",
    "\n",
    "def check_embeddings_coverage(embeddings, vocab_count_dict):\n",
    "    '''\n",
    "    v1.0: initital draft\n",
    "    n_covered: Words in the intersection of vocab and embeddings are stored in covered along with their counts.\n",
    "    n_oov: Words in vocab that don't exist in embeddings are stored in oov along with their counts.\n",
    "    n_covered and n_oov are total number of counts and they are used for calculating coverage percentages.\n",
    "    '''\n",
    "    vocab = vocab_count_dict \n",
    "    import operator\n",
    "    \n",
    "    covered = {}\n",
    "    oov = {}    \n",
    "    n_covered = 0\n",
    "    n_oov = 0\n",
    "    \n",
    "    for word in vocab:\n",
    "        try:\n",
    "            covered[word] = embeddings[word]\n",
    "            n_covered += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            n_oov += vocab[word]\n",
    "            \n",
    "    vocab_coverage = len(covered) / len(vocab) # %match of text-vocab with embedding\n",
    "    text_coverage = (n_covered / (n_covered + n_oov)) # %match of text-vocab with embedding\n",
    "    print(f'vocab:{len(vocab)}, covered:{len(covered)}, oov:{len(oov)}')\n",
    "    \n",
    "    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "    return sorted_oov, vocab_coverage, text_coverage, n_covered, n_oov\n",
    "\n",
    "#corpus = create_corpus(df=tweet, text_col='text')\n",
    "def text_to_int(X_train_text, X_valid_text, padding_maxlen=100):\n",
    "    '''\n",
    "    v1.0 : initial ver\n",
    "    \n",
    "    chk:\n",
    "    #padding_maxlen = X_train_seq.shape[1] #TBC , TBT\n",
    "    '''\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "    #1 Tokenize the sentences\n",
    "    tokenizer = Tokenizer()\n",
    "    \n",
    "    #2 preparing vocabulary\n",
    "    tokenizer.fit_on_texts(list(X_train_text)) # fit on X_train\n",
    "    #word_index = token.word_index #TBC\n",
    "    vocab_size=len(tokenizer.word_index) + 1 #+1 for padding, not used dueto high dimension\n",
    "    \n",
    "    #3 converting text into integer sequences\n",
    "    #X_train_seq=tokenizer.texts_to_sequences(corpus) # passed corpus (lists of list of words/sentence)\n",
    "    X_train_seq  = tokenizer.texts_to_sequences(X_train_text)\n",
    "    X_valid_seq  = tokenizer.texts_to_sequences(X_valid_text)\n",
    "    \n",
    "    #4 padding to prepare sequences of same length\n",
    "    #X_train_seq  = pad_sequences(X_train_seq, maxlen=padding_maxlen,truncating='post',padding='post') # TBC/Ref\n",
    "    #padding_maxlen = X_train_seq.shape[1] #TBC , TBT\n",
    "    X_train_seq  = pad_sequences(X_train_seq, maxlen=padding_maxlen)\n",
    "    X_valid_seq  = pad_sequences(X_valid_seq, maxlen=padding_maxlen)\n",
    "    \n",
    "    word_index=tokenizer.word_index\n",
    "    print('Number of unique words:', len(word_index))\n",
    "\n",
    "    return X_train_seq, X_valid_seq, vocab_size, tokenizer\n",
    "\n",
    "def load_Glove_embedding_vector(File):\n",
    "    import numpy as np\n",
    "    print(\"Loading Glove Model\")\n",
    "    glove_embedding_vector = {}\n",
    "    with open(File,'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array(split_line[1:], dtype=np.float32)\n",
    "            glove_embedding_vector[word] = embedding\n",
    "        f.close()\n",
    "    #print(f\"{len(glove_embedding_vector)} words loaded!\")\n",
    "    return glove_embedding_vector\n",
    "\n",
    "def embedding_vector_2_matrix(vocab_size, tokenizer, glove_embedding_vector):\n",
    "    '''\n",
    "    '''\n",
    "    from numpy import zeros\n",
    "    # create a weight matrix for words in training docs\n",
    "    vector_size = len(list(glove_embedding_vector.values())[0]) # find vector size\n",
    "    \n",
    "    embedding_matrix = zeros((vocab_size, vector_size))\n",
    "    \n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = glove_embedding_vector.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "    \n",
    "def build_NN_text_class_model(vocab_size, vector_size, X_train_seq, y_train, X_valid_seq, y_valid,\n",
    "                               trained_weight=None, model_type='learning_embeddings_NN',\n",
    "                               num_classes = 1, score_metrics = 'accuracy', DropOutRate=0.25, optimizer = 'Adam',\n",
    "                               n_epochs = 1, batch_size=32, n_neurons= 50, learning_rate=0.01, l2_loss_lambda=0.10,\n",
    "                               model_save_path='D:\\\\NLP\\\\models\\\\NN_models', test_type='XYZ'):\n",
    "    '''\n",
    "    model_type: learning_embeddings_LSTM / trained_embeddings\n",
    "    X_train_seq.shape[1]\n",
    "    \n",
    "    1.0 initial ver\n",
    "    1.1 updated return and added model_eval_dict\n",
    "    \n",
    "    chk\n",
    "    y_train can be used to find num_classes , binary - 2\n",
    "    '''\n",
    "    \n",
    "    #create model save name\n",
    "    model_save_pathname = model_save_path+'/'+model_type+'_'+test_type+'.h5'\n",
    "    \n",
    "        \n",
    "    import tensorflow as tf\n",
    "    \n",
    "    #set loss and output_activation w.r.t num_classes\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, LeakyReLU, GlobalMaxPooling2D\n",
    "    #from keras.layers.core import Dropout, Dense, Flatten, Activation\n",
    "    from keras import layers, models, optimizers, regularizers\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, LearningRateScheduler, ReduceLROnPlateau\n",
    "    \n",
    "    from keras.layers import LSTM, Dense, Dropout, Embedding, Masking\n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    \n",
    "    input_size = X_train_seq.shape[1] # same as padding_maxlen also\n",
    "    \n",
    "    if num_classes == 1: #binary it should be 2\n",
    "        #loss ='sparse_categorical_crossentropy'\n",
    "        loss = 'binary_crossentropy'\n",
    "        output_activation = 'sigmoid'\n",
    "    else:\n",
    "        loss = 'categorical_crossentropy'\n",
    "        output_activation = 'softmax'\n",
    "        \n",
    "        y_train = to_categorical(y_train, num_classes) #multi classification\n",
    "        y_valid = to_categorical(y_valid, num_classes)\n",
    "\n",
    "    # L2 regularization\n",
    "    l2 = None if l2_loss_lambda is None else regularizers.l2(l2_loss_lambda)\n",
    "        \n",
    "    if model_type == 'lrng_embd_LSTM':\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, vector_size, input_length=input_size, trainable=True))\n",
    "        model.add(LSTM(128, return_sequences=True, dropout=0.2))\n",
    "        model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(num_classes, activation=output_activation, kernel_regularizer=l2))\n",
    "    elif model_type == 'lrng_embd_NN': #Shallow_NN \n",
    "        model = Sequential()\n",
    "        model.add(Dense(50, input_dim=X_train_seq.shape[1], kernel_initializer=\"uniform\", activation=\"relu\"))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(num_classes, activation=output_activation, kernel_regularizer=l2))\n",
    "    elif model_type == 'lrng_embd_ShlowNN': # same as lrng_embd_NN\n",
    "        input_layer = layers.Input((input_size, ), sparse=True)\n",
    "        hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "        output_layer = layers.Dense(num_classes, activation=output_activation)(hidden_layer)\n",
    "        model = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "        \n",
    "    #trained models\n",
    "    # embedding=Embedding(vocab_size, vector_size, embeddings_initializer=Constant(trained_weight), input_length=MAX_LEN, trainable=False)\n",
    "    elif model_type == 'trnd_embd_LSTM1':\n",
    "        model = Sequential()\n",
    "        #model.add(Embedding(input_dim = num_words, output_dim = EMBEDDING_DIM, input_length= X_train_pad.shape[1], weights = [gensim_weight_matrix], trainable = False))\n",
    "        model.add(Embedding(vocab_size, vector_size, input_length=input_size, trainable=False,  weights=[trained_weight]))\n",
    "        #model.add(SpatialDropout1D(0.2)) #TBC\n",
    "        model.add(LSTM(128, return_sequences=True, dropout=0.2))\n",
    "        model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(num_classes, activation=output_activation, kernel_regularizer=l2))\n",
    "    elif model_type == 'trnd_embd_CNN1D':\n",
    "        input_layer = layers.Input((input_size, ))\n",
    "        embedding_layer = layers.Embedding(vocab_size, vector_size, trainable=False, weights=[trained_weight])(input_layer)\n",
    "        embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "        conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer) # Add the convolutional Layer\n",
    "        pooling_layer = tf.keras.layers.GlobalMaxPooling1D()(conv_layer) # Add the pooling Layer\n",
    "        output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer) # Add the output Layers\n",
    "        output_layer1 = layers.Dropout(0.25)(output_layer1) # Add the output Layers\n",
    "        output_layer2 = layers.Dense(num_classes, activation=output_activation)(output_layer1) #final output Layer\n",
    "        model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    elif model_type == 'trnd_embd_LSTM2':\n",
    "        input_layer = layers.Input((input_size, ))\n",
    "        embedding_layer = layers.Embedding(vocab_size, vector_size, trainable=False, weights=[trained_weight])(input_layer)\n",
    "        embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "        lstm_layer = layers.LSTM(100)(embedding_layer)\n",
    "        output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "        output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "        output_layer2 = layers.Dense(num_classes, activation=\"sigmoid\")(output_layer1)\n",
    "        model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    elif model_type == 'trnd_embd_GRU1':\n",
    "        input_layer = layers.Input((input_size, ))\n",
    "        embedding_layer = layers.Embedding(vocab_size, vector_size, trainable=False, weights=[trained_weight])(input_layer)\n",
    "        embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "        gru_layer = layers.GRU(100)(embedding_layer) # GRU layer\n",
    "        output_layer1 = layers.Dense(50, activation=\"relu\")(gru_layer)\n",
    "        output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "        output_layer2 = layers.Dense(num_classes, activation=\"sigmoid\")(output_layer1)\n",
    "        model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    elif model_type == 'trnd_embd_BidirGRU1':\n",
    "        input_layer = layers.Input((input_size, ))\n",
    "        embedding_layer = layers.Embedding(vocab_size, vector_size, trainable=False, weights=[trained_weight])(input_layer)\n",
    "        embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "        lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
    "        output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "        output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "        output_layer2 = layers.Dense(num_classes, activation=\"sigmoid\")(output_layer1)\n",
    "        model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    elif model_type == 'trnd_embd_BidirRNN1':\n",
    "        input_layer = layers.Input((input_size, ))\n",
    "        embedding_layer = layers.Embedding(vocab_size, vector_size, trainable=False, weights=[trained_weight])(input_layer)\n",
    "        embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "        rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
    "        conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "        pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "        output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "        output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "        output_layer2 = layers.Dense(num_classes, activation=\"sigmoid\")(output_layer1)\n",
    "        model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    elif model_type == 'trnd_embd_BidirCuDNNLSTM':\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, vector_size, input_length=input_size, trainable=False,  weights=[trained_weight]))\n",
    "        #model.add(Embedding(input_dim = num_words, output_dim = EMBEDDING_DIM, input_length= X_train_pad.shape[1], weights = [gensim_weight_matrix], trainable = False))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(layers.Bidirectional(layers.CuDNNLSTM(100,return_sequences=True)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(layers.Bidirectional(layers.CuDNNLSTM(200,return_sequences=True)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(layers.Bidirectional(layers.CuDNNLSTM(100,return_sequences=False)))\n",
    "        model.add(Dense(num_classes, activation=output_activation, kernel_regularizer=l2))\n",
    "        #model.add(Dense(class_num, activation = 'softmax'))\n",
    "\n",
    "    # compile with adam optimizer & categorical_crossentropy loss function\n",
    "    #model.add(Dense(num_classes, activation=output_activation, kernel_regularizer=l2))\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[score_metrics])\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)  \n",
    "    mc=ModelCheckpoint(filepath = model_save_pathname, monitor='val_acc', mode='max', save_best_only=True, verbose=1)  # model_path to be updated\n",
    "    model_history = model.fit(np.array(X_train_seq), np.array(y_train), batch_size=batch_size, epochs=n_epochs, \n",
    "                        validation_data=(np.array(X_valid_seq), np.array(y_valid)), verbose=1, callbacks=[es, mc])\n",
    "\n",
    "    history_df = pd.DataFrame(model_history.history)\n",
    "\n",
    "    # evaluate the model\n",
    "    train_loss, train_acc = model.evaluate(np.array(X_train_seq), np.array(y_train), verbose=0)\n",
    "    valid_loss, valid_acc = model.evaluate(np.array(X_valid_seq), np.array(y_valid), verbose=0)\n",
    "    model_eval_dict = model.evaluate(np.array(X_valid_seq), np.array(y_valid), batch_size=batch_size, return_dict=True)\n",
    "\n",
    "    #save the model\n",
    "    model.save(model_save_pathname)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    test_pred = model.predict(X_valid_seq)\n",
    "    #size = X_valid_seq[0] # chk y_valid\n",
    "    #test_pred=np.round(test_pred).astype(int).reshape(size) # reshape to be chk\n",
    "    #sub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':test_pred})\n",
    "    #sub.to_csv('submission.csv',index=False)\n",
    "    \n",
    "    #predict / inverse_predict\n",
    "    #test_pred = model.predict(test_X)\n",
    "    #test_pred = scaler_y.inverse_transform(test_pred)\n",
    "\n",
    "#         if num_classes == 1:\n",
    "#             pred = [int(round(p[0])) for p in pred] # binary classification TBC\n",
    "#         else:\n",
    "#             pred = pred.argmax(axis=-1) # multi classification\n",
    "\n",
    "    #return model, history_df, train_loss, train_acc, valid_loss, valid_acc\n",
    "    return model_eval_dict, test_pred, model, history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dd897b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_index(dataframe):\n",
    "    dataframe = dataframe.reset_index(inplace = False)\n",
    "    return dataframe\n",
    "\n",
    "def category_encode(dataframe):\n",
    "    global category_cols\n",
    "    global target_col\n",
    "    X = dataframe[category_cols]\n",
    "    y = dataframe[target_col]\n",
    "    ce_ord = ce.OrdinalEncoder(cols=category_cols)\n",
    "    dataframe[category_cols] = ce_ord.fit_transform(X, y)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e8e52e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#common functions\n",
    "\n",
    "#TBR from common_functions\n",
    "# def chk_create_update_df_dict(data=None, file_path='D:\\\\XYZ\\\\', file_name='report_df_output',\n",
    "#                               dict_key=None, dict_value=None, save_df=None):\n",
    "#     import os, sys\n",
    "#     import pickle\n",
    "#     from datetime import datetime\n",
    "#     import pandas as pd\n",
    "#     '''\n",
    "#     1.2 : added feat_list/feat_dict\n",
    "#     1.1 : added df_dict\n",
    "#     ver 1.0\n",
    "#     '''\n",
    "\n",
    "#     # 1 create /upload report_df file if exist or create\n",
    "#     file_path_name = file_path + file_name\n",
    "\n",
    "#     if os.path.isfile(file_path_name+'_pkl'):\n",
    "#         print('1 update report_df/report_dict exist, open for use')\n",
    "#         infile = open(file_path_name+'_pkl', 'rb')\n",
    "#         report_df = pickle.load(infile)\n",
    "#         report_dict = pickle.load(infile)\n",
    "#         df_dict = pickle.load(infile) # added 1.1\n",
    "#     else:\n",
    "#         print('1 create new report_df/report_dict')\n",
    "#         report_df = pd.DataFrame()\n",
    "#         report_dict = {}\n",
    "#         df_dict = {} # added 1.1\n",
    "\n",
    "#     # 2 update the data, convert return dict into series before appending in df, index will be date\n",
    "#     report_df = report_df.append(pd.Series(data, name=str(datetime.now())))\n",
    "#     report_dict[dict_key] = dict_value\n",
    "#     df_dict[dict_key] = save_df # added 1.1\n",
    "\n",
    "#     # 3 save as excel file for ref and analysis\n",
    "#     report_df.to_excel(file_path_name + '.xlsx', index_label='index')\n",
    "\n",
    "#     # 4 save report_df as pickle for future update\n",
    "#     outfile = open(file_path_name+'_pkl', 'wb')\n",
    "#     pickle.dump(report_df, outfile)\n",
    "#     pickle.dump(report_dict, outfile)\n",
    "#     pickle.dump(df_dict, outfile) # added 1.1\n",
    "#     outfile.close()\n",
    "#     return report_df\n",
    "\n",
    "# tt_cv_pt TBR by latest function tt_cv_pt_class from model_function\n",
    "def tt_cv_pt(X, y, X_train, y_train, X_valid, y_valid, X_test, y_test, model, param_grid, cv, train_type='tt',\n",
    "             scoring=('accuracy', 'precision', 'recall', 'f1', 'roc_auc')):\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "    from sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, roc_curve, precision_recall_curve, auc\n",
    "    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "    from sklearn.metrics import brier_score_loss, log_loss\n",
    "    from numpy import argmax, sqrt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    '''\n",
    "    1.2 TBA random search and HyperOpt\n",
    "    1.1 updated train_type tt/cv/pt -->'TrainTest', 'CrossValid', 'GridSearch'\n",
    "    1.0 initial ver\n",
    "    '''\n",
    "    # 1 split train/test\n",
    "    #X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # 2 call train_test / cross_val / grid_search\n",
    "    if train_type == 'TrainTest':\n",
    "        model.fit(X_train, y_train)\n",
    "        y_preds = model.predict(X_valid)\n",
    "\n",
    "        # cm = confusion_matrix(y_valid, y_preds)\n",
    "        # cr = classification_report(y_valid, y_preds)\n",
    "        train_score = model.score(X_train, y_train)\n",
    "        test_score = model.score(X_valid, y_valid)\n",
    "        train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
    "        test_accuracy = accuracy_score(y_valid, model.predict(X_valid))\n",
    "        train_precision = precision_score(y_train, model.predict(X_train))\n",
    "        test_precision = precision_score(y_valid, model.predict(X_valid))\n",
    "        train_recall = recall_score(y_train, model.predict(X_train))\n",
    "        test_recall = recall_score(y_valid, model.predict(X_valid))\n",
    "        train_f1 = f1_score(y_train, model.predict(X_train))\n",
    "        test_f1 = f1_score(y_valid, model.predict(X_valid))\n",
    "        y_actual = y_valid\n",
    "        trained_model = model #for prediction\n",
    "\n",
    "        return_dict = {'train_score': train_score, 'test_score': test_score,\n",
    "                       'train_accuracy': train_accuracy, 'test_accuracy': test_accuracy,\n",
    "                       'train_precision': train_precision, 'test_precision': test_precision,\n",
    "                       'train_recall': train_recall, 'test_recall': test_recall,\n",
    "                       'train_f1': train_f1, 'test_f1': test_f1}\n",
    "\n",
    "        # calculate y_probs\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_probs = model.predict_proba(X_valid)[:, 1]\n",
    "        else:  # use decision function\n",
    "            y_probs = model.decision_function(X_valid)\n",
    "\n",
    "    elif train_type == 'CrossValid':\n",
    "        # score = cross_val_score(pipe, X, y, cv=cv).mean()\n",
    "        return_dict = cross_validate(model, X, y, scoring=scoring, cv=cv, return_train_score=True)\n",
    "        y_preds = cross_val_predict(model, X, y, cv=cv)\n",
    "        y_probs = cross_val_predict(model, X, y, cv=cv, method='predict_proba')[:, 1]  # y_probs\n",
    "\n",
    "        return_dict = dict((k, v.mean()) for k, v in return_dict.items())  # avg of each array items\n",
    "        y_actual = y\n",
    "        trained_model = model #for prediction\n",
    "\n",
    "    elif train_type == 'GridSearch':\n",
    "        estimator = GridSearchCV(model, param_grid=param_grid, cv=cv)\n",
    "        # estimator.fit(X, y)\n",
    "        estimator.fit(X_train, y_train)\n",
    "        best_score = estimator.best_score_\n",
    "        best_params = estimator.best_params_\n",
    "        best_estimator = estimator.best_estimator_\n",
    "        cv_results = estimator.cv_results_\n",
    "        y_preds = best_estimator.predict(X_valid)\n",
    "        y_probs = best_estimator.predict_proba(X_valid)[:, 1]  # y_probs\n",
    "        gs_result_df = pd.DataFrame(cv_results)\n",
    "\n",
    "        train_accuracy = accuracy_score(y_train, best_estimator.predict(X_train))\n",
    "        test_accuracy = accuracy_score(y_valid, best_estimator.predict(X_valid))\n",
    "        train_recall = recall_score(y_train, best_estimator.predict(X_train))\n",
    "        test_recall = recall_score(y_valid, best_estimator.predict(X_valid))\n",
    "        train_precision = precision_score(y_train, best_estimator.predict(X_train))\n",
    "        test_precision = precision_score(y_valid, best_estimator.predict(X_valid))\n",
    "        train_f1 = f1_score(y_train, best_estimator.predict(X_train))\n",
    "        test_f1 = f1_score(y_valid, best_estimator.predict(X_valid))\n",
    "        y_actual = y_valid\n",
    "        trained_model = best_estimator #for prediction\n",
    "\n",
    "        return_dict = {'best_score': best_score, 'best_params': best_params,\n",
    "                       'train_accuracy': train_accuracy, 'test_accuracy': test_accuracy,\n",
    "                       'train_precision': train_precision, 'test_precision': test_precision,\n",
    "                       'train_recall': train_recall, 'test_recall': test_recall,\n",
    "                       'train_f1': train_f1, 'test_f1': test_f1}\n",
    "\n",
    "    # 3 calculate common metrics as per probs\n",
    "    loss_log = log_loss(y_actual, y_probs)\n",
    "    #brier_loss = brier_score_loss(y_actual, y_probs)\n",
    "    auc_roc_score = roc_auc_score(y_actual, y_probs)\n",
    "    model_precision, model_recall, thresold_pr = precision_recall_curve(y_actual, y_probs)\n",
    "    auc_prc_score = auc(model_recall, model_precision)\n",
    "\n",
    "    # 4 calculate best_threshold using roc curves\n",
    "    fpr, tpr, thresholds = roc_curve(y_actual, y_preds)\n",
    "    gmeans = sqrt(tpr * (1 - fpr))  # calculate the g-mean for each threshold\n",
    "    ix = argmax(gmeans)  # locate the index of the largest g-mean\n",
    "    best_threshold = thresholds[ix]\n",
    "    best_gmean = gmeans[ix]\n",
    "\n",
    "    common_metrics_dict = {'loss_log': loss_log, #'brier_loss': brier_loss,\n",
    "                           'auc_roc_score': auc_roc_score,\n",
    "                           'best_threshold': best_threshold, 'best_gmean': best_gmean, 'train_type': train_type,\n",
    "                           'auc_prc_score': auc_prc_score, 'trained_model': trained_model}\n",
    "    return_dict.update(common_metrics_dict)\n",
    "\n",
    "    # 5 calculate common metrics as per preds & confusion metrics\n",
    "    cm = confusion_matrix(y_actual, y_preds)\n",
    "    TN, FP, FN, TP = confusion_matrix(y_actual, y_preds).ravel()\n",
    "    cr = classification_report(y_actual, y_preds)\n",
    "\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP / (TP + FN) if (TP + FN) else 0\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN / (TN + FP) if (TN + FP) else 0\n",
    "    # Precision or positive predictive value\n",
    "    PPV = TP / (TP + FP) if (TP + FP) else 0\n",
    "    # Negative predictive value\n",
    "    NPV = TN / (TN + FN) if (TN + FN) else 0\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP / (FP + TN) if (FP + TN) else 0\n",
    "    # False negative rate\n",
    "    FNR = FN / (FN + TP) if (FN + TP) else 0\n",
    "    # False discovery rate\n",
    "    FDR = FP / (TP + FP) if (TP + FP) else 0\n",
    "    # Overall accuracy\n",
    "    ACC = (TN + TP) / (TP + FP + TN + FN) if (TP + FP + TN + FN) else 0\n",
    "\n",
    "    confusion_metrics_dict = {'cm': cm, 'cr': cr, 'TP': TP, 'FP': FP, 'TN': TN, 'FN': FN,\n",
    "                              'PPV': PPV, 'NPV': NPV, 'FDR': FDR, 'TPR': TPR, 'FPR': FPR, 'TNR': TNR, 'FNR': FNR,\n",
    "                              'ACC': ACC}\n",
    "\n",
    "    return_dict.update(confusion_metrics_dict)\n",
    "    return return_dict, y_preds, y_probs, y_actual, trained_model\n",
    "\n",
    "\n",
    "def classification_error_score_metrics(y_actual, y_preds, y_probs, model_name='LSTM', test_type='XYZ',\n",
    "                                       n_features=1, plot=False, plot_path=None):\n",
    "    '''\n",
    "    ver 1.0 intial ver\n",
    "    ver 1.1 : added mape_mean\n",
    "    '''\n",
    "\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, roc_curve, precision_recall_curve, auc\n",
    "    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "    from sklearn.metrics import brier_score_loss, log_loss\n",
    "    from numpy import argmax, sqrt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # 1 calculate common metrics as per preds\n",
    "    if y_preds is not None:\n",
    "        acc = accuracy_score(y_actual, y_preds)\n",
    "        precision = precision_score(y_actual, y_preds)\n",
    "        recall = recall_score(y_actual, y_preds)\n",
    "        f1 = f1_score(y_actual, y_preds)\n",
    "        \n",
    "        # 1.1 calculate best_threshold using roc curves\n",
    "        fpr, tpr, thresholds = roc_curve(y_actual, y_preds)\n",
    "        gmeans = sqrt(tpr * (1 - fpr))  # calculate the g-mean for each threshold\n",
    "        ix = argmax(gmeans)  # locate the index of the largest g-mean\n",
    "        best_threshold = thresholds[ix]\n",
    "        best_gmean = gmeans[ix]\n",
    "        \n",
    "        preds_metrics_dict = {'acc': acc, 'precision': precision, 'recall': recall, 'f1': f1,\n",
    "                   'best_threshold': best_threshold, 'best_gmean': best_gmean, 'auc_prc_score': auc_prc_score,  \n",
    "                   'test_type': test_type, 'model_name': model_name}\n",
    "        \n",
    "        # 1.2 calculate common metrics as per preds & confusion metrics\n",
    "        cm = confusion_matrix(y_actual, y_preds)\n",
    "        TN, FP, FN, TP = confusion_matrix(y_actual, y_preds).ravel()\n",
    "        cr = classification_report(y_actual, y_preds)\n",
    "        \n",
    "        #1.3 Sensitivity, hit rate, recall, or true positive rate\n",
    "        TPR = TP / (TP + FN) if (TP + FN) else 0\n",
    "        # Specificity or true negative rate\n",
    "        TNR = TN / (TN + FP) if (TN + FP) else 0\n",
    "        # Precision or positive predictive value\n",
    "        PPV = TP / (TP + FP) if (TP + FP) else 0\n",
    "        # Negative predictive value\n",
    "        NPV = TN / (TN + FN) if (TN + FN) else 0\n",
    "        # Fall out or false positive rate\n",
    "        FPR = FP / (FP + TN) if (FP + TN) else 0\n",
    "        # False negative rate\n",
    "        FNR = FN / (FN + TP) if (FN + TP) else 0\n",
    "        # False discovery rate\n",
    "        FDR = FP / (TP + FP) if (TP + FP) else 0\n",
    "        # Overall accuracy\n",
    "        ACC = (TN + TP) / (TP + FP + TN + FN) if (TP + FP + TN + FN) else 0\n",
    "\n",
    "        confusion_metrics_dict = {'cm': cm, 'cr': cr, 'TP': TP, 'FP': FP, 'TN': TN, 'FN': FN,\n",
    "                              'PPV': PPV, 'NPV': NPV, 'FDR': FDR, 'TPR': TPR, 'FPR': FPR, 'TNR': TNR, 'FNR': FNR,\n",
    "                              'ACC': ACC}\n",
    "        preds_metrics_dict.update(confusion_metrics_dict)\n",
    "        \n",
    "    # 2 calculate common metrics as per probs\n",
    "    if y_probs is not None:\n",
    "        loss_log = log_loss(y_actual, y_probs)\n",
    "        #brier_loss = brier_score_loss(y_actual, y_probs)\n",
    "        auc_roc_score = roc_auc_score(y_actual, y_probs)\n",
    "        model_precision, model_recall, thresold_pr = precision_recall_curve(y_actual, y_probs)\n",
    "        auc_prc_score = auc(model_recall, model_precision)\n",
    "        \n",
    "        probs_metrics_dict = {'loss_log': loss_log, #'brier_loss': brier_loss,\n",
    "                           'auc_roc_score': auc_roc_score, 'auc_prc_score': auc_prc_score}\n",
    "        \n",
    "        preds_metrics_dict.update(probs_metrics_dict)\n",
    "    return preds_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49bdc408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_textacy_entity_extraction_knowlege_graph(text_data, ent_type = \"DATE\", ent_plot_filter = None, plot_save_path=None):\n",
    "\n",
    "    '''\n",
    "    1.0 May03,2023 initial version\n",
    "\n",
    "    plot_save_path: if not none then it will create Network Graph\n",
    "    ent_type = NUMERIC, PERSON, ORG, LOC, NUMERIC, DATE\n",
    "    '''\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    import spacy \n",
    "    import textacy  #0.12.0\n",
    "    import networkx as nx  #3.0 (also pygraphviz==1.10)\n",
    "    import dateparser #1.1.7\n",
    "    \n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text_data)\n",
    "\n",
    "    #3 from text to a list of sentences\n",
    "    lst_docs = [sent for sent in doc.sents]\n",
    "\n",
    "    dic_sub_vrb_obj = {\"id\":[], \"text\":[], \"sub\":[], \"relation\":[], \"object\":[]}\n",
    "    dic_ent = {\"id\":[], \"text\":[], ent_type:[]}\n",
    "    dic_all_ent = {\"id\":[], \"text\":[], 'all_ent':[]}\n",
    "    dic_all_noun = {\"id\":[], \"text\":[], 'noun':[]}\n",
    "\n",
    "    for n, sentence in enumerate(lst_docs):\n",
    "        #print(f'\\n sentence no:{n}, sentence:{sentence}')\n",
    "        lst_sub_vrb_obj = list(textacy.extract.subject_verb_object_triples(sentence))\n",
    "        lst_ent = list(textacy.extract.entities(sentence, include_types={ent_type}))\n",
    "        lst_all_ent = list(textacy.extract.entities(sentence))\n",
    "        lst_all_noun = list(textacy.extract.noun_chunks(sentence))\n",
    "        list_all_tokens = list(textacy.extract.words(sentence))\n",
    "\n",
    "        #1 lst_ent\n",
    "        if len(lst_ent) > 0:\n",
    "            for attr in lst_ent:\n",
    "                dic_ent[\"id\"].append(n)\n",
    "                dic_ent[\"text\"].append(sentence.text)\n",
    "                dic_ent[ent_type].append(str(attr))\n",
    "        else:\n",
    "            dic_ent[\"id\"].append(n)\n",
    "            dic_ent[\"text\"].append(sentence.text)\n",
    "            dic_ent[ent_type].append(np.nan)\n",
    "\n",
    "        dtf_ent = pd.DataFrame(dic_ent)\n",
    "\n",
    "        #2 lst_all_ent\n",
    "        if len(lst_all_ent) > 0:\n",
    "            for attr1 in lst_all_ent:\n",
    "                dic_all_ent[\"id\"].append(n)\n",
    "                dic_all_ent[\"text\"].append(sentence.text)\n",
    "                dic_all_ent['all_ent'].append(str(attr1))\n",
    "        else:\n",
    "            dic_all_ent[\"id\"].append(n)\n",
    "            dic_all_ent[\"text\"].append(sentence.text)\n",
    "            dic_all_ent['all_ent'].append(np.nan)\n",
    "\n",
    "        dtf_all_ent = pd.DataFrame(dic_all_ent)\n",
    "        dtf_all_ent = dtf_all_ent.groupby('id').agg(lambda x: list(x)).reset_index()\n",
    "\n",
    "        #3 lst_sub_vrb_obj\n",
    "        if len(lst_sub_vrb_obj) > 0:\n",
    "            for m, sent in enumerate(lst_sub_vrb_obj): #one sentence can have more than 1 one sub/verb/obj\n",
    "                subj = \"_\".join(map(str, sent.subject))\n",
    "                obj  = \"_\".join(map(str, sent.object))\n",
    "                relation = \"_\".join(map(str, sent.verb))\n",
    "                dic_sub_vrb_obj[\"id\"].append(n)\n",
    "                dic_sub_vrb_obj[\"text\"].append(sentence.text)\n",
    "                dic_sub_vrb_obj[\"sub\"].append(subj)\n",
    "                dic_sub_vrb_obj[\"object\"].append(obj)\n",
    "                dic_sub_vrb_obj[\"relation\"].append(relation)\n",
    "        else:\n",
    "            dic_sub_vrb_obj[\"id\"].append(n)\n",
    "            dic_sub_vrb_obj[\"text\"].append(sentence.text)\n",
    "            dic_sub_vrb_obj[\"sub\"].append(np.nan)\n",
    "            dic_sub_vrb_obj[\"object\"].append(np.nan)\n",
    "            dic_sub_vrb_obj[\"relation\"].append(np.nan)\n",
    "\n",
    "        dtf_sub_vrb_obj = pd.DataFrame(dic_sub_vrb_obj)\n",
    "\n",
    "        #4 lst_all_noun\n",
    "        if len(lst_all_noun) > 0:\n",
    "            for attr1 in lst_all_noun:\n",
    "                dic_all_noun[\"id\"].append(n)\n",
    "                dic_all_noun[\"text\"].append(sentence.text)\n",
    "                dic_all_noun['noun'].append(str(attr1))\n",
    "        else:\n",
    "            dic_all_noun[\"id\"].append(n)\n",
    "            dic_all_noun[\"text\"].append(sentence.text)\n",
    "            dic_all_noun['all_ent'].append(np.nan)\n",
    "\n",
    "        dtf_all_noun = pd.DataFrame(dic_all_noun)\n",
    "        dtf_all_noun = dtf_all_noun.groupby('id').agg(lambda x: list(x)).reset_index()\n",
    "\n",
    "    # merge the all files\n",
    "    dtf_final = dtf_sub_vrb_obj.merge(dtf_ent[[ent_type, 'id']], how=\"left\", on=\"id\")\n",
    "    dtf_final = dtf_final.merge(dtf_all_ent[['all_ent', 'id']], how=\"left\", on=\"id\")\n",
    "    dtf_final = dtf_final.merge(dtf_all_noun[['noun', 'id']], how=\"left\", on=\"id\")\n",
    "\n",
    "    if plot_save_path:\n",
    "        if ent_plot_filter:\n",
    "            dtf_final = dtf_final[(dtf_final[\"sub\"]==ent_plot_filter) | (dtf_final[\"object\"]==ent_plot_filter)]\n",
    "\n",
    "        ## create full graph\n",
    "        G = nx.from_pandas_edgelist(dtf_final, source=\"sub\", target=\"object\", \n",
    "                                    edge_attr=\"relation\", create_using=nx.DiGraph())\n",
    "        ## plot\n",
    "        plt.figure(figsize=(15,10))\n",
    "\n",
    "        pos = nx.spring_layout(G, k=1)\n",
    "        node_color = \"skyblue\"\n",
    "        edge_color = \"black\"\n",
    "\n",
    "        nx.draw(G, pos=pos, with_labels=True, node_color=node_color, \n",
    "                edge_color=edge_color, cmap=plt.cm.Dark2, \n",
    "                node_size=2000, connectionstyle='arc3,rad=0.1')\n",
    "\n",
    "        nx.draw_networkx_edge_labels(G, pos=pos, label_pos=0.5, \n",
    "                                 edge_labels=nx.get_edge_attributes(G,'relation'),\n",
    "                                 font_size=12, font_color='black', alpha=0.6)\n",
    "        plt.show()\n",
    "        plt.savefig(plot_save_path+'EntityKnowledgeGraph.png')\n",
    "\n",
    "    return dtf_final\n",
    "\n",
    "# import pandas as pd\n",
    "# data_path = 'D:\\\\projects\\\\CAI\\\\'\n",
    "# plot_save_path = 'D:\\\\projects\\\\CAI\\\\'\n",
    "\n",
    "# file_name = 'User_Journey.xlsx'\n",
    "# df = pd.read_excel(data_path+file_name, sheet_name='Chat_dummy')\n",
    "# df['chat'] = df['chat'].astype('object')\n",
    "# text_data = ','.join([str(i) for i in df['chat'].values.tolist()])\n",
    "\n",
    "# df_entity = spacy_textacy_entity_extraction_knowlege_graph(text_data, ent_type = \"DATE\", ent_plot_filter = None, plot_save_path=plot_save_path)\n",
    "# df_entity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a577705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2022/09/real-or-not-disaster-tweets-classification-with-roberta/\n",
    "\n",
    "#1. De-abbreviation\n",
    "def deabbreviate(text):\n",
    "    import re\n",
    "    text = text.upper()\n",
    "    text = re.sub(r'bAFAIKb', ' As Far As I Know ',text)\n",
    "    text = re.sub(r'bAFKb', ' From Keyboard ',text)\n",
    "    text = re.sub(r'bASAPb', ' As Soon As Possible ',text)\n",
    "    text = re.sub(r'bATKb', ' At The Keyboard ',text)\n",
    "    text = re.sub(r'bA3b', ' Anytime, Anywhere, Anyplace ',text)\n",
    "    text = re.sub(r'bBAKb', ' Back At Keyboard ',text)\n",
    "    text = re.sub(r'bBBLb', ' Be Back Later ',text)\n",
    "    text = re.sub(r'bBBSb', ' Be Back Soon ',text)\n",
    "    text = re.sub(r'bBFNb', ' Bye For Now ',text)\n",
    "    text = re.sub(r'bBRBb', ' Be Right Back ',text)\n",
    "    text = re.sub(r'bBRTb', ' Be Right There ',text)\n",
    "    text = re.sub(r'bBTWb', ' By The Way ',text)\n",
    "    text = re.sub(r'bB4b', ' Before ',text)\n",
    "    text = re.sub(r'bB4Nb', ' Bye For Now ',text)\n",
    "    text = re.sub(r'bCUb', ' See You ',text)\n",
    "    text = re.sub(r'bCUL8Rb', ' See You Later ',text)\n",
    "    text = re.sub(r'bCYAb', ' See You ',text)\n",
    "    text = re.sub(r'bFAQb', ' Frequently Asked Questions ',text)\n",
    "    text = re.sub(r'bFYIb', ' For Your Information ',text)\n",
    "    text = re.sub(r'bGNb', ' Good Night ',text)\n",
    "    text = re.sub(r'bGR8b', ' Great ',text)\n",
    "    text = re.sub(r'bICb', ' I See ',text)\n",
    "    text = re.sub(r'bLOLb', ' Laughing Out Loud ',text)\n",
    "    text = re.sub(r'bL8Rb', ' Later ',text)\n",
    "    text = re.sub(r'bM8b', ' Mate ',text)\n",
    "    text = re.sub(r'bTHXb', ' Thank You ',text)\n",
    "    text = re.sub(r'bTTFNb', ' BYE ',text)\n",
    "    text = re.sub(r'bTTFNb', ' BYE ',text)\n",
    "    text = re.sub(r'bUb', ' You ',text)\n",
    "    text = re.sub(r'bU2b', ' You TOO ',text)\n",
    "    text = re.sub(r'bWTFb', ' What The Heck ',text)\n",
    "    text = re.sub(r'bW8b', ' Wait ',text)\n",
    "    text = re.sub(r'bFAVb', ' Favourite ',text)\n",
    "    text = re.sub(r'bHWYb',\" highway \",text)\n",
    "    text = re.sub(r'bPPLb',\" people \",text)\n",
    "    text = re.sub(r'bGVb',\" give \",text)\n",
    "    text = re.sub(r'bWANNAb',\" want to \",text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "#2. Filter Ascii\n",
    "def return_ascii(text):\n",
    "    ret_str = \"\"\n",
    "    for char in list(text):\n",
    "        if char.isascii():\n",
    "            ret_str += char\n",
    "    return ret_str\n",
    "\n",
    "#3. General Preprocessing\n",
    "punctuations = '''!\"#$%&()'*+,-./:;?@[]^_`{|}~'''\n",
    "\n",
    "def preprocess(text):\n",
    "    import re\n",
    "    #1.Removing AM and PM\n",
    "    text = re.sub(r'b[AP]{1}Mb',\" \",text)\n",
    "\n",
    "    #2.Lowercasing \n",
    "    text = text.lower()\n",
    "\n",
    "    #3.removing mentions\n",
    "    text =re.sub(r'@[^ ]+',' ',text)\n",
    "\n",
    "    #4.removing urls\n",
    "    text = re.sub(r'https*://t.co/w+',' ',text)\n",
    "    text = re.sub(r'https*://[^ ]+',' ',text)\n",
    "\n",
    "    #5.De-abbreviating\n",
    "    text = deabbreviate(text)\n",
    "\n",
    "    #6.cleaning mscl\n",
    "    text = re.sub(r\"\",\" \",text)\n",
    "    text = re.sub(r'&[^ ]+',\" \",text)\n",
    "    text = re.sub(r'b[a-z]+-[0-9]+b',\" \",text)\n",
    "    text = re.sub(r'bd+[a-z]+d+b',\" \",text)\n",
    "    text = re.sub(r'brtb',\" \",text)\n",
    "    text = re.sub(r'bfavb',\" \",text)\n",
    "    text = re.sub(r'b[h]{1}a[ha]+b',\"haa\",text)\n",
    "    text = re.sub(r'b[a-z]+d+[a-z]+b',\" \",text)\n",
    "    text = re.sub(r'<[^]+>',' ',text)\n",
    "\n",
    "    #7.removing months\n",
    "    text = re.sub(r\" jan | feb | mar | apr | may | jun | jul | aug | sep | oct | nov | dec \",' ',text)\n",
    "\n",
    "    #8.decontracting\n",
    "    text = re.sub(r\"aren't\",'are not',text)\n",
    "    text = re.sub(r\"won't\",' will not ',text)\n",
    "    text = re.sub(r\"bi'mb\",' I am ',text)\n",
    "    text = re.sub(r\"bi'db\",' I would ',text)\n",
    "    text = re.sub(r\"bit'sb\",' it is ',text)\n",
    "    text = re.sub(r\"bthat'sb\",' that is ',text)\n",
    "    text = re.sub(r\"bcan'tb\",' can not ',text)\n",
    "    text = re.sub(r\"bi'veb\",' I have ',text)\n",
    "    text = re.sub(r\"bthere'sb\",' there is ',text)\n",
    "    text = re.sub(r\"bdidn'tb\",' did not ',text)\n",
    "    text = re.sub(r\"bcouldn'tb\",' could not ',text)\n",
    "    text = re.sub(r\"bisn'tb\",' is not ',text)\n",
    "    text = re.sub(r\"bwe'reb\", ' we are ',text)\n",
    "    text = re.sub(r\"bthey'reb\",' they are ',text)\n",
    "    text = re.sub(r\"bdon'tb\",' do not ',text)\n",
    "    text = re.sub(r\"blet'sb\",' let us ',text)\n",
    "    text = re.sub(r\"bli'lb\",' little ',text)\n",
    "    text = re.sub(r\"bshe'sb\",' she is ',text)\n",
    "    text = re.sub(r\"bhe'sb\",' he is ',text)\n",
    "    text = re.sub(r\"bhow'reb\",' How are ',text)\n",
    "    text = re.sub(r\"wasn't\",' was not ',text)\n",
    "    text = re.sub(r\"bwhat'sb\",' what is ',text)\n",
    "    text = re.sub(r\"bhe'llb\",' he will ',text)\n",
    "    text = re.sub(r\"bi'llb\",' i will ',text)\n",
    "    text = re.sub(r\"bshe'llb\",' she will ',text)\n",
    "    text = re.sub(r\"byou'llb\",' you will ',text)\n",
    "    text = re.sub(r\"byou'reb\",' you are ',text)\n",
    "    text = re.sub(r\"bwe'veb\",' we have ',text)\n",
    "    text = re.sub(r\"byou'veb\",' you have ',text)\n",
    "    text = re.sub(r\"bthey'veb\",' they have ',text)\n",
    "\n",
    "    #9.removing expression\n",
    "    text =re.sub(r'A[^ ]+:',' ',text)\n",
    "    text =re.sub(r'A[^ ]+ [^ ]+:',' ',text)\n",
    "    text =re.sub(r'A[^ ]+ [^ ]+ [^ ]+:',' ',text)\n",
    "\n",
    "    #10.Removing the punctuations\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p,\"   \")\n",
    "\n",
    "    #11.removing certain patters\n",
    "    text = re.sub(r'lo+l',\"laughing out loud\",text)\n",
    "    text = re.sub(r'coo+l',\"cool\",text)\n",
    "    text = re.sub(r'go+a+l+','goal',text)\n",
    "    text = re.sub(r'so+',\"so\",text)\n",
    "    text = re.sub(r'bo+h+o*b','oh',text)\n",
    "\n",
    "    #12.Removing the digits\n",
    "    text = re.sub(r'd+',\" \",text)\n",
    "\n",
    "    #13.New line as space\n",
    "    text = re.sub(r'n',\" \",text)\n",
    "\n",
    "    #14.Removing extra spaces\n",
    "    text = re.sub(r'[ ]+',\" \",text)\n",
    "\n",
    "    #15.Stripping the end parts\n",
    "    text = text.strip()\n",
    "\n",
    "    #16.removing the accents\n",
    "    text = return_ascii(text)\n",
    "\n",
    "    #17.removing certain patterns after getting only the ascii characters\n",
    "    text = re.sub(r\"bcantb\",' can not ',text)\n",
    "    text = re.sub(r\"bwontb\",' will not ',text)\n",
    "    text = re.sub(r\"bimb\",' I am ',text)\n",
    "    text = re.sub(r\"bdidntb\",' did not ',text)\n",
    "    text = re.sub(r\"bcouldntb\",' could not ',text)\n",
    "    text = re.sub(r\"bisntb\",' is not ',text)\n",
    "    text = re.sub(r\"bdontb\",' do not ',text)\n",
    "    text = re.sub(r\"blilb\",' little ',text)\n",
    "    text = re.sub(r\"balilb\",' a little ',text)\n",
    "    text = re.sub(r\"view and download video\",' ',text)\n",
    "    text = re.sub(r\"bviaZ\",' ',text)\n",
    "\n",
    "    #18.Removing words with lengths less than 2\n",
    "    text = [ele for ele in text.split(\" \") if len(ele) > 1 ]\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8f5c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bba65e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2022/09/understanding-word-embeddings-and-building-your-first-rnn-model/\n",
    "# clean the text data using regex and data cleaning function\n",
    "def datacleaning(text):\n",
    "    whitespace = re.compile(r\"s+\")\n",
    "    user = re.compile(r\"(?i)@[a-z0-9_]+\")\n",
    "    text = whitespace.sub(' ', text)\n",
    "    text = user.sub('', text)\n",
    "    text = re.sub(r\"[[^()]*]\",\"\", text)\n",
    "    text = re.sub(\"d+\", \"\", text)\n",
    "    text = re.sub(r'[^ws]','',text)\n",
    "    text = re.sub(r\"(?:@S*|#S*|http(?=.*://)S*)\", \"\", text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # removing stop-words\n",
    "    text = [word for word in text.split() if word not in list(STOPWORDS)]\n",
    "    \n",
    "    # word lemmatization\n",
    "    sentence = []\n",
    "    for word in text:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        sentence.append(lemmatizer.lemmatize(word,'v'))\n",
    "        \n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96a4696e",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_path = 'D:\\\\NLP\\\\data\\\\Tweets2\\\\'\n",
    "df_train= pd.read_csv(data_path+'train.csv')\n",
    "df_test=pd.read_csv(data_path+'test.csv')\n",
    "\n",
    "#df_train1= extract_text_features(df=df_train, text_col='text', STOPWORDS_list=['a', 'an', 'the'])\n",
    "#df_train2 = text_cleaning(data_df=df_train1, text_field='text', STOPWORDS_list=['a', 'an', 'the'])\n",
    "\n",
    "df = pd.read_csv(data_path+'train.csv')\n",
    "#2. Removing any null values in the text column\n",
    "df.dropna(subset=['text'],inplace=True)\n",
    "\n",
    "#3. Removing all the duplicate text data and also data points with ambiguous labels\n",
    "#any exact data point which has been labeled as 1 as well as 0\n",
    "df_dup = df[df['text'].duplicated()]\n",
    "text_arr = np.unique(df_dup['text'].values)\n",
    "df_dup = df[df['text'].isin(text_arr)].sort_values(by=['text'])\n",
    "df_dup = df_dup[~df_dup.duplicated()].sort_values(by=['text'])\n",
    "df_dup_contradiction = df_dup[df_dup['text'].duplicated()]\n",
    "df_dup_contradiction.to_csv(\"ambiguous_datapoints_bert.csv\")\n",
    "df_dedup = df.drop_duplicates(subset=['text'])\n",
    "df_dedup = df_dedup[~df_dedup['text'].isin(df_dup_contradiction['text'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "434b24b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    words = regexp_tokenize(text, pattern='w+|$[d.]+|S+')\n",
    "    tokens = [w for w in words if w.lower() not in string.punctuation]\n",
    "    stopw = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopw]\n",
    "    tokens = [word for word in tokens if len(word)>=3]\n",
    "    lemma = WordNetLemmatizer()\n",
    "    tokens = [lemma.lemmatize(word) for word in tokens] \n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "#df['text']=df['text'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41e6012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/the-triune-pipeline-for-three-major-transformers-in-nlp-18c14e20530\n",
    "# custom linguistic features\n",
    "\n",
    "class SegmentFeaturizer:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_md\")\n",
    "        self.future_words = [\"tomorrow\", \"future\", \"futures\"]\n",
    "    \n",
    "    @staticmethod\n",
    "    def count_pronouns(doc):\n",
    "        segment = doc.text.lower().split()\n",
    "        counter = {\"1sg\": 0, \"1pl\": 0}\n",
    "        for pronoun in FIRST_SINGULAR:\n",
    "            counter[\"1sg\"] += segment.count(pronoun)\n",
    "        for pronoun in FIRST_PLURAL:\n",
    "            counter[\"1pl\"] += segment.count(pronoun)\n",
    "        return counter\n",
    "\n",
    "    def get_n_future_oriented_words(self, doc):\n",
    "        will_aux = [t for t in doc if t.tag_ == \"MD\" and t.lower_ in {\"will\", \"wo\", \"shall\", \"sha\"}]\n",
    "        going_to = [t for t in doc if t.dep_ == \"xcomp\" and t.head.lemma_ == \"go\"]\n",
    "        other_future_words = [t for t in doc if t.lower_ in self.future_words]\n",
    "        return len(will_aux) + len(going_to) + len(other_future_words)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_n_word_mentions(doc):\n",
    "        segment = doc.text\n",
    "        word_mentions_dict = {\n",
    "            \"n_trump_mentions\": segment.count(\"Trump\"),\n",
    "            \"n_other_candidate_mentions\": sum(\n",
    "                [segment.count(c.title()) for c in CANDIDATE_NAMES]\n",
    "            ),\n",
    "        }\n",
    "        return word_mentions_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def get_n_words_before_main_verb(doc):\n",
    "        numbers = [0]\n",
    "        for sent in doc.sents:\n",
    "            main = [t for t in sent if t.dep_ == \"ROOT\"][0]\n",
    "            if main.pos_ == \"VERB\":\n",
    "                dist_to_init = main.i - sent[0].i\n",
    "                numbers.append(dist_to_init)\n",
    "        return np.mean(numbers)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_n_complex_clauses(doc):\n",
    "        embedded_elements_count = []\n",
    "        for sent in doc.sents:\n",
    "            n_embedded = len(\n",
    "                [t for t in sent if t.dep_ in {\"ccomp\", \"xcomp\", \"advcl\", \"dative\"}]\n",
    "            )\n",
    "            embedded_elements_count.append(n_embedded)\n",
    "        return np.mean(embedded_elements_count)\n",
    "    \n",
    "    # putting it all together!\n",
    "    def featurize(self, segments):\n",
    "        feature_dicts = []\n",
    "        docs = self.nlp.pipe(segments)\n",
    "        for doc in docs:\n",
    "            feature_dict = {\n",
    "                \"n_trump_mentions\": self.get_n_word_mentions(doc)[\"n_trump_mentions\"],\n",
    "                \"n_future_oriented_words\": self.get_n_future_oriented_words(doc),\n",
    "                \"n_words_before_main_verb\": self.get_n_words_before_main_verb(doc),\n",
    "                \"n_complex_clauses\": self.get_n_complex_clauses(doc),\n",
    "                \"mean_sent_length\": self.get_sent_length_stats(doc)[\"mean_sent_length\"],\n",
    "            }\n",
    "            feature_dicts.append(feature_dict)\n",
    "        return feature_dicts"
   ]
  },
  {
   "cell_type": "raw",
   "id": "080b8189",
   "metadata": {},
   "source": [
    "#https://towardsdatascience.com/how-to-build-a-reusable-nlp-code-pipeline-with-scikit-learn-with-an-emphasis-on-feature-504f8aa14699\n",
    "import numpy as np\n",
    "import spacy\n",
    "from textstat import textstat\n",
    "from lexicalrichness import LexicalRichness\n",
    "\n",
    "#nlp = spacy.load(\"en_core_web_sm\")  # better as an attribute of the class\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "class BaseFeaturizer():\n",
    "    ...\n",
    "    @staticmethod\n",
    "    def get_n_words_before_main_verb(text):\n",
    "        doc = nlp(text)\n",
    "        main = [t for t in doc if t.dep_ == \"ROOT\"][0]\n",
    "        if main.pos_ == \"VERB\":\n",
    "            return main.i\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    @staticmethod\n",
    "    def get_readability_mean(text):\n",
    "        scores = []\n",
    "        scores.append(textstat.coleman_liau_index(text))\n",
    "        scores.append(textstat.flesch_kincaid_grade(text))\n",
    "        return np.mean(scores)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_lexical_density(text):\n",
    "    # measure of textual lexical diversity (e.g., McCarthy 2005)\n",
    "        if re.search(\"[a-zA-Z]\", text) is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return LexicalRichness(text).mtld(threshold=0.72)\n",
    "    \n",
    "class SyntacticComplexityFeaturizer(BaseFeaturizer):\n",
    "    def featurize(self, corpus):\n",
    "        feature_dicts = []\n",
    "        for text in corpus:\n",
    "            feature_dict = {\n",
    "                \"n_words_before_main_verb\": self.get_n_words_before_main_verb(text),\n",
    "                \"readability\": self.get_readability_mean(text),\n",
    "            }\n",
    "            feature_dicts.append(feature_dict)\n",
    "        return feature_dicts\n",
    "        \n",
    "        \n",
    "syntactic_featurizer = SyntacticComplexityFeaturizer\n",
    "#feature_dicts = syntactic_featurizer.featurize(corpus)\n",
    "\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "\n",
    "# class BaseTrainingPipeline(ABC):\n",
    "#     def __init__(self):\n",
    "#         self.X_train = None\n",
    "#         self.X_test = None\n",
    "#         self.y_train = None\n",
    "#         self.y_test = None\n",
    "#         self.y_preds = {} # algorithm_name: y_pred\n",
    "#         self.models = {}  # algorithm_name: trained model\n",
    "#         self.vectorizer = None\n",
    "        \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class BaseTrainingPipeline(ABC):\n",
    "    ...\n",
    "    def __init__(self):\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.y_preds = {} # algorithm_name: y_pred\n",
    "        self.models = {}  # algorithm_name: trained model\n",
    "        self.vectorizer = None\n",
    "        \n",
    "    @abstractmethod       \n",
    "    def train_model(self):\n",
    "        pass\n",
    "\n",
    "    def pipeline_main(\n",
    "        self,\n",
    "        corpus,\n",
    "        labels,\n",
    "        FeaturizerClass,\n",
    "        algorithms,\n",
    "        test_ratio=0.2,\n",
    "    ):\n",
    "        # featurize (get list of feature dicts)\n",
    "        featurizer = FeaturizerClass()\n",
    "        feature_dicts = featurizer.featurize(corpus)\n",
    "\n",
    "        # split features into train and test\n",
    "        self.features_train, self.features_test = split_train_test(\n",
    "            feature_dicts, test_ratio=test_ratio\n",
    "        )\n",
    "        \n",
    "        # vectorize the features\n",
    "        self.vectorizer = DictVectorizer()\n",
    "        self.X_train = self.vectorizer.fit_transform(self.features_train)\n",
    "        self.X_test = self.vectorizer.transform(self.features_test)\n",
    "        \n",
    "        # split labels into train and test\n",
    "        self.y_train, self.y_test = split_train_test(labels, test_ratio=test_ratio)\n",
    "        \n",
    "        # train (more on this below)\n",
    "        for algo in algorithms:\n",
    "            model = self.train_model(algo, self.X_train, self.y_train)\n",
    "            self.classifier = model\n",
    "            self.models.update({algo: model})\n",
    "            \n",
    "        \n",
    "class ClassificationTrainingPipeline(BaseTrainingPipeline):    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.initialized_models = {\n",
    "            \"linear_svm\": LinearSVC(),\n",
    "            \"logistic_regression\": LogisticRegression(),\n",
    "            \"naive_bayes\": GaussianNB(),\n",
    "            \"random_forest\": RandomForestClassifier(),\n",
    "\n",
    "#     def train_model(self, algorithm):\n",
    "#         clf = self.initialized_models[algorithm]\n",
    "#         if algorithm == \"logistic_regression\":\n",
    "#             param_grid = {\n",
    "#                 \"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "#                 \"max_iter\": [100, 1000, 10000],\n",
    "#             }\n",
    "#             grid_search = GridSearchCV(clf, param_grid)\n",
    "#             grid_search.fit(X_train, y_train)\n",
    "#             clf.set_params(**grid_search.best_params_)\n",
    "#         else:\n",
    "#             ValueError(\"{model} is not a valid model.\")\n",
    "#         clf.fit(X_train, y_train)\n",
    "#         return clf\n",
    "\n",
    "class RegressorTrainingPipeline(BaseTrainingPipeline):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.initialized_models = {\n",
    "            \"huber_regression\": HuberRegressor(),\n",
    "            \"linear_regression\": LinearRegression(),\n",
    "        }\n",
    "\n",
    "#     def train_model(self, algorithm):\n",
    "#         regressor = self.initialized_models[algorithm]\n",
    "#         regressor.fit(self.X_train, self.y_train)\n",
    "#         return regressor\n",
    "            \n",
    "# train a random forest and a logistic regresion model with the features \n",
    "# specified in SyntacticComplexityFeaturizer\n",
    "classifier_trainer = ClassifierTrainingPipeline()\n",
    "classifier_trainer.pipeline_main(\n",
    "    corpus=corpus,\n",
    "    labels=labels,\n",
    "    FeaturizerClass=SyntacticComplexityFeaturizer,\n",
    "    algorithms=[\"random_forest\", \"logistic_regression\"],\n",
    "    test_ratio=0.2,\n",
    ")\n",
    "            \n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def write_regressor_results(self, algorithm, outfile):\n",
    "    # save model predictions \n",
    "    model = self.models[algorithm]\n",
    "    y_pred = model.predict(self.X_test)\n",
    "    self.y_preds.update({algo: y_pred})\n",
    "    \n",
    "    # get results (and save to a file)\n",
    "    results_dict = {\n",
    "        \"MAE\": round(mean_absolute_error(self.y_test, y_pred), 3),\n",
    "        \"MSE\": round(mean_squared_error(self.y_test, y_pred), 3),\n",
    "        \"RMSE\": round(np.sqrt(mean_squared_error(self.y_test, y_pred)), 3),\n",
    "        \"R^2\": round(r2_score(self.y_test, y_pred), 3),\n",
    "    }\n",
    "    results_df = pd.DataFrame.from_dict(results_dict, orient=\"index\")\n",
    "    outfile.write(results_df.to_string(header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4307ea3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "#nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0af75aeb",
   "metadata": {},
   "source": [
    "from gensim.models import Word2Vec\n",
    "model_ted = Word2Vec(sentences=good_token_clean, window=10, min_count=1, workers=4, sg=0)\n",
    "model_ted.predict_output_word(['taste'], topn=10)\n",
    "model_ted.predict_output_word(['service'], topn=10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "928284d5",
   "metadata": {},
   "source": [
    "#extract_data from url\n",
    "def extract_data(url):\n",
    "    list1 = []\n",
    "    count = 0\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        soup = BeautifulSoup(resp.text,'html.parser')\n",
    "        l = soup.find(class_ = 'av-company-description-page mb-2')\n",
    "        web = ''.join([i.text for i in l.find_all(['p', 'li'])])\n",
    "        list1.append(web)\n",
    "        return web\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "        \n",
    "jd_links = ['https://datahack.analyticsvidhya.com/jobathon/clix-capital/senior-manager-growthrisk-analytics-2',\n",
    "'https://datahack.analyticsvidhya.com/jobathon/clix-capital/manager-growth-analytics-2',\n",
    "'https://datahack.analyticsvidhya.com/jobathon/clix-capital/manager-risk-analytics-2',\n",
    "'https://datahack.analyticsvidhya.com/jobathon/cropin/data-scientist-85']\n",
    "\n",
    "jd_df = pd.DataFrame(columns = ['links', 'data'])\n",
    "jd_df['links'] = jd_links\n",
    "\n",
    "#Extracting the JD data from the links:\n",
    "for i in range(len(jd_df)):\n",
    "    jd_df['data'][i] = extract_data(jd_df['links'][i])\n",
    "    \n",
    "jd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15ef68a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP preprocess /TBI / read again to apply new features\n",
    "#https://www.analyticsvidhya.com/blog/2023/04/end-to-end-nlp-project-on-quora-duplicate-questions-pair-identification/\n",
    "\n",
    "def preprocess(q):\n",
    "    \n",
    "    q = str(q).lower().strip()\n",
    "    \n",
    "    # Replace certain special characters with their string equivalents\n",
    "    q = q.replace('%', ' percent')\n",
    "    q = q.replace('$', ' dollar ')\n",
    "    q = q.replace('', ' rupee ')\n",
    "    q = q.replace('', ' euro ')\n",
    "    q = q.replace('@', ' at ')\n",
    "    \n",
    "    # The pattern '[math]' appears around 900 times in the whole dataset.\n",
    "    q = q.replace('[math]', '')\n",
    "    \n",
    "    # Replacing some numbers with string equivalents\n",
    "    q = q.replace(',000,000,000 ', 'b ')\n",
    "    q = q.replace(',000,000 ', 'm ')\n",
    "    q = q.replace(',000 ', 'k ')\n",
    "    q = re.sub(r'([0-9]+)000000000', r'\\1b', q)\n",
    "    q = re.sub(r'([0-9]+)000000', r'\\1m', q)\n",
    "    q = re.sub(r'([0-9]+)000', r'\\1k', q)\n",
    "    \n",
    "    # Decontracting words\n",
    "    # https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions\n",
    "    # https://stackoverflow.com/a/19794953\n",
    "    contractions = { \n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"can't've\": \"can not have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "    }\n",
    "    \n",
    "    q_decontracted = []\n",
    "\n",
    "    for word in q.split():\n",
    "        if word in contractions:\n",
    "            word = contractions[word]\n",
    "\n",
    "        q_decontracted.append(word)\n",
    "        \n",
    "    q = ' '.join(q_decontracted)\n",
    "    q = q.replace(\"'ve\", \" have\")\n",
    "    q = q.replace(\"n't\", \" not\")\n",
    "    q = q.replace(\"'re\", \" are\")\n",
    "    q = q.replace(\"'ll\", \" will\")\n",
    "    \n",
    "    # Removing HTML tags\n",
    "    q = BeautifulSoup(q)\n",
    "    q = q.get_text()\n",
    "    \n",
    "    # Remove punctuations\n",
    "    pattern = re.compile('\\W')\n",
    "    q = re.sub(pattern, ' ', q).strip()\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f8fba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modeling_ldamodel(data_df = None, text_col = 'utterance', coherence='u_mass', num_topics = 5,\n",
    "        num_topics_plot_chk = True, extend_stopword_list=[], filter_extremes=False, visualize_topics = False):\n",
    "\n",
    "    '''\n",
    "    ver 1.0 May12,2023 initial version\n",
    "    ver 1.1 May15,2023 added topic_name,filter_extremes\n",
    "    \n",
    "    Note\n",
    "    coherence='u_mass' # C_v, C_p, C_uci, C_umass, C_npmi, C_a\n",
    "    \n",
    "    filter_extremes should be true when database ig high\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import spacy\n",
    "\n",
    "    #import en_core_web_md\n",
    "    from gensim.corpora.dictionary import Dictionary\n",
    "    from gensim.models import LdaMulticore\n",
    "    from gensim.models import CoherenceModel\n",
    "\n",
    "    series_data = data_df[text_col]\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    removal= ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE', 'NUM', 'SYM']\n",
    "\n",
    "    if extend_stopword_list:\n",
    "        nlp.Defaults.stop_words |= set(extend_stopword_list)\n",
    "        \n",
    "    tokens = []\n",
    "    for summary in nlp.pipe(series_data):\n",
    "        proj_tok = [token.lemma_.lower() for token in summary if token.pos_ not in removal and not token.is_stop and token.is_alpha]\n",
    "        tokens.append(proj_tok)\n",
    "\n",
    "    data_df['tokens'] = tokens\n",
    "\n",
    "    #3 Create dictionary and corpus \n",
    "    #Dictionary: The idea of the dictionary is to give each token a unique ID.\n",
    "    #Corpus: Having assigned a unique ID to each token, the corpus simply contains each ID and its frequency \n",
    "\n",
    "    # I will apply the Dictionary Object from Gensim, which maps each word to their unique ID:\n",
    "    dictionary = Dictionary(data_df['tokens'])\n",
    "    print('dictionary:', dictionary.token2id)\n",
    "\n",
    "    #filter out low-frequency and high-frequency tokens, also limit the vocabulary to a max of 1000 words:\n",
    "    #dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)\n",
    "\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in data_df['tokens']]\n",
    "    #print('corpus:', corpus)\n",
    "\n",
    "    # #4 Coherence score and perplexity provide a convinent way to measure how good a given topic model is.\n",
    "    # #https://medium.com/analytics-vidhya/topic-modeling-using-gensim-lda-in-python-48eaa2344920\n",
    "    # id2word = TBC\n",
    "\n",
    "    # lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "    #                                            id2word=id2word,\n",
    "    #                                            num_topics=20, \n",
    "    #                                            random_state=100,\n",
    "    #                                            update_every=1,\n",
    "    #                                            chunksize=100,\n",
    "    #                                            passes=10,\n",
    "    #                                            alpha='auto',\n",
    "    #                                            per_word_topics=True)\n",
    "\n",
    "    # #4.1 Compute Perplexity - a measure of how good the model is. lower the better.\n",
    "    # print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  \n",
    "\n",
    "    # #4.2 Compute Coherence Score -Higher the topic coherence, the topic is more human interpretable.\n",
    "    # coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    # coherence_lda = coherence_model_lda.get_coherence()\n",
    "    # print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "\n",
    "    #4.3 coherence score - measures the degree of semantic similarity between high scoring words in each topic.\n",
    "    #The coherence score for C_v ranges from 0 (complete incoherence) to 1 (complete coherence). Values above 0.5 are fairly good\n",
    "    if num_topics_plot_chk:\n",
    "        topics = []\n",
    "        score = []\n",
    "        for i in range(1,20,1):\n",
    "            lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=10, num_topics=i, workers = 4, passes=10, random_state=100)\n",
    "            cm = CoherenceModel(model=lda_model, corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
    "            topics.append(i)\n",
    "            score.append(cm.get_coherence())\n",
    "        _=plt.plot(topics, score)\n",
    "        _=plt.xlabel('Number of Topics')\n",
    "        _=plt.ylabel('Coherence Score')\n",
    "        plt.show()\n",
    "\n",
    "    # check best topic w.r.t score - ranges from 0 to 1; Values above 0.5 are fairly good\n",
    "    topic_coherence_score_dict = {k: v for k, v in zip(topics, score)} #TBU/for ref\n",
    "    num_topics = score.index(min(score)) if num_topics is None else num_topics\n",
    "\n",
    "    #5 Model building - chg num_topics as per above plot\n",
    "    # LdaMulticore, uses all CPU cores to parallelize and speed up model training.\n",
    "    lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=50, num_topics=num_topics, workers = 4, passes=10)\n",
    "    #if does not work LdaMulticore then use an equivalent single-core model LdaModel\n",
    "    #gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, iterations=50, num_topics=10, workers = 4, passes=10)\n",
    "\n",
    "    topic_proba_list = lda_model.print_topics(-1)\n",
    "\n",
    "    # sort the list of tuples (topic, score) and add it\n",
    "    data_df['topic_id'] = [sorted(lda_model[corpus][text], key = lambda x: x[1], reverse=True)[0][0] for text in range(len(data_df[text_col]))]\n",
    "    data_df['topic_name'] = data_df['topic_id'].apply(lambda x: topic_proba_list[x][1].split('+')[0].split('*')[1].strip().strip('\"'))\n",
    "    \n",
    "    # Visualize topics / installation issue\n",
    "    if visualize_topics:\n",
    "        lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "        pyLDAvis.display(lda_display)\n",
    "\n",
    "        # Save the report\n",
    "        pyLDAvis.save_html(lda_display, 'index.html')\n",
    "\n",
    "    return topic_proba_list, data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "805df2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_textacy_entity_extraction_knowlege_graph(text_data, ent_type = \"DATE\",\n",
    "                records_groupby_id = None, ent_plot_filter = None, plot_save_path=None):\n",
    "\n",
    "    '''\n",
    "    1.1 May12,2023 added records_groupby_id option/ used to group records like chat @ user is level\n",
    "    1.0 May03,2023 initial version\n",
    "\n",
    "    plot_save_path: if not none then it will create Network Graph\n",
    "    ent_type = NUMERIC, PERSON, ORG, LOC, NUMERIC, DATE\n",
    "    '''\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    import spacy \n",
    "    import textacy  #0.12.0\n",
    "    import networkx as nx  #3.0 (also pygraphviz==1.10)\n",
    "    import dateparser #1.1.7\n",
    "    \n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text_data)\n",
    "\n",
    "    #3 from text to a list of sentences\n",
    "    lst_docs = [sent for sent in doc.sents]\n",
    "\n",
    "    dic_sub_vrb_obj = {\"id\":[], \"text\":[], \"sub\":[], \"relation\":[], \"object\":[]}\n",
    "    dic_ent = {\"id\":[], \"text\":[], ent_type:[]}\n",
    "    dic_all_ent = {\"id\":[], \"text\":[], 'all_ent':[]}\n",
    "    dic_all_noun = {\"id\":[], \"text\":[], 'noun':[]}\n",
    "\n",
    "    for n, sentence in enumerate(lst_docs):\n",
    "        #print(f'\\n sentence no:{n}, sentence:{sentence}')\n",
    "        lst_sub_vrb_obj = list(textacy.extract.subject_verb_object_triples(sentence))\n",
    "        lst_ent = list(textacy.extract.entities(sentence, include_types={ent_type}))\n",
    "        lst_all_ent = list(textacy.extract.entities(sentence))\n",
    "        lst_all_noun = list(textacy.extract.noun_chunks(sentence))\n",
    "        list_all_tokens = list(textacy.extract.words(sentence))\n",
    "\n",
    "        #1 lst_ent\n",
    "        if len(lst_ent) > 0:\n",
    "            for attr in lst_ent:\n",
    "                dic_ent[\"id\"].append(n)\n",
    "                dic_ent[\"text\"].append(sentence.text)\n",
    "                dic_ent[ent_type].append(str(attr))\n",
    "        else:\n",
    "            dic_ent[\"id\"].append(n)\n",
    "            dic_ent[\"text\"].append(sentence.text)\n",
    "            dic_ent[ent_type].append(np.nan)\n",
    "\n",
    "        dtf_ent = pd.DataFrame(dic_ent)\n",
    "\n",
    "        #2 lst_all_ent\n",
    "        if len(lst_all_ent) > 0:\n",
    "            for attr1 in lst_all_ent:\n",
    "                dic_all_ent[\"id\"].append(n)\n",
    "                dic_all_ent[\"text\"].append(sentence.text)\n",
    "                dic_all_ent['all_ent'].append(str(attr1))\n",
    "        else:\n",
    "            dic_all_ent[\"id\"].append(n)\n",
    "            dic_all_ent[\"text\"].append(sentence.text)\n",
    "            dic_all_ent['all_ent'].append(np.nan)\n",
    "\n",
    "        dtf_all_ent = pd.DataFrame(dic_all_ent)\n",
    "        if records_groupby_id:\n",
    "            dtf_all_ent = dtf_all_ent.groupby(records_groupby_id).agg(lambda x: list(x)).reset_index()\n",
    "\n",
    "        #3 lst_sub_vrb_obj\n",
    "        if len(lst_sub_vrb_obj) > 0:\n",
    "            for m, sent in enumerate(lst_sub_vrb_obj): #one sentence can have more than 1 one sub/verb/obj\n",
    "                subj = \"_\".join(map(str, sent.subject))\n",
    "                obj  = \"_\".join(map(str, sent.object))\n",
    "                relation = \"_\".join(map(str, sent.verb))\n",
    "                dic_sub_vrb_obj[\"id\"].append(n)\n",
    "                dic_sub_vrb_obj[\"text\"].append(sentence.text)\n",
    "                dic_sub_vrb_obj[\"sub\"].append(subj)\n",
    "                dic_sub_vrb_obj[\"object\"].append(obj)\n",
    "                dic_sub_vrb_obj[\"relation\"].append(relation)\n",
    "        else:\n",
    "            dic_sub_vrb_obj[\"id\"].append(n)\n",
    "            dic_sub_vrb_obj[\"text\"].append(sentence.text)\n",
    "            dic_sub_vrb_obj[\"sub\"].append(np.nan)\n",
    "            dic_sub_vrb_obj[\"object\"].append(np.nan)\n",
    "            dic_sub_vrb_obj[\"relation\"].append(np.nan)\n",
    "\n",
    "        dtf_sub_vrb_obj = pd.DataFrame(dic_sub_vrb_obj)\n",
    "\n",
    "        #4 lst_all_noun\n",
    "        if len(lst_all_noun) > 0:\n",
    "            for attr1 in lst_all_noun:\n",
    "                dic_all_noun[\"id\"].append(n)\n",
    "                dic_all_noun[\"text\"].append(sentence.text)\n",
    "                dic_all_noun['noun'].append(str(attr1))\n",
    "        else:\n",
    "            dic_all_noun[\"id\"].append(n)\n",
    "            dic_all_noun[\"text\"].append(sentence.text)\n",
    "            dic_all_noun['all_ent'].append(np.nan)\n",
    "\n",
    "        dtf_all_noun = pd.DataFrame(dic_all_noun)\n",
    "        dtf_all_noun = dtf_all_noun.groupby('id').agg(lambda x: list(x)).reset_index()\n",
    "\n",
    "    # merge the all files\n",
    "    dtf_final = dtf_sub_vrb_obj.merge(dtf_ent[[ent_type, 'id']], how=\"left\", on=\"id\")\n",
    "    dtf_final = dtf_final.merge(dtf_all_ent[['all_ent', 'id']], how=\"left\", on=\"id\")\n",
    "    dtf_final = dtf_final.merge(dtf_all_noun[['noun', 'id']], how=\"left\", on=\"id\")\n",
    "\n",
    "    if plot_save_path:\n",
    "        if ent_plot_filter:\n",
    "            dtf_final = dtf_final[(dtf_final[\"sub\"]==ent_plot_filter) | (dtf_final[\"object\"]==ent_plot_filter)]\n",
    "\n",
    "        ## create full graph\n",
    "        G = nx.from_pandas_edgelist(dtf_final, source=\"sub\", target=\"object\", \n",
    "                                    edge_attr=\"relation\", create_using=nx.DiGraph())\n",
    "        ## plot\n",
    "        plt.figure(figsize=(15,10))\n",
    "\n",
    "        pos = nx.spring_layout(G, k=1)\n",
    "        node_color = \"skyblue\"\n",
    "        edge_color = \"black\"\n",
    "\n",
    "        nx.draw(G, pos=pos, with_labels=True, node_color=node_color, \n",
    "                edge_color=edge_color, cmap=plt.cm.Dark2, \n",
    "                node_size=2000, connectionstyle='arc3,rad=0.1')\n",
    "\n",
    "        nx.draw_networkx_edge_labels(G, pos=pos, label_pos=0.5, \n",
    "                                 edge_labels=nx.get_edge_attributes(G,'relation'),\n",
    "                                 font_size=12, font_color='black', alpha=0.6)\n",
    "        plt.show()\n",
    "        plt.savefig(plot_save_path+'EntityKnowledgeGraph.png')\n",
    "\n",
    "    return dtf_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "810e6034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_clustering(data_df, text_col):\n",
    "    '''\n",
    "    May 15,2023: initial version\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.cluster import MiniBatchKMeans\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.decomposition import PCA\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    random_state = 0 \n",
    "\n",
    "    vec = TfidfVectorizer(stop_words=\"english\")\n",
    "    vec.fit(data_df[text_col].values)\n",
    "    features = vec.transform(data_df[text_col].values)\n",
    "\n",
    "    cls = MiniBatchKMeans(n_clusters=5, random_state=random_state)\n",
    "    cls.fit(features)\n",
    "    \n",
    "    y_pred= cls.predict(features)\n",
    "    \n",
    "    data_df['cluster_label'] = cls.labels_\n",
    "    return data_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp397]",
   "language": "python",
   "name": "conda-env-nlp397-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
