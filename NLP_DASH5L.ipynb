{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba5db782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text target\n",
       "0  Go until jurong point, crazy.. Available only ...    ham\n",
       "1                      Ok lar... Joking wif u oni...    ham\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   spam\n",
       "3  U dun say so early hor... U c already then say...    ham\n",
       "4  Nah I don't think he goes to usf, he lives aro...    ham"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'D:\\\\NLP\\\\data\\\\SMS_Spam\\\\'\n",
    "file_name = 'spam.csv'\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(data_path+file_name, encoding=\"ISO-8859-1\")\n",
    "df.rename(columns={'v1': 'target', 'v2': 'text'}, inplace=True)\n",
    "\n",
    "df = df[['text', 'target']]\n",
    "\n",
    "text_col = 'text'\n",
    "target_col = 'target'\n",
    "\n",
    "# X = df[text_col].values\n",
    "# y = df[target_col].values\n",
    "\n",
    "X = df[text_col]\n",
    "y = df[target_col]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)\n",
    "\n",
    "#df = df.dropna(subset=['text', 'target'], inplace=True) #TBC\n",
    "print(f'X_train:{type(X_train)}')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61b0e2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "\n",
    "from Common_Functions import *\n",
    "\n",
    "\n",
    "from PP_Pipeline_Functions import *\n",
    "%store -r feature_create_dict\n",
    "%store -r pp_dict\n",
    "%store -r model\n",
    "%store -r params\n",
    "%store -r feature_drop_dict\n",
    "%store -r feature_discretisation_dict\n",
    "%store -r feature_transformer_dict\n",
    "%store -r feature_outlier_dict\n",
    "%store -r feature_lagfreqwindow_dict\n",
    "%store -r feature_datetime_dict\n",
    "%store -r feature_datetime_list\n",
    "%store -r feature_lagfreqwindow_list\n",
    "%store -r feature_selection_dict\n",
    "%store -r imputer_dict\n",
    "%store -r imputaion_data_type_dict\n",
    "%store -r encoder_dict\n",
    "%store -r simple_encoder_dict\n",
    "%store -r scaler_dict\n",
    "%store -r sample_type_dict\n",
    "%store -r reduction_type_dict\n",
    "\n",
    "%store -r DropDupRows_fs\n",
    "%store -r DropNullRows_fs\n",
    "%store -r DropDupRows_ft\n",
    "%store -r DropNullRows_ft\n",
    "\n",
    "%store -r GroupFeatures_ft\n",
    "%store -r GroupFeatures_fs\n",
    "\n",
    "\n",
    "from Models_Functions import *\n",
    "%store -r models_fname_listR\n",
    "%store -r models_sname_listR\n",
    "%store -r models_fdictR\n",
    "%store -r models_sdictR\n",
    "%store -r params_pipeline_fdictR\n",
    "%store -r params_pipeline_sdictR\n",
    "\n",
    "%store -r models_fname_listC\n",
    "%store -r models_sname_listC\n",
    "%store -r models_fdictC\n",
    "%store -r models_sdictC\n",
    "%store -r params_pipeline_fdictC\n",
    "%store -r params_pipeline_sdictC\n",
    "\n",
    "%store -r tt_cv_pt_dict\n",
    "\n",
    "%store -r keras_metrics_flistR\n",
    "%store -r keras_metrics_slistR\n",
    "%store -r keras_metrics_flistC\n",
    "%store -r keras_metrics_slistC\n",
    "\n",
    "from NN_Functions import *\n",
    "from NLP_Functions import *\n",
    "#from TimeSeries_Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3298d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b60b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.6 / Oct01,2022 / moved all functions to respective functions\n",
    "'''\n",
    "Conut_vect /tf-idf/Glove_Embedding to be added as check box of pipeline thr column transformer\n",
    "Text feature/clean/Aug/scaling/selection to be added as check box of pipeline\n",
    "folder creation as per traget selection / folderpath text i/p to be added\n",
    "pp graph : traget distribution / text word cloud to be added\n",
    "\n",
    "ML_Models:\n",
    "Train_test/corss_val tt_cv_pt latest function TBU\n",
    "\n",
    "'''\n",
    "import dash\n",
    "from dash import dcc, html, dash_table\n",
    "import dash_bootstrap_components as dbc\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from dash.dependencies import State, Input, Output\n",
    "from dash.exceptions import PreventUpdate\n",
    "import dash_daq as daq\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "#external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "#app = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "#path\n",
    "project = 'spam_class'\n",
    "\n",
    "# data_path = 'D:\\\\NLP\\\\data\\\\'\n",
    "# pred_path = 'D:\\\\NLP\\\\pred\\\\'\n",
    "# #pred_consolidated_path = 'D:\\\\NLP\\\\pred\\\\consolidated\\\\'\n",
    "# reports_path = 'D:\\\\NLP\\\\reports\\\\'\n",
    "# model_path = 'D:\\\\NLP\\\\models\\\\'\n",
    "# NN_model_path = 'D:\\\\NLP\\\\models\\\\NN_models'\n",
    "# plot_path = 'D:\\\\NLP\\\\plots\\\\'\n",
    "\n",
    "#glove details\n",
    "glove_path = 'D:\\\\NLP\\\\glove_word_embedding\\\\'\n",
    "file50d = 'glove.6B.50d.txt'\n",
    "file100d = 'glove.6B.100d.txt'\n",
    "file200d = 'glove.6B.200d.txt'\n",
    "glove_file = file50d \n",
    "\n",
    "#tab pp_mpdel variables\n",
    "STOPWORDS_list = ['a', 'an', 'the']\n",
    "\n",
    "#tab ml_model variables\n",
    "score_error_list = ['test_recall', 'test_f1', 'loss_log', 'auc_roc_score', 'auc_prc_score', 'TPR', 'FPR', 'TNR', 'FNR', 'ACC', 'PPV', 'NPV']\n",
    "feat_impot_list = ['VarianceThreshold', 'SelectKBest', 'SelectPercentile', 'SelectFromModel', 'RFE', 'RFECV', 'SelectFpr', 'SelectFdr', 'SelectFwe']\n",
    "\n",
    "#tab nn_model variables\n",
    "#nn_model_list_lrng = ['lrng_embd_LSTM', 'lrng_embd_NN', 'lrng_embd_ShlowNN']\n",
    "#nn_model_list_trnd = ['trnd_embd_LSTM1', 'trnd_embd_CNN1D', 'trnd_embd_LSTM2', 'trnd_embd_GRU1', 'trnd_embd_BidirGRU1', 'trnd_embd_BidirRNN1', 'trnd_embd_BidirCuDNNLSTM']\n",
    "\n",
    "nn_model_list = ['lrng_embd_LSTM', 'lrng_embd_NN', 'lrng_embd_ShlowNN', 'trnd_embd_LSTM1', 'trnd_embd_CNN1D',\n",
    "        'trnd_embd_LSTM2', 'trnd_embd_GRU1', 'trnd_embd_BidirGRU1', 'trnd_embd_BidirRNN1', 'trnd_embd_BidirCuDNNLSTM']\n",
    "nn_score_error_list = ['mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_error', 'mean_squared_logarithmic_error', 'mean']\n",
    "nn_score_error_list1 = list(nn_score_error_list) # copy list , it will not impact to orignal list\n",
    "nn_score_error_list1.remove('mean') # NN_model does not support mean / TBC\n",
    "\n",
    "\n",
    "app.layout = html.Div([\n",
    "    #html.H1('Dash Tabs component demo'),\n",
    "    dcc.Tabs(id=\"NLP_Classification\", value='NLP Classification', children=[\n",
    "        dcc.Tab(label='PreProcess', value='Text_PP', children=[\n",
    "            html.Br(),\n",
    "            dbc.Row([\n",
    "                dbc.Col(dcc.Upload(id='upload-data', children=html.Div(['Drag and Drop or ', html.A('Select Files')]))\n",
    "                       ),\n",
    "                \n",
    "                dbc.Col(html.Div([\"Select ngrams max and min range\",\n",
    "                        dcc.RangeSlider(\n",
    "                        id='text_ngram_slider',\n",
    "                        min=0,\n",
    "                        max=5,\n",
    "                        step=1,\n",
    "                        value=[1, 2], tooltip = { 'always_visible': True })])\n",
    "                        ),\n",
    "            ]), #r1\n",
    "            html.Br(),\n",
    "\n",
    "            dbc.Row([\n",
    "                dbc.Col(dcc.Dropdown(id=\"pp_text_dd\",\n",
    "                    multi=False,\n",
    "                    placeholder=\"Select Text data column\",\n",
    "                    clearable=False,  \n",
    "                    searchable=False),\n",
    "                    ), #r2c1\n",
    "                    \n",
    "                dbc.Col(dcc.Dropdown(id=\"pp_target_dd\",\n",
    "                    multi=False,\n",
    "                    placeholder=\"Select target column\",\n",
    "                    clearable=False,  \n",
    "                    searchable=False),\n",
    "                    ), #r2c2\n",
    "                \n",
    "                dbc.Col(daq.ToggleSwitch(id='pp_text_feat_chk', label='Text Features', value=False)),\n",
    "                dbc.Col(daq.ToggleSwitch(id='pp_text_clean_chk', label='Text Clean', value=False)),\n",
    "                dbc.Col(daq.ToggleSwitch(id='pp_text_aug_chk', label='Text Augmentation', value=False)),\n",
    "                dbc.Col(daq.ToggleSwitch(id='pp_data_scale_chk', label='Feature Scaling', value=False)),\n",
    "                dbc.Col(daq.ToggleSwitch(id='pp_feat_select_chk', label='Imp Feature Selection', value=False)),\n",
    "                \n",
    "            ]), #r2\n",
    "            html.Br(),\n",
    "            \n",
    "            dbc.Row([\n",
    "                dbc.Col(dcc.RadioItems(\n",
    "                id='pp_text_encoding_radio', value='Count_vect',\n",
    "                options=[{'label': x, 'value': x} for x in [ 'Count_vect', 'TfIdf_vect', 'Glove_Embedding']],\n",
    "                labelStyle={'width': '100px', 'display': 'inline-block'}),\n",
    "                       ), #r4c6\n",
    "                \n",
    "                dbc.Col(dcc.RadioItems(\n",
    "                id='pp_data_type_radio', value='Dense',\n",
    "                options=[{'label': x, 'value': x} for x in [ 'Dense', 'Sparse']],\n",
    "                labelStyle={'width': '100px', 'display': 'inline-block'}),\n",
    "                      ), #r4c6\n",
    "                \n",
    "            ]), #r3\n",
    "            html.Br(),\n",
    "            html.Div([\n",
    "                dbc.Row([\n",
    "                    dbc.Col(dcc.Input(id=\"pp_min_df\", type=\"number\", placeholder=\"enter min df\", min=0.0, max=1.0, debounce=True,\n",
    "                              style={'width': '200px', 'margin-left':'10px', 'display': 'inline-block'})\n",
    "                           ),\n",
    "\n",
    "                    dbc.Col(dcc.Input(id=\"pp_max_df\", type=\"number\", placeholder=\"enter max df\", min=0.0, max=1.0, debounce=True,\n",
    "                              style={'width': '200px', 'margin-left':'10px', 'display': 'inline-block'})\n",
    "                           ),\n",
    "\n",
    "                    dbc.Col(dcc.Input(id=\"pp_max_feat\", type=\"number\", placeholder=\"enter max features\", debounce=True,\n",
    "                              style={'width': '200px', 'margin-left':'10px', 'display': 'inline-block'})\n",
    "                           ),\n",
    "\n",
    "                    dbc.Col(dcc.Dropdown(\n",
    "                        id=\"pp_analyzer_dd\",\n",
    "                        options=[{'label':x, 'value':x} for x in ['word', 'char', 'char_wb']],\n",
    "                        multi=False,\n",
    "                        placeholder=\"Select analyzer\",\n",
    "                        clearable=False,  \n",
    "                        searchable=False,\n",
    "                        style={'width': '200px', 'margin-left':'10px', 'display': 'inline-block'})\n",
    "                           ),\n",
    "                ]), #r4\n",
    "            ], style= {'display': 'block'}), #html.Div , will be changed by the dropdown callback\n",
    "            html.Br(),\n",
    "            \n",
    "\n",
    "                #4 data button\n",
    "                html.Button(\"Preprocess\", id=\"pp_btn\"),\n",
    "\n",
    "                # output dispaly\n",
    "                html.Div(id=\"output_data_pp\"),\n",
    "\n",
    "                #tab ts_train end\n",
    "                html.Br(),\n",
    "                dcc.Graph(id=\"Data_PP\"),\n",
    "    \n",
    "            ]), # # end tab data_pp\n",
    "\n",
    "        \n",
    "        \n",
    "        dcc.Tab(label='ML Models', value='ML_Models', children=[\n",
    "            html.Div(style={'color': 'blue', 'fontSize': 16, 'textAlign': 'center'}, children=[\n",
    "            html.Br(),\n",
    "            dbc.Row(\n",
    "                dbc.Col(dcc.Dropdown(\n",
    "                    id=\"ml_pp_ptrn_dd\",\n",
    "                    multi=False,\n",
    "                    placeholder=\"Select PP pattern data\",\n",
    "                    clearable=False,  \n",
    "                    searchable=False,\n",
    "                    style={'width': '1000px', 'margin-left':'10px', 'display': 'inline-block'})\n",
    "                       )\n",
    "                ),\n",
    "                html.Br(),\n",
    "                \n",
    "            dbc.Row(\n",
    "                dbc.Col(dcc.Checklist(id='ml_models_chk',\n",
    "                    #options=[{'label':x, 'value':x} for x in list(models_dict.keys())], \n",
    "                    options=[{'label':x, 'value':x} for x in list(models_fdictC.keys())], # Oct01,2022\n",
    "                    value=['LogisticRegression'],\n",
    "                    #style={'width': '200px', 'margin-left':'10px', 'display': 'inline-block'}\n",
    "                    labelStyle={\"display\": \"inline-block\"})\n",
    "                       ),\n",
    "                ),\n",
    "                 #score/error dropdown\n",
    "            dbc.Row([\n",
    "                dbc.Col(dcc.Dropdown(\n",
    "                    id=\"ml_score_dd\",\n",
    "                    options=[{'label':x, 'value':x} for x in score_error_list],\n",
    "                    multi=False,\n",
    "                    placeholder=\"Select score/error Name\",\n",
    "                    clearable=False,  \n",
    "                    searchable=False,\n",
    "                    style={'width': '200px', 'margin-left':'10px', 'display': 'inline-block'})\n",
    "                       ),\n",
    "                \n",
    "                dbc.Col(dcc.RadioItems(\n",
    "                id='pp_training_type_radio', value='TrainTest',\n",
    "                options=[{'label': x, 'value': x} for x in [ 'TrainTest', 'CrossValid', 'GridSearch']],\n",
    "                labelStyle={'width': '100px', 'display': 'inline-block'})\n",
    "                        ), #r4c6\n",
    "            ]),\n",
    "                html.Br(),\n",
    "                \n",
    "                #4 download data button\n",
    "                html.Button(\"train/test ML Models\", id=\"ml_btn\"),\n",
    "\n",
    "                # ts_train output dispaly\n",
    "                html.Div(id=\"output_ml_model\"),\n",
    "\n",
    "                 #tab ts_train end\n",
    "                   ], className='row'),\n",
    "                   html.Br(),\n",
    "                   dcc.Graph(id=\"ML_model\"),\n",
    "        ]), # end tab ML_Models\n",
    "        dcc.Tab(label='NN Models', value='NN_Models', children=[\n",
    "            html.Br(),\n",
    "            dbc.Row(\n",
    "                dbc.Col(dcc.Dropdown(\n",
    "                    id=\"nn_pp_ptrn_dd\",\n",
    "                    multi=False,\n",
    "                    placeholder=\"Select PP pattern data\",\n",
    "                    clearable=False,  \n",
    "                    searchable=False),\n",
    "                    width={\"size\": 10},\n",
    "                    ),\n",
    "            justify=\"center\",), #r1\n",
    "            \n",
    "            html.Br(),\n",
    "            dbc.Row(\n",
    "                #dbc.Col(\n",
    "                dbc.Checklist(id='nn_models_chk',\n",
    "                    options=[{'label':x, 'value':x} for x in nn_model_list],\n",
    "                    value=['lrng_embd_NN'],\n",
    "                    inline=True),\n",
    "            ), #r2\n",
    "            \n",
    "            #html.Br(),\n",
    "            dbc.Row(\n",
    "                #dbc.Col(\n",
    "                dbc.Checklist(id='nn_cols_chk',\n",
    "                    inline=True,\n",
    "                    labelStyle={ \"display\": \"inline-block\"}),\n",
    "                 #   ),\n",
    "            ), #r3\n",
    "            \n",
    "            html.Br(),\n",
    "            dbc.Row([\n",
    "                dbc.Col(dbc.Input(id=\"n_epochs\", type=\"number\", placeholder=\"enter n_epochs\", debounce=True)\n",
    "                    ),\n",
    "                dbc.Col(dbc.Input(id=\"n_neurons\", type=\"number\", placeholder=\"enter n_neurons\", debounce=True)\n",
    "                    ),\n",
    "                dbc.Col(dbc.Input(id=\"n_batch\", type=\"number\", placeholder=\"enter n_batch\", debounce=True)\n",
    "                    ),\n",
    "                dbc.Col(dbc.Input(id=\"n_repeats\", type=\"number\", placeholder=\"enter n_repeats\", debounce=True)\n",
    "                    ),\n",
    "            ]), #r4\n",
    "            \n",
    "            html.Br(),\n",
    "            dbc.Row([ \n",
    "                dbc.Col(dcc.Dropdown(\n",
    "                    id=\"nn_optimizer_dd\",\n",
    "                    options=[{'label':x, 'value':x} for x in ['Adadelta', 'Adagrad', 'Adam', 'RMSprop', 'SGD']],\n",
    "                    multi=False,\n",
    "                    placeholder=\"Select optimizer\",\n",
    "                    clearable=False,  \n",
    "                    searchable=False)\n",
    "                    ),\n",
    "                \n",
    "                dbc.Col(dcc.Dropdown(\n",
    "                    id=\"nn_score_dd\",\n",
    "                    options=[{'label':x, 'value':x} for x in nn_score_error_list1],\n",
    "                    multi=False,\n",
    "                    placeholder=\"Select score matrix\",\n",
    "                    clearable=False,  \n",
    "                    searchable=False)\n",
    "                    ),\n",
    "                \n",
    "                dbc.Col(dcc.Dropdown(\n",
    "                    id=\"nn_loss_dd\",\n",
    "                    options=[{'label':x, 'value':x} for x in nn_score_error_list1],\n",
    "                    multi=False,\n",
    "                    placeholder=\"Select loss function\",\n",
    "                    clearable=False,  \n",
    "                    searchable=False)\n",
    "                    ),\n",
    "            ]), #r5\n",
    "            \n",
    "            html.Br(),\n",
    "            dbc.Row([\n",
    "                dbc.Col(html.Div([\"Select learning rate\",\n",
    "                    dcc.Slider(\n",
    "                        id='nn_lr_slider',\n",
    "                        min=.001,\n",
    "                        max=0.1,\n",
    "                        step=0.001,\n",
    "                        value=0.1,  tooltip = { 'always_visible': True })],)\n",
    "                    ), \n",
    "                \n",
    "            ]), #r6\n",
    "            \n",
    "            dbc.Row(\n",
    "                dbc.Col(html.Button('train/test NN Models', id='nn_btn'),\n",
    "                       width={\"size\": 6, \"offset\": 3},),\n",
    "                align=\"center\", justify=\"center\",), #r5\n",
    "            \n",
    "                html.Div(id=\"output_nn_model\"),\n",
    "\n",
    "               #], className='row'), #html.Div(style close\n",
    "        \n",
    "               html.Br(),\n",
    "               dcc.Graph(id=\"NN_model\"),\n",
    "        ]),\n",
    "        \n",
    "        dcc.Tab(label='Stat Models', value='Stat_Models', children=[]),\n",
    "        \n",
    "    ]) #Tabs\n",
    "]) #layout\n",
    "\n",
    "# Tab: Text_PP\n",
    "# def read_file(contents, filename):\n",
    "#     import base64\n",
    "#     import io\n",
    "    \n",
    "#     print('read_file')\n",
    "#     if contents is not None:\n",
    "#         content_type, content_string = contents.split(',')\n",
    "#         global df\n",
    "        \n",
    "#         decoded = base64.b64decode(content_string)\n",
    "#         try:\n",
    "#             if 'csv' in filename:  #user uploaded a CSV file\n",
    "#                 #df = pd.read_csv(io.StringIO(decoded.decode('utf-8')))\n",
    "#                 df = pd.read_csv(io.StringIO(decoded.decode('ISO-8859-1')))\n",
    "#                 print('csv:', df.shape)\n",
    "#             elif 'xls' in filename: #user uploaded a xls file\n",
    "#                 df = pd.read_excel(io.BytesIO(decoded))\n",
    "#                 print('xls:', df.shape)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#         return df\n",
    "\n",
    "@app.callback(\n",
    "      [Output('pp_text_dd', 'options'),\n",
    "      Output('pp_target_dd', 'options'),\n",
    "      ],\n",
    "      Input('upload-data', 'contents'), # chk file contents\n",
    "      State('upload-data', 'filename'),\n",
    "      )\n",
    "\n",
    "def data_pp_dd_text_target1(contents, filename):\n",
    "    print('callback data_pp_dd_text_target1')\n",
    "    global df # moved from read_file Oct2,2022\n",
    "    \n",
    "    df = read_file(contents, filename)\n",
    "    print(f'df-shp:{df.shape}, df-cols:{df.columns.tolist()}')\n",
    "    \n",
    "    if contents is not None:\n",
    "        option=[{'label':x, 'value':x} for x in df.columns.tolist()]\n",
    "        #print('list_of_contents is not None')\n",
    "        options = [option, option]\n",
    "        return options\n",
    "    \n",
    "    \n",
    "@app.callback(\n",
    "      Output(component_id='pp_min_df', component_property='style'),\n",
    "#      Output(component_id='pp_max_df', component_property='style'),\n",
    "#      Output(component_id='pp_max_feat', component_property='style'),\n",
    "#      Output(component_id='pp_analyzer_dd', component_property='style')\n",
    "   [Input(component_id='pp_text_encoding_radio', component_property='value')])\n",
    "\n",
    "def show_hide_element_Data_pp2(visibility_state):\n",
    "    print('callback show_hide_element_Data_pp2')\n",
    "    if (visibility_state == 'CountVect') or (visibility_state == 'TfIdf'):\n",
    "        print('if CountVect/tfidf')\n",
    "        return {'display': 'block'}\n",
    "    else:\n",
    "        print('else other')\n",
    "        return {'display': 'none'}\n",
    "    \n",
    "    \n",
    "@app.callback(\n",
    "    [\n",
    "    Output('Data_PP', 'figure'),\n",
    "    Output(\"output_data_pp\", \"children\"),\n",
    "    ],\n",
    "    [\n",
    "     Input('pp_text_dd', 'value'),\n",
    "     Input('pp_target_dd', 'value'),\n",
    "     Input('pp_text_feat_chk', 'value'), #\n",
    "     Input('pp_text_clean_chk', 'value'),\n",
    "     Input('pp_text_aug_chk', 'value'),\n",
    "     Input('pp_data_scale_chk', 'value'),\n",
    "        \n",
    "     Input('pp_text_encoding_radio', 'value'),\n",
    "     Input('pp_min_df', 'value'),\n",
    "     Input('pp_max_df', 'value'),\n",
    "     Input('pp_max_feat', 'value'),\n",
    "     \n",
    "     Input('pp_ngram_slider', 'value'),\n",
    "    \n",
    "     Input('pp_analyzer_dd', 'value'),\n",
    "     Input('pp_feat_select_chk', 'value'),\n",
    "     Input('pp_data_type_radio', 'value'),\n",
    "     Input(\"pp_btn\", \"n_clicks\")\n",
    "    ], prevent_initial_call=True)\n",
    "\n",
    "def update_Data_pp_final3(pp_text_dd, pp_target_dd, pp_text_feat_chk, pp_text_clean_chk, pp_text_aug_chk, pp_data_scale_chk,\n",
    "                   pp_text_encoding_radio, pp_min_df, pp_max_df, pp_max_feat, pp_ngram_slider, pp_analyzer_dd,\n",
    "                   pp_feat_select_chk, pp_data_type_radio, pp_btn):\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    print('callback update_Data_pp_final3')\n",
    "    print(f'df-shp:{df.shape}, df-cols:{df.columns.tolist()}, pp_text_dd:{pp_text_dd}, pp_target_dd:{pp_target_dd}')\n",
    "    #df = df.copy()\n",
    "    #df = df[[pp_text_dd, pp_target_dd]] # this will now work,local variable 'df' referenced before assignment \n",
    "    \n",
    "    print('#1. filter df for text and target')\n",
    "    df1 = df[[pp_text_dd, pp_target_dd]]\n",
    "    print(f'df1-shp:{df1.shape}, df1-cols:{df1.columns.tolist()}')\n",
    "    \n",
    "    print('#2. rename columns as text and target')\n",
    "    global target_col, text_col\n",
    "    target_col = 'target'\n",
    "    text_col='text'\n",
    "    df1.rename(columns={pp_text_dd: text_col, pp_target_dd:target_col}, inplace=True)\n",
    "    print(f'df1-shp:{df1.shape}, df1-cols:{df1.columns.tolist()}')\n",
    "    \n",
    "    \n",
    "    print('#3. create project folder') #Oct01,2022\n",
    "    eda_text_input_project = 'D:\\\\NLP1\\\\' # temp path_text to be added in FE\n",
    "    print('eda_text_input_project:', eda_text_input_project)\n",
    "    global data_path, pred_path, reports_path, model_path, NN_model_path, plot_path\n",
    "    if eda_text_input_project:\n",
    "        project_path = eda_text_input_project\n",
    "    else: # create default AIML folder @ current working dir and add folders as per folder_list\n",
    "        project_folder = os.getcwd()+'\\\\NLP1'\n",
    "\n",
    "        if not os.path.exists(project_folder):\n",
    "            os.mkdir(project_folder)\n",
    "        \n",
    "        project_path = project_folder+'\\\\'+target_col # Sep15,2022\n",
    "        \n",
    "    folder_list = ['data', 'pred', 'reports', 'models', 'NN_models', 'plots']\n",
    "    data_path, pred_path, reports_path, model_path, NN_model_path, plot_path = create_ml_folders(project_path, folder_list)\n",
    "    print('eda_text_input_project:', eda_text_input_project)\n",
    "    print(f'data_path:{data_path}, pred_path:{pred_path}, reports_path:{reports_path}, model_path:{model_path},\\\n",
    "    NN_model_path:{NN_model_path}, plot_path:{plot_path}')\n",
    "    \n",
    "    \n",
    "    print('#4. set default values')\n",
    "    text_feat = 1 if pp_text_feat_chk is True else 0\n",
    "    text_clean = 1 if pp_text_clean_chk is True else 0\n",
    "    text_aug = 1 if pp_text_aug_chk is True else 0\n",
    "    data_scale = 1 if pp_data_scale_chk is True else 0\n",
    "    feat_select = 1 if pp_feat_select_chk is True else 0\n",
    "    \n",
    "    \n",
    "    pp_ngram = pp_ngram_slider\n",
    "    if pp_min_df is None: pp_min_df = 0.0\n",
    "    if pp_max_df is None: pp_max_df = 1.0   \n",
    "    if pp_max_feat is None: pp_max_feat = 100\n",
    "    if pp_analyzer_dd is None : pp_analyzer_dd = 'word'\n",
    "        \n",
    "    selected_value = f'pp_text_dd:{pp_text_dd}, pp_target_dd:{pp_target_dd}, text_feat:{text_feat}, text_clean:{text_clean},\\\n",
    "            text_aug:{text_aug}, data_scale:{data_scale}, pp_text_encoding_radio:{pp_text_encoding_radio},\\\n",
    "            pp_min_df:{pp_min_df}, pp_max_df:{pp_max_df}, pp_ngram_slider:{pp_ngram}, pp_analyzer_dd:{pp_analyzer_dd}'\n",
    "    print(f'df-shp:{df1.shape}, df-cols:{df1.columns.tolist()}, selected_value:{selected_value}')\n",
    "    \n",
    "    print('#5. text features')\n",
    "    print(f'df-typ:{type(df1)}, df-cols:{df1.columns}')\n",
    "    #text_col = 'text'\n",
    "    if text_feat == 1:\n",
    "        df1= extract_text_features(data=df1, STOPWORDS_list=STOPWORDS_list, stopword=True, fillna=True,\n",
    "                          text_col=text_col, word_feat=True, sentiment=True, pos=True, drop_text_col=False)\n",
    "        print(f'df1-shp:{df1.shape}, df1-cols:{df1.columns.tolist()}')\n",
    "        \n",
    "        # text feature to be created before text clean, to preserve features for X_train, X_valid,\n",
    "        X=df1[text_col] # return series after split\n",
    "        y=df1[target_col]\n",
    "        X_train, X_valid, _, _ = train_test_split(X, y, random_state=0) # return series of X_train, X_valid w.r.t X,y\n",
    "        print(f'X_train:{type(X_train)}, X_valid:{type(X_valid)}')\n",
    "        X_train_feat_df = df1.iloc[X_train.index,:].drop([text_col, target_col], axis=1) # only text features with split index\n",
    "        X_valid_feat_df = df1.iloc[X_valid.index,:].drop([text_col, target_col], axis=1) # only text features with split index\n",
    "        \n",
    "    \n",
    "    print('#6. text clean')\n",
    "    if text_clean == 1:\n",
    "        df1 = text_cleaning(data=df1, STOPWORDS_list=STOPWORDS_list, text_col=text_col,\n",
    "                n_rare_words = 10, n_most_common=10, series_out=False)\n",
    "        print(f'df1-shp:{df1.shape}, df1-cols:{df1.columns.tolist()}')\n",
    "        \n",
    "    print('#7. text augmenation')\n",
    "    if text_aug == 1:\n",
    "        pass\n",
    "    \n",
    "    print('#8. data imbalance')\n",
    "        \n",
    "    print('#9. targt encoding / dict')\n",
    "    target_key_set = set(df1[target_col].values)\n",
    "    df1[target_col] = df1[target_col].factorize()[0]\n",
    "    target_val_set = set(df1[target_col].values)\n",
    "    n_target = len(target_key_set)\n",
    "    target_dict = dict(zip(target_key_set, target_val_set))\n",
    "\n",
    "    print('#10. separate X, y')\n",
    "    X=df1[text_col].values # return arrays after split\n",
    "    y=df1[target_col].values\n",
    "    print(f'X-shp:{X.shape}, y-unq:{set(y)}, target_dict:{target_dict}, n_target:{n_target}')\n",
    "    \n",
    "    print('#11. split train/valid')\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0) # return arrays of X_train, X_valid w.r.t X,y\n",
    "          \n",
    "    print('#12. X encoding / Embedding')\n",
    "    object_param_dict = {}\n",
    "    \n",
    "    if (pp_text_encoding_radio == 'Count_vect') | (pp_text_encoding_radio == 'TfIdf_vect'):\n",
    "        vect, X_train = text_BoW_feature_selection_train(X_train, y_train, analyzer=pp_analyzer_dd,\n",
    "                max_df=pp_max_df, min_df=pp_min_df, max_features=pp_max_feat, ngram_range=pp_ngram, p_value_limit = 0.95,\n",
    "                matrix_type=pp_data_type_radio, BoW_type=pp_text_encoding_radio, feature_selection=feat_select)\n",
    "\n",
    "        X_valid = text_BoW_feature_selection_test(vect, X_valid, matrix_type=pp_data_type_radio)\n",
    "        object_param_dict['vect'] = vect\n",
    "        \n",
    "        # merge text features , needed only for classical models\n",
    "        if text_feat == 1:\n",
    "            X_train = add_vect_text_features(vect, vect_matrix=X_train, text_feat_df=X_train_feat_df)\n",
    "            X_valid = add_vect_text_features(vect, vect_matrix=X_valid, text_feat_df=X_valid_feat_df)\n",
    "\n",
    "    elif pp_text_encoding_radio == 'Glove_Embedding':\n",
    "        X_train, X_valid, vocab_size, tokenizer =  text_to_int(X_train, X_valid, padding_maxlen=100)\n",
    "        object_param_dict['vocab_size'] = vocab_size\n",
    "        object_param_dict['tokenizer'] = tokenizer\n",
    "        \n",
    "        # merge text features , not needed for NN models, But TBC how to merge with embedding\n",
    "        if text_feat == 1:#TBA\n",
    "            pass\n",
    "        \n",
    "    print('#13. Feature Scaling') # for scaling convert sparse to dense and after scaling convert back to sparse\n",
    "    if data_scale == 1:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        import numpy as np\n",
    "        scaler_X = MinMaxScaler(feature_range = (0, 1))\n",
    "        #scaler_y = MinMaxScaler(feature_range = (0, 1))\n",
    "    \n",
    "#         #scaling works only on dense data\n",
    "#         X_train = X_train.todense()\n",
    "#         X_valid = X_valid.todense()\n",
    "\n",
    "#         scaler_X.fit(X_train)\n",
    "#         X_train = scaler_X.transform(X_train)\n",
    "#         X_valid = scaler_X.transform(X_valid)\n",
    "        \n",
    "#         #convert back to sparse\n",
    "#         from scipy import sparse\n",
    "#         X_train=sparse.csr_matrix(X_train)\n",
    "#         X_valid=sparse.csr_matrix(X_valid)\n",
    "\n",
    "        #object_list.append[scaler_X] # other objects to be added\n",
    "        object_param_dict['scaler_X'] = scaler_X\n",
    "\n",
    "    print('#14. final test_type update')\n",
    "    test_type = f'{project}--{pp_text_encoding_radio}-{pp_data_type_radio}-{text_feat}-{text_clean}-{text_aug}-{data_scale}-{feat_select}'\n",
    "    print('test_type:', test_type)\n",
    "    \n",
    "    print('#15. update NLP objests eg. encoding/scaler')\n",
    "    #object_list = [vect]\n",
    "    FileName = 'NLP_objects_params'\n",
    "    scaled_df = chk_create_update_df_dict(data=None, file_path=data_path, file_name=FileName,\n",
    "                            dict_key=test_type, dict_value=object_param_dict, save_df=None)\n",
    "    \n",
    "    print('#16. update pre-processed data in pickle')\n",
    "    FileName = 'NLP_train_valid'\n",
    "    data_pp_Xy_tpl = X_train, X_valid, y_train, y_valid\n",
    "    report_df = chk_create_update_df_dict(data=None, file_path=data_path, file_name=FileName,\n",
    "                            dict_key=test_type, dict_value=data_pp_Xy_tpl, save_df=None)\n",
    "\n",
    "    output_dispay = selected_value+'_'+test_type\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]]) # Create figure with secondary y-axis\n",
    "    return fig, output_dispay\n",
    "    \n",
    "#################################Common#########################\n",
    "#1 drop down PP data\n",
    "\n",
    "@app.callback( \n",
    "    [Output('ml_pp_ptrn_dd', 'options'),\n",
    "    Output('nn_pp_ptrn_dd', 'options')],\n",
    "    Input('NLP_Classification', 'value'), prevent_initial_call=True) #dcc.Tabs(id=\"NLP_Classification\", value='NLP Classification', children=[\n",
    "\n",
    "def dropdown_pp_data(tab):\n",
    "    print('callback dropdown_pp_data')\n",
    "    FileName = 'NLP_train_valid'+'_pkl'\n",
    "    if tab is None:\n",
    "        raise PreventUpdate\n",
    "    else:\n",
    "        #if (tab == 'ML_Model'): #dcc.Tab(label='Stat Model', value='Stat_Model', children=[\n",
    "        if (tab == 'ML_Model') or (tab == 'NN_Model'): #dcc.Tab(label='Stat Model', value='Stat_Model', children=[\n",
    "            if os.path.exists(data_path+FileName):\n",
    "                infile = open(data_path+FileName, 'rb')\n",
    "                null_df = pickle.load(infile)\n",
    "                data_pp_dict = pickle.load(infile)\n",
    "            else:\n",
    "                data_pp_dict = {'data_pp to be executed1': 2, 'data_pp to be executed2': 3}\n",
    "        else:\n",
    "            if os.path.exists(data_path+FileName):\n",
    "                infile = open(data_path+FileName, 'rb')\n",
    "                null_df = pickle.load(infile)\n",
    "                data_pp_dict = pickle.load(infile)\n",
    "            else:\n",
    "                data_pp_dict = {'data_pp to be executed1': 2, 'data_pp to be executed2': 3}\n",
    "\n",
    "        ml_options=[{'label':x, 'value':x} for x in list(data_pp_dict.keys())] # ML tabs\n",
    "        data_pp_dict1 = {key:val for key, val in data_pp_dict.items() if 'Embedding' in key}\n",
    "        nn_options=[{'label':x, 'value':x} for x in list(data_pp_dict1.keys())] # NN tabs , it doe snot work for CountVect/TfIdf\n",
    "        \n",
    "        #print('options:', options)\n",
    "    #return options\n",
    "    return [ml_options, nn_options]\n",
    "\n",
    "#################################ML Models#########################\n",
    "# tab : ML Model - callback function\n",
    "\n",
    "#3 final callback\n",
    "@app.callback(\n",
    "    [\n",
    "    Output('ML_model', 'figure'),\n",
    "    Output(\"output_ml_model\", \"children\"),\n",
    "    ],\n",
    "    [\n",
    "     Input('ml_pp_ptrn_dd', 'value'), \n",
    "     Input('ml_models_chk', 'value'),\n",
    "     Input('ml_score_dd', 'value'),\n",
    "     Input('pp_training_type_radio', 'value'),\n",
    "     Input(\"ml_btn\", \"n_clicks\")\n",
    "    ], prevent_initial_call=True)\n",
    "\n",
    "def update_ML_model_final(ml_pp_ptrn_dd, ml_models_chk, ml_score_dd, pp_training_type_radio, ml_btn):\n",
    "    print('callback update_ML_model_final')\n",
    "    selection_status = f'selected ml_pp_ptrn_dd: {ml_pp_ptrn_dd}, ml_models_chk: {ml_models_chk},\\\n",
    "    ml_score_dd:{ml_score_dd}, pp_training_type_radio:{pp_training_type_radio}'\n",
    "    print('selection_status:', selection_status)\n",
    "    '''\n",
    "    Known Error:\n",
    "    to be done:\n",
    "    feature importance:\n",
    "    '''\n",
    "    print('1. setup default values')\n",
    "    if ml_score_dd is None: ml_score_dd = 'auc_prc_score'   \n",
    "    \n",
    "    print('2. read pre-processed data')\n",
    "    FileName = 'NLP_train_valid'+'_pkl'\n",
    "                              \n",
    "    if os.path.exists(data_path+FileName):\n",
    "        infile = open(data_path+FileName, 'rb')\n",
    "        null_df = pickle.load(infile)\n",
    "        data_pp_dict = pickle.load(infile)\n",
    "    else:\n",
    "        data_pp_dict = {'data_pp to be executed': 2}\n",
    "        \n",
    "    test_type = ml_pp_ptrn_dd\n",
    "    X_train, X_valid, y_train, y_valid = data_pp_dict[ml_pp_ptrn_dd]\n",
    "    print(f'X_train:{X_train.shape}')\n",
    "                              \n",
    "    print('3. read object list to update further')\n",
    "    FileName = 'NLP_objects_params'+'_pkl'\n",
    "                              \n",
    "    if os.path.exists(data_path+FileName):\n",
    "        infile = open(data_path+FileName, 'rb')\n",
    "        null_df = pickle.load(infile)\n",
    "        object_dict = pickle.load(infile)\n",
    "    else:\n",
    "        object_dict = {'data_pp to be executed': 2}\n",
    "           \n",
    "    #object_list = object_dict[ml_pp_ptrn_dd]\n",
    "    object_param_dict = object_dict[ml_pp_ptrn_dd]\n",
    "                              \n",
    "    print('4. train the selected models')                         \n",
    "    if ml_btn is None:\n",
    "        raise PreventUpdate\n",
    "    else:\n",
    "        print('ml_models_chk:', ml_models_chk)\n",
    "        for model_key in ml_models_chk:\n",
    "            #model = models_dict[model_key]\n",
    "            model = models_fdictC[model_key] # Oct01,2022\n",
    "            model_name = model.__class__.__name__\n",
    "            print('model in loop:', model_name)\n",
    "            \n",
    "            X=None; y=None; X_train=X_train; X_valid=X_valid; X_test=None; y_test=None; cv=3\n",
    "            report_dict, y_preds, y_probs, y_actual, trained_model = tt_cv_pt(X, y, X_train, y_train,\n",
    "                X_valid, y_valid, X_test, y_test,  model=model, param_grid=None, cv=cv, train_type=pp_training_type_radio)\n",
    "            \n",
    "            #temp\n",
    "#             class_type =\n",
    "#             class_mapping_dict =\n",
    "#             target_col =\n",
    "#             test_type =\n",
    "#             report_dict, pred_df, y_probs, trained_model = tt_cv_pt_class(X, y, X_train, y_train, X_valid, y_valid,\n",
    "#                         X_test=None, y_test=None, model=model, param_grid=None, n_splits=2, train_type=pp_training_type_radio,\n",
    "#                         class_type = class_type, class_mapping_dict=class_mapping, target=target_col, data_table = True, test_type=test_type)\n",
    "            \n",
    "            # add score/error in test_type\n",
    "            score_error = round(report_dict[ml_score_dd], 2)\n",
    "            #test_type = ml_pp_ptrn_dd+'__ML-'+model_name+'__'+ml_score_dd+'-'+str(score_error)\n",
    "            test_type = test_type+'-'+f'{pp_training_type_radio}__ML-{model_name}__{ml_score_dd}-{score_error}' # for ptrn dict_key\n",
    "            report_dict['test_type'] = test_type # it will overwrite earlier test_type by new test_type with score\n",
    "            print('test_type:', test_type)\n",
    "            \n",
    "            print('5. update models score in pickle')\n",
    "            FileName = 'NLP_model_score'\n",
    "            report_df = chk_create_update_df_dict(data=report_dict, file_path=data_path, file_name=FileName,\n",
    "                                    dict_key=test_type, dict_value=trained_model, save_df=None)\n",
    "                              \n",
    "            print('6. update NLP objests eg. trained models')\n",
    "            #object_list.append(trained_model)\n",
    "            print('object_param_dict before update:', object_param_dict)\n",
    "            object_param_dict['trained_model'] = trained_model\n",
    "            print('object_param_dict after update:', object_param_dict)\n",
    "                              \n",
    "            FileName = 'NLP_objects_params'\n",
    "            scaled_df = chk_create_update_df_dict(data=None, file_path=data_path, file_name=FileName,\n",
    "                                    dict_key=test_type, dict_value=object_param_dict, save_df=None)\n",
    "                              \n",
    "    selection_status = f'selected ml_pp_ptrn_dd: {ml_pp_ptrn_dd}, ml_models_chk: {ml_models_chk}, ml_score_dd:{ml_score_dd},\\\n",
    "    score_dict:{report_dict}' \n",
    "    print('selection_status:', selection_status)\n",
    "    \n",
    "    output_dispay = selection_status+'_'+test_type\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]]) # Create figure with secondary y-axis\n",
    "    return fig, output_dispay\n",
    "\n",
    "#################################NN Models#########################\n",
    "#NN Models\n",
    "@app.callback(\n",
    "    [\n",
    "    Output('NN_model', 'figure'),\n",
    "    Output(\"output_nn_model\", \"children\"),\n",
    "    ],\n",
    "    [\n",
    "     Input('nn_pp_ptrn_dd', 'value'), \n",
    "     Input('nn_models_chk', 'value'),    \n",
    "     Input('nn_cols_chk', 'value'),\n",
    "     Input('n_epochs', 'value'),\n",
    "     Input('n_neurons', 'value'),\n",
    "     Input('n_batch', 'value'),\n",
    "     Input('n_repeats', 'value'),\n",
    "     Input('nn_lr_slider', 'value'),\n",
    "     Input('nn_optimizer_dd', 'value'),\n",
    "     Input('nn_score_dd', 'value'),\n",
    "     Input('nn_loss_dd', 'value'),\n",
    "     Input(\"nn_btn\", \"n_clicks\")\n",
    "    ],prevent_initial_call=True)\n",
    "\n",
    "def update_NN_model_final(nn_pp_ptrn_dd, nn_models_chk, nn_cols_chk, n_epochs, n_neurons, n_batch, n_repeats, nn_lr_slider,\n",
    "                    nn_optimizer_dd, nn_score_dd, nn_loss_dd, nn_btn):\n",
    "    print('callback update_NN_model_final')\n",
    "    \n",
    "    '''\n",
    "    Known Error:\n",
    "    1. score/loss does not support 'mean'\n",
    "    \n",
    "    to be done:\n",
    "    '''\n",
    "    print('# setup default values')\n",
    "    if n_epochs is None: n_epochs = 2\n",
    "    if n_neurons is None: n_neurons = 20\n",
    "    if n_batch is None: n_batch = 2\n",
    "    \n",
    "    if nn_optimizer_dd is None: nn_optimizer_dd = 'Adam'\n",
    "    if nn_score_dd is None: nn_score_dd = 'mean_absolute_error'\n",
    "    if nn_loss_dd is None: nn_loss_dd = 'mean_absolute_error' \n",
    "    #print(f'default values->nn_forecast_period:{nn_forecast_period}, nn_steps_in_window:{nn_steps_in_window}')\n",
    "\n",
    "    selection_status = f'selected nn_pp_ptrn_dd: {nn_pp_ptrn_dd}, nn_models_chk: {nn_models_chk}, nn_cols_chk: {nn_cols_chk}' #add other i/p\n",
    "    print('selection_status:', selection_status)\n",
    "     \n",
    "    print('2. read pre-processed data')\n",
    "    FileName = 'NLP_train_valid'+'_pkl'\n",
    "                              \n",
    "    if os.path.exists(data_path+FileName):\n",
    "        infile = open(data_path+FileName, 'rb')\n",
    "        null_df = pickle.load(infile)\n",
    "        data_pp_dict = pickle.load(infile)\n",
    "    else:\n",
    "        data_pp_dict = {'data_pp to be executed': 2}\n",
    "        \n",
    "    test_type = nn_pp_ptrn_dd\n",
    "    X_train, X_valid, y_train, y_valid = data_pp_dict[nn_pp_ptrn_dd]\n",
    "    print(f'X_train:{X_train.shape}')\n",
    "                              \n",
    "    print('3. read object list to update further')\n",
    "    FileName = 'NLP_objects_params'+'_pkl'\n",
    "                              \n",
    "    if os.path.exists(data_path+FileName):\n",
    "        infile = open(data_path+FileName, 'rb')\n",
    "        null_df = pickle.load(infile)\n",
    "        object_dict = pickle.load(infile)\n",
    "    else:\n",
    "        object_dict = {'data_pp to be executed': 2}\n",
    "           \n",
    "    #object_list = object_dict[ml_pp_ptrn_dd]\n",
    "    object_param_dict = object_dict[nn_pp_ptrn_dd]\n",
    "    \n",
    "    if (nn_pp_ptrn_dd.find('Embedding') != -1):\n",
    "        vocab_size = object_param_dict['vocab_size']\n",
    "        tokenizer = object_param_dict['tokenizer']\n",
    "                              \n",
    "    print('4. train the selected models')                         \n",
    "    if nn_btn is None:\n",
    "        raise PreventUpdate\n",
    "    else:\n",
    "        for model in nn_models_chk:\n",
    "            print('model in loop:', model)\n",
    "            model_name = model\n",
    "            '''\n",
    "            pending:\n",
    "            num_classes / retrun-model_eval_dict TBC\n",
    "            '''\n",
    "            \n",
    "            if model.startswith('lrng'):\n",
    "                vector_size = 300 #TBC X_train.shape[0]\n",
    "                trained_weight=None\n",
    "            elif model.startswith('trnd'):\n",
    "                glove_embedding_vector = load_Glove_embedding_vector(glove_path+glove_file)\n",
    "                embedding_matrix = embedding_vector_2_matrix(vocab_size, tokenizer, glove_embedding_vector)\n",
    "\n",
    "                vector_size = len(list(glove_embedding_vector.values())[0]) # find vector size\n",
    "                trained_weight = embedding_matrix\n",
    "                \n",
    "            model_return = build_NN_text_class_model(vocab_size, vector_size, X_train, y_train, X_valid, y_valid,\n",
    "                                       trained_weight, model_type=model, num_classes = 1,\n",
    "                                       score_metrics = nn_score_dd, DropOutRate=0.25, optimizer = nn_optimizer_dd,\n",
    "                                       n_epochs = n_epochs, batch_size=n_batch, n_neurons= n_neurons, learning_rate=nn_lr_slider,\n",
    "                                       l2_loss_lambda=0.10, model_save_path='D:\\\\NLP\\\\models\\\\NN_models', test_type=test_type)\n",
    "\n",
    "            #trianed_model, history_df, train_loss, train_acc, valid_loss, valid_acc = model_return\n",
    "            model_eval_dict, test_pred, trianed_model, history_df = model_return\n",
    "            print('model_eval_dict:', model_eval_dict)\n",
    "            val_loss1 = round(history_df['val_loss'].iloc[-1], 2) #TBC which is best value\n",
    "\n",
    "            #test_type = f'{nn_pp_ptrn_dd}__step_in-{nn_steps_in_window}__NN-{model}__val_loss-{val_loss1}'\n",
    "            test_type = f'{nn_pp_ptrn_dd}__NN-{model}__val_loss-{val_loss1}'\n",
    "            print(f'test_type:{test_type}')\n",
    "            NN_model_save_path_name = NN_model_path+'\\\\'+test_type+'.h5'\n",
    "            trianed_model.save(NN_model_save_path_name)\n",
    "\n",
    "#             preds_metrics_dict = classification_error_score_metrics(y_actual=y_valid, y_preds=test_pred, y_probs=None,\n",
    "#                             model_name=model, test_type=test_type, n_features=1, plot=False, plot_path=None)\n",
    "#             print('preds_metrics_dict:', preds_metrics_dict)\n",
    "\n",
    "\n",
    "#             test_type = f'model_eval_dict: {model_eval_dict}, optimizer:{nn_optimizer_dd}, price_slid:{nn_price_slider},\\\n",
    "#             lrearning_rate:{nn_lr_slider}, n_epochs:{n_epochs}, n_batch:{n_batch}, n_neurons:{n_neurons}'\n",
    "#             print(f'history_df-shp:{history_df.shape}, history_df-cols:{history_df.columns}')\n",
    "\n",
    "#             print('\\n #6 regression other metrics')\n",
    "#             print(f'valid_y-len:{len(valid_y)}, test_pred-len:{len(test_pred)}')\n",
    "#             report_dict = regression_error_score_metrics(actual_y=valid_y, preds=test_pred, model_name=model,\n",
    "#                                                          n_features=n_features, plot=False, plot_path=None)\n",
    "#             #report_dict['n_repeats'] = i\n",
    "            \n",
    "#             # add selected_cols_list/all_cols_list in report_dict\n",
    "#             report_dict['selected_cols_list'] =  ','.join(selected_cols_list)\n",
    "#             report_dict['all_cols_list'] =  ','.join(all_cols_list)\n",
    "            \n",
    "#             report_dict['train_loss'] = history_df['loss'].iloc[-1]\n",
    "#             report_dict['val_loss'] = history_df['val_loss'].iloc[-1]\n",
    "\n",
    "#             #report_dict['forecast_period'] = nn_forecast_period\n",
    "#             #report_dict['n_records'] = nn_n_records\n",
    "#             report_dict['price_range'] = nn_price_slider\n",
    "\n",
    "#             report_dict['n_features'] = n_features\n",
    "#             report_dict['nn_steps_out'] = nn_forecast_period\n",
    "#             report_dict['nn_steps_in'] = nn_steps_in_window\n",
    "\n",
    "#             report_dict['optimizer'] = nn_optimizer_dd\n",
    "#             report_dict['loss'] = nn_loss_dd\n",
    "#             report_dict['score'] = nn_score_dd\n",
    "\n",
    "#             report_dict['n_epochs'] = n_epochs\n",
    "#             report_dict['n_batch'] = n_batch\n",
    "#             report_dict['n_neurons'] = n_neurons\n",
    "#             report_dict['learning_rate'] = nn_lr_slider\n",
    "#             report_dict['test_type'] = test_type\n",
    "\n",
    "#             #save report_dict\n",
    "#             FileName = 'dash_report_df_Score'\n",
    "#             report_df = chk_create_update_df_dict(data=report_dict, file_path=data_path, file_name=FileName,\n",
    "#                                                   dict_key=test_type, dict_value=None, save_df=df_tail)\n",
    "#             print('#save scaler_X, scaler_y')\n",
    "#             FileName = 'ts_scaled_Xy'\n",
    "#             scaled_df = chk_create_update_df_dict(data=None, file_path=data_path, file_name=FileName,\n",
    "#                                                   dict_key=test_type, dict_value=scaler_Xy, save_df=None)\n",
    "\n",
    "            print('#out data')\n",
    "            out_data_nn = f'optimizer:{nn_optimizer_dd}, lrearning_rate:{nn_lr_slider}, n_epochs:{n_epochs}, n_batch:{n_batch},\\\n",
    "            n_neurons:{n_neurons}, test_type:{test_type}'\n",
    "            print(f'history_df-shp:{history_df.shape}, history_df-cols:{history_df.columns}')\n",
    "\n",
    "    fig_nn = make_subplots(rows=1, cols=2)\n",
    "    fig_nn.append_trace(go.Scatter(x=history_df.index, y=history_df['loss'], name='loss', line = dict(color='royalblue', width=4, dash='dash')), row=1, col=1)\n",
    "    fig_nn.append_trace(go.Scatter(x=history_df.index, y=history_df['val_loss'], name='val_loss', line=dict(color='red', width=4, dash='dot')), row=1, col=1)\n",
    "    fig_nn.append_trace(go.Scatter(x=history_df.index, y=history_df[nn_score_dd], name=nn_score_dd, line = dict(color='blue', width=4, dash='dash')), row=1, col=2)\n",
    "    fig_nn.append_trace(go.Scatter(x=history_df.index, y=history_df['val_'+nn_score_dd], name='val_'+nn_score_dd, line=dict(color='yellow', width=4, dash='dot')), row=1, col=2)\n",
    "\n",
    "    return fig_nn, out_data_nn\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     app.run_server(port=8055)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba76235d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8058/\n",
      "\n",
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8058/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [18/Nov/2022 11:23:59] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Nov/2022 11:23:59] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Nov/2022 11:23:59] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Nov/2022 11:23:59] \"GET /_favicon.ico?v=2.0.0 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Nov/2022 11:23:59] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "callback data_pp_dd_text_target1\n",
      "read_file: None\n",
      "callback show_hide_element_Data_pp2\n",
      "else other\n",
      "Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\flask\\app.py\", line 2077, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\flask\\app.py\", line 1525, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\flask\\app.py\", line 1523, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\ajverma\\AppData\\Roaming\\Python\\Python39\\site-packages\\flask\\app.py\", line 1509, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**req.view_args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\dash\\dash.py\", line 1336, in dispatch\n",
      "    response.set_data(func(*args, outputs_list=outputs_list))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\dash\\_callback.py\", line 151, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"C:\\Users\\ajverma\\AppData\\Local\\Temp\\ipykernel_23196\\1085146365.py\", line 367, in data_pp_dd_text_target1\n",
      "    print(f'df-shp:{df.shape}, df-cols:{df.columns.tolist()}')\n",
      "AttributeError: 'NoneType' object has no attribute 'shape'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [18/Nov/2022 11:23:59] \"POST /_dash-update-component HTTP/1.1\" 500 -\n",
      "127.0.0.1 - - [18/Nov/2022 11:24:01] \"GET /_dash-component-suites/dash/dcc/async-upload.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Nov/2022 11:24:01] \"GET /_dash-component-suites/dash/dcc/async-slider.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Nov/2022 11:24:01] \"GET /_dash-component-suites/dash/dcc/async-dropdown.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Nov/2022 11:24:01] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Nov/2022 11:24:01] \"GET /_dash-component-suites/dash/dcc/async-graph.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Nov/2022 11:24:01] \"GET /_dash-component-suites/dash/dcc/async-plotlyjs.js HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "callback dropdown_pp_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [18/Nov/2022 11:24:06] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "callback data_pp_dd_text_target1\n",
      "read_file: spam.csv\n",
      "csv: (5572, 5)\n",
      "df-shp:(5572, 5), df-cols:['v1', 'v2', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [18/Nov/2022 11:25:00] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "callback show_hide_element_Data_pp2\n",
      "else other\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [18/Nov/2022 11:26:28] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "callback show_hide_element_Data_pp2\n",
      "else other\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [18/Nov/2022 11:26:33] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "callback show_hide_element_Data_pp2\n",
      "else other\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [18/Nov/2022 12:01:46] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "callback show_hide_element_Data_pp2\n",
      "else other\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run_server(port=8058)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d3690e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
